{"cells":[{"cell_type":"markdown","source":["# Chess Moves Generator\n\n####Big Data Computing Project - A.Y. 2021/2022 - Valerio Goretti 1811110 \n###goretti.1811110@studenti.uniroma1.it\n<img src=\"https://www.dropbox.com/s/dthoa5ltlvk1aua/chess-wallpaper-23567-24220-hd-wallpapers-1782160017-1519587376418.jpg?dl=1\">"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"782b1108-758e-4a60-8fe1-bcd781dec5e6"}}},{"cell_type":"markdown","source":["#Project Index\n<p><i><b>Introduction</b></i>. The goal of the project, the approaches used and the context are presented </p>\n<p><i><b>Section 1: Dataset presentation</b></i>. In this section, the dataset is read and explored </p>\n<p><i><b>Section 2: First approach</b></i>. This section provides an explanation of the <b>first approach</b> used to achieve the goal and the results obtained</p>\n<p><i><b>Section 3: Second approach</b></i>. This section provides an explanation of the <b>second approach</b> used to achieve the goal and the results obtained</p>\n<p><i><b>Section 4: Third approach</b></i>. This section provides an explanation of the <b>third approach</b> used to achieve the goal and the results obtained</p>\n<p><i><b>Section 5: Application to use case</b></i>. In this section an application is developed to test the model create</p>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"41fbac93-0a5b-4568-b978-9220ccdd0a52"}}},{"cell_type":"markdown","source":["# Introduction\n<p>Chess is one of the oldest and most popular board games, played by two opponents on a chessboard with 64 squares arraged in eigth-by-eigth grid. The two players have a set of pieces (pawn, knight, bishop, queen, king) and can move them in alternating turns on the board according to precise rules of movement.</p>\n  <img src=\"https://www.dropbox.com/s/8j8z1tj9aqpsej2/ChessboardPosition.png?dl=1\" width=\"400\" height=\"500\">\n<p>People have been trying to write programs that could play perfect chess games since 1951. The first person to write a program to move the pieces on the board in the best possible way was Alan Turing. The program is called <a href=\"https://www.chessprogramming.org/Turochamp\">Turochamp</a>.<br>\nIn recent years, chess engines have surpassed the level of humans but are still unable to make perfect games from the first move to the last. This is due to the fact that all possible moves in a chess game are 10^123. This number was calculated by Claude Shannon and took the name \"Shannon's number\". </p>\n\n<p>A chess game is divided into 3 parts: <b>Opening</b>, the initial part of the game in which space is given to the pieces. <b>Middlegame</b>, the middle part of the game, is the longest and most reasoned part. Moreover, it is the least textbook part; players try to make strategies and tactics to get to the final part of the game with an advantage. The final part is called the <b>endgame</b>, and here players try to finish the game with a win.</p>\n\n<p> <b>The goal</b> of this work is to create a model that predicts the next move during a chess game. <b>The challenge</b>, however, is to make the model as if the chess game is a conversation. So the moves take the place of the words and the game becomes a conversation. To do this I need a neural network that can generate text. So as to make a chess game an exchange of words between two people. </p>\n\n<p>In order to manage the dataset, Spark has been used. Concerning the Machine Learning parts, Keras and Tensorflow have been used for implementations.<br>\nThe project was developed on Databricks Community Edition, Kaggle, and Paperspace. Since Databricks does not provide a gpu, Kaggle and Paperspace were used because they provide this possibility. After the data preparation cells for learning, these data were saved in Databricks DBFS in order to export and use them on other platforms. Once used, the produced cells were reported on this notebook and the produced outputs were imported through Dropbox. all cells reported from other platforms were re-run on databricks to show that it works. The cells containing model fits have been executed to demonstrate the functionality of the code but then blocked because it have already been executed externally. These cells are always followed by cells used to import the relative models.</p>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"88ed2cec-6e1a-4570-98ef-d975e5fdc93c"}}},{"cell_type":"markdown","source":["## Neural Network Background\n#### Since the same Neural Network was used for all approaches, it is explained in the introduction to avoid repetition to the reader while reading the notebook\n\n\n<p>Recurrent Neural Networks (RNN) deal with my task. Indeed, the RNN taking sequences as input and modeling them over time. But RNNs have a problem, they can't store sequences that are too long. So LSTM neural networks are used. LSTM is a RNN created to solve the problem of RNNs. These networks can process sequences of data and maintain their states for a long time using different types of gates. When a new input comes in RNN, it modifies the existing information without deciding if the incoming input is important or not, whereas in the case of LSTM gates are available to allow only important inputs to modify the existing information.\n</p> <br>\n\n<img src=\"https://www.dropbox.com/s/ufsxap9xq59jjhl/RNNVSLSTM.jpg?dl=1\"> <br>\n\n<p>In more detail, RNNs during the processing of their data in addition to the input also use the output of the previous step. In the LSTM it works the same way, but there is an internal state. So the input to be processed will not only be that given by the input and the output of the previous step but also an additional input given by the internal state. Once these 3 data are processed, the processing provides not only the output but also an update to the internal state. The internal state consists of 3 parts. <b>Forget Gate</b>, Input state and output state. Forget state is used for This gate is for selecting relevant information and discarding irrelevant information. The data is passed through a sigmoid function that returns a value between 0 and 1, if it returns 0 the information is ignored otherwise considered within the input. <b>Input Gate</b> is resposible for adding information to the model. It create an array of information that contains value from -1 to 1 with the tanh function. This value is filtered by the sigmoid function that mantain the relevant information should be added or updated into the internal state. The <b>Output Gate</b> is responsible for generating outputs. In particuar between all the information that are stored in a state which part of it should be output in this particular instace. These gates are part of the network parameters and are learned during the training.</p>\n<p></p>\n\n<p>This neural network is used in this work, below you can see the general architecture of the network used. </p>\n<img src=\"https://www.dropbox.com/s/tx5gzq5fyl2xgpo/LSTM_Architecture.jpg?dl=1\"> <br>\n<p>In the 3 proposed approaches, the network used generally follows the reported architecture.  For each approach, more than one model was trained. In some models the networks may be slightly different, changes affect the numbers of units for the LSTM networks, and dropouts were added or not added and at different percentage rates. These changes were made to increase model performance and to evaluate the different shades of the network during evaluation</p>\n\n## Use of data in the network\n\n<p>Data within the lstm network were passed using the sliding window technique. This technique consists of breaking sentences (in the case of this work chess games) into arrays of fixed length so that the chosen sentence is covered all over.<br>\nFor example, the sliding window of the sentence \"the man is walking down the street.\" is: </p>\n<img src=\"https://www.dropbox.com/s/3d4fc2zzim1of08/slide_window.jpeg?dl=1\"> <br>\n<p>In this work the approaches differ in this aspect. The data are handled in the same way but are modeled differently. the modeling started from a very raw granularity of the move, considering only the piece, to the move that also includes disambiguations.</p>\n\n<p>Peculiarities of algebraic notation (it is explained in the \"Chess Background\" section) were not considered in all approaches. The features not considered are: piece takes (the \"x\" symbol in the move ), check and ckeckmate. It was not considered since these are features that need the state of the chessboard. Since chessboard state is not used in this project these types of moves were modified without these features. For example, the move Nxf3 will be considered as Nf3. </p>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"387c5caf-2e50-4f7a-8e1a-d71575221988"}}},{"cell_type":"markdown","source":["## Evaluation Background\n#### Since the same methods of evaluation was used for all approaches, it is explained in the introduction to avoid repetition to the reader while reading the notebook\n\n<p>Two types of evaluations were made, one Extrinsic and one Intrinsic.</p>\n<p><b>The Extrinsic evaluation</b>, it was measured how often the models correctly predicted the moves played in the test set games. Using this method, since the model has never seen the games used in evaluation, it is as if high-level players play with hints given by the models, and if one of them accepts the hint, the model's score is increased.</p>\n<p>In <b>The Intrinsic  evaluation</b> metrics to evaluate the model are used. Compared to Extrinsic evaluation, the context of use is lost. In this work the <b>perplexity</b> metric was used. Perplexity is a measurement of how well a probability model predicts a sample.<br>\nThe perplexity is calculated with the formula below:</p>\n<img src=\"https://www.dropbox.com/s/qrbz6aykypi5tam/perplexity.png?dl=1\"> \n<p>The perplexity is calculated as the multiplication between all the probabilities of the predictions made during a game by normalizing the multiplication by the number of moves to be predicted. <br>\nIn this work, in evaluation phase, the model made predictions and the probabilities of each prediction were multiplied and then normalized. At the end of evaluation each model has a perplexity list that defines the perplexities for all the test set matches seen. In order to have a single value with which to compare the models, an average is calculated among these perplexities.\n</p>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0b17119e-497a-4614-9c67-cfcd33b091d7"}}},{"cell_type":"markdown","source":["## Chess Background\n####This part is included to give the domain knowledge needed to understand the work \n<p id=\"notation\">In the chess game to identify a move there is a notation called <a href=\"https://en.wikipedia.org/wiki/Algebraic_notation_(chess)\">algebraic notation</a>. This notation is also used for this work. First, this notation involves naming the cells of the chessboard with a letter to identify the column and a number to identify the row related to the cell. </p>\n\n\n<img src=\"https://www.dropbox.com/s/njage6me1mxni0m/Notation.png?dl=1\" width=\"400\" height=\"500\">\n\n<p>The <b>pieces</b> are identified with an uppercase letter: K for king, Q for queen, R for rook, B for bishop, and N for knight. When no piece is indicated on the move we refer to pawns.</p>\n<p>Each <b>move</b>of a piece is indicated by the piece's uppercase letter, plus the coordinate of the destination square. For example, Nf3 means that the Knight is moved in the cell f3. </p>\n<p>If the knight in cell f3 takes an opponent's piece, an \"x\" will be added just after the piece and the move will become \"Nxf3\". </p>\n<p> Another distinction is made between pieces of the same color. Having two knights available, if both can go into the f3 cell, we need to distinguish between them. To <b>disambiguate the moves</b> there are 2 ways.If the pieces are distinguishable on the basis of the column, we place the identifying letter of the column after the piece letter. For example, One knight is on d4, the other is on g5, both can go to f3, but we want to move the one that is on d4 and so the move will be \"Ndf3\". If the knight on f3 takes an opponent's piece, the move becomes Ndxf3. Instead, if The knights are on the same column, for example, one knight is on d2 and one is on d4, we will distinguish the two pieces by noting the line on which they are placed. For example, if we want to move the knight to d4 the move will be \"N4f3\", the same rule applies for the \"x\", if with this move the knight takes a piece the \"x\" should be put after the disambiguation and before the cell. </p>\n<p>When a pawn arrives in the last row of the column, it can be changed to another piece at will, this move is called <b>pawn promotion</b>. In case the white pawn in the column in and is promoted to queen the move will be \"e8Q\".</p>\n<p>In chess it is possible to make some special moves such as <b>Castling</b>. Castling is a special move to protect the king. It's the only time in chess you can move two pieces in one move. The king moves two squares to the right or left and the rook moves directly to the other side of the king. You can only castle if neither the rook or king have moved and there are no pieces in the way. Depending on where the king moves, whether he \"swaps\" position with the rook closest to him is called kingside castling and is identified with the \"0-0\" string. However, if it is exchanged with the rook closest to the queen it is called queenside castling and the move is identified with \"0-0-0\". </p>\n<p>The situation in which the king is under attack from an opposing piece is called <b>check</b>. A move that places the opponent's king in check usually has the symbol \"+\" at the end of the move. For example, \"Nc7+\". </p>\n<p>If the king is under attack by two pieces at the same time, it is called a <b>double check</b> and is marked ++. For example, \"Bh6++\".</p>\n<p>The situation in which one of the two kings is attacked by an opponent's piece and the player has no more legal moves to play is called <b>checkmate</b>. The move that led to checkmate is marked by # at the end. for example, \"Qh8#\".</p>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2639b80e-2cae-4b7c-a2e8-dd000caa6d3d"}}},{"cell_type":"markdown","source":["# Section 1: Dataset presentation\n<p> I this section the dataset is loaded, we do an exploration on the dataset and prepare it for the task</p>\n<p>The dataset was chosen by kaggle, and can be found <a href=\"https://www.kaggle.com/datasets/milesh1/35-million-chess-games\">here</a>. The dataset is contained in a txt file and includes 3.5 million chess games. For each game there is an identifying number, the date, the final result of the game, The score of the players at that time, the length of the game, some control fields, and the set of moves in the game. All data are separated by a space except for the game that has ### before. Each move in the match is separated by a space and has a letter W (White's move) or B (Black's move) and a number indicating the respective player's ith move.\nFor example, a row of the dataset is: </p>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a20a004f-74df-4a87-a2f0-40ac57006f54"}}},{"cell_type":"markdown","source":["> 2 2000.03.14 1-0 2851 None 53 date_false result_false welo_false belo_true edate_true setup_false fen_false result2_false oyrange_false blen_false ### W1.e4 B1.d5 W2.exd5 B2.Qxd5 W3.Nc3 B3.Qa5 W4.d4 B4.Nf6 W5.Nf3 B5.c6 W6.Ne5 B6.Bf5 W7.g4 B7.Be4 W8.f3 B8.Bd5 W9.a3 B9.Nbd7 W10.Be3 B10.Nxe5 W11.dxe5 B11.Nxg4 W12.Bd4 B12.e6 W13.b4 B13.Qd8 W14.Nxd5 B14.Qxd5 W15.c4 B15.Ne3 W16.cxd5 B16.Nxd1 W17.dxc6 B17.bxc6 W18.Rxd1 B18.Be7 W19.Ba6 B19.O-O W20.Ke2 B20.Rab8 W21.Rc1 B21.Rfd8 W22.Rhd1 B22.c5 W23.Bxc5 B23.Rxd1 W24.Rxd1 B24.Bxc5 W25.bxc5 B25.g6 W26.c6 B26.Rb2+ W27.Rd2"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4d4b80ca-3756-4a9a-bc87-155121ee957e"}}},{"cell_type":"markdown","source":["In the following cells we set up the environment for running the code, installing and importing the necessary libraries"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"273a8d19-3ade-4709-9821-efa1bb1f9c08"}}},{"cell_type":"code","source":["%pip install --upgrade tensorflow\n%pip install --upgrade category_encoders\n%pip install chess"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6066eeea-e2bc-4330-b3cc-798ed82a823c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Python interpreter will be restarted.\nCollecting tensorflow\n  Downloading tensorflow-2.9.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\nCollecting tensorboard<2.10,>=2.9\n  Downloading tensorboard-2.9.0-py3-none-any.whl (5.8 MB)\nCollecting gast<=0.4.0,>=0.2.1\n  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\nCollecting wrapt>=1.11.0\n  Downloading wrapt-1.14.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (81 kB)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow) (52.0.0)\nCollecting astunparse>=1.6.0\n  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\nCollecting tensorflow-io-gcs-filesystem>=0.23.1\n  Downloading tensorflow_io_gcs_filesystem-0.26.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\nCollecting libclang>=13.0.0\n  Downloading libclang-14.0.1-py2.py3-none-manylinux1_x86_64.whl (14.5 MB)\nRequirement already satisfied: protobuf<3.20,>=3.9.2 in /databricks/python3/lib/python3.8/site-packages (from tensorflow) (3.17.2)\nCollecting opt-einsum>=2.3.2\n  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\nCollecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\nCollecting google-pasta>=0.1.1\n  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\nCollecting flatbuffers<2,>=1.12\n  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\nCollecting keras<2.10.0,>=2.9.0rc0\n  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\nCollecting termcolor>=1.1.0\n  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\nCollecting absl-py>=1.0.0\n  Downloading absl_py-1.1.0-py3-none-any.whl (123 kB)\nRequirement already satisfied: six>=1.12.0 in /databricks/python3/lib/python3.8/site-packages (from tensorflow) (1.15.0)\nCollecting keras-preprocessing>=1.1.1\n  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\nRequirement already satisfied: numpy>=1.20 in /databricks/python3/lib/python3.8/site-packages (from tensorflow) (1.20.1)\nCollecting typing-extensions>=3.6.6\n  Downloading typing_extensions-4.2.0-py3-none-any.whl (24 kB)\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.8/site-packages (from tensorflow) (20.9)\nCollecting grpcio<2.0,>=1.24.3\n  Downloading grpcio-1.46.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\nCollecting h5py>=2.9.0\n  Downloading h5py-3.7.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.5 MB)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /databricks/python3/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow) (0.36.2)\nCollecting werkzeug>=1.0.1\n  Downloading Werkzeug-2.1.2-py3-none-any.whl (224 kB)\nRequirement already satisfied: requests<3,>=2.21.0 in /databricks/python3/lib/python3.8/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.25.1)\nCollecting google-auth<3,>=1.6.3\n  Downloading google_auth-2.6.6-py2.py3-none-any.whl (156 kB)\nCollecting google-auth-oauthlib<0.5,>=0.4.1\n  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\nCollecting tensorboard-data-server<0.7.0,>=0.6.0\n  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\nCollecting tensorboard-plugin-wit>=1.6.0\n  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\nCollecting markdown>=2.6.8\n  Downloading Markdown-3.3.7-py3-none-any.whl (97 kB)\nCollecting pyasn1-modules>=0.2.1\n  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\nCollecting rsa<5,>=3.1.4\n  Downloading rsa-4.8-py3-none-any.whl (39 kB)\nCollecting cachetools<6.0,>=2.0.0\n  Downloading cachetools-5.2.0-py3-none-any.whl (9.3 kB)\nCollecting requests-oauthlib>=0.7.0\n  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\nCollecting importlib-metadata>=4.4\n  Downloading importlib_metadata-4.11.4-py3-none-any.whl (18 kB)\nCollecting zipp>=0.5\n  Downloading zipp-3.8.0-py3-none-any.whl (5.4 kB)\nCollecting pyasn1<0.5.0,>=0.4.6\n  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2020.12.5)\nRequirement already satisfied: chardet<5,>=3.0.2 in /databricks/python3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (4.0.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.25.11)\nRequirement already satisfied: idna<3,>=2.5 in /databricks/python3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.10)\nCollecting oauthlib>=3.0.0\n  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\nRequirement already satisfied: pyparsing>=2.0.2 in /databricks/python3/lib/python3.8/site-packages (from packaging->tensorflow) (2.4.7)\nBuilding wheels for collected packages: termcolor\n  Building wheel for termcolor (setup.py): started\n  Building wheel for termcolor (setup.py): finished with status 'done'\n  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=83fe61c005fcd16212e3210420fa9d4aac7f16bdc836dddf84f8f9b1c041f3e7\n  Stored in directory: /root/.cache/pip/wheels/a0/16/9c/5473df82468f958445479c59e784896fa24f4a5fc024b0f501\nSuccessfully built termcolor\nInstalling collected packages: pyasn1, zipp, rsa, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, importlib-metadata, google-auth, werkzeug, tensorboard-plugin-wit, tensorboard-data-server, markdown, grpcio, google-auth-oauthlib, absl-py, wrapt, typing-extensions, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, opt-einsum, libclang, keras-preprocessing, keras, h5py, google-pasta, gast, flatbuffers, astunparse, tensorflow\nSuccessfully installed absl-py-1.1.0 astunparse-1.6.3 cachetools-5.2.0 flatbuffers-1.12 gast-0.4.0 google-auth-2.6.6 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.46.3 h5py-3.7.0 importlib-metadata-4.11.4 keras-2.9.0 keras-preprocessing-1.1.2 libclang-14.0.1 markdown-3.3.7 oauthlib-3.2.0 opt-einsum-3.3.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.1 rsa-4.8 tensorboard-2.9.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.9.1 tensorflow-estimator-2.9.0 tensorflow-io-gcs-filesystem-0.26.0 termcolor-1.1.0 typing-extensions-4.2.0 werkzeug-2.1.2 wrapt-1.14.1 zipp-3.8.0\nPython interpreter will be restarted.\nPython interpreter will be restarted.\nCollecting category_encoders\n  Downloading category_encoders-2.5.0-py2.py3-none-any.whl (69 kB)\nRequirement already satisfied: pandas>=1.0.5 in /databricks/python3/lib/python3.8/site-packages (from category_encoders) (1.2.4)\nRequirement already satisfied: numpy>=1.14.0 in /databricks/python3/lib/python3.8/site-packages (from category_encoders) (1.20.1)\nRequirement already satisfied: patsy>=0.5.1 in /databricks/python3/lib/python3.8/site-packages (from category_encoders) (0.5.1)\nRequirement already satisfied: statsmodels>=0.9.0 in /databricks/python3/lib/python3.8/site-packages (from category_encoders) (0.12.2)\nRequirement already satisfied: scipy>=1.0.0 in /databricks/python3/lib/python3.8/site-packages (from category_encoders) (1.6.2)\nRequirement already satisfied: scikit-learn>=0.20.0 in /databricks/python3/lib/python3.8/site-packages (from category_encoders) (0.24.1)\nRequirement already satisfied: pytz>=2017.3 in /databricks/python3/lib/python3.8/site-packages (from pandas>=1.0.5->category_encoders) (2020.5)\nRequirement already satisfied: python-dateutil>=2.7.3 in /databricks/python3/lib/python3.8/site-packages (from pandas>=1.0.5->category_encoders) (2.8.1)\nRequirement already satisfied: six in /databricks/python3/lib/python3.8/site-packages (from patsy>=0.5.1->category_encoders) (1.15.0)\nRequirement already satisfied: joblib>=0.11 in /databricks/python3/lib/python3.8/site-packages (from scikit-learn>=0.20.0->category_encoders) (1.0.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.8/site-packages (from scikit-learn>=0.20.0->category_encoders) (2.1.0)\nInstalling collected packages: category-encoders\nSuccessfully installed category-encoders-2.5.0\nPython interpreter will be restarted.\nPython interpreter will be restarted.\nCollecting chess\n  Downloading chess-1.9.1-py3-none-any.whl (148 kB)\nInstalling collected packages: chess\nSuccessfully installed chess-1.9.1\nPython interpreter will be restarted.\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Python interpreter will be restarted.\nCollecting tensorflow\n  Downloading tensorflow-2.9.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\nCollecting tensorboard<2.10,>=2.9\n  Downloading tensorboard-2.9.0-py3-none-any.whl (5.8 MB)\nCollecting gast<=0.4.0,>=0.2.1\n  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\nCollecting wrapt>=1.11.0\n  Downloading wrapt-1.14.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (81 kB)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow) (52.0.0)\nCollecting astunparse>=1.6.0\n  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\nCollecting tensorflow-io-gcs-filesystem>=0.23.1\n  Downloading tensorflow_io_gcs_filesystem-0.26.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\nCollecting libclang>=13.0.0\n  Downloading libclang-14.0.1-py2.py3-none-manylinux1_x86_64.whl (14.5 MB)\nRequirement already satisfied: protobuf<3.20,>=3.9.2 in /databricks/python3/lib/python3.8/site-packages (from tensorflow) (3.17.2)\nCollecting opt-einsum>=2.3.2\n  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\nCollecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\nCollecting google-pasta>=0.1.1\n  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\nCollecting flatbuffers<2,>=1.12\n  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\nCollecting keras<2.10.0,>=2.9.0rc0\n  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\nCollecting termcolor>=1.1.0\n  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\nCollecting absl-py>=1.0.0\n  Downloading absl_py-1.1.0-py3-none-any.whl (123 kB)\nRequirement already satisfied: six>=1.12.0 in /databricks/python3/lib/python3.8/site-packages (from tensorflow) (1.15.0)\nCollecting keras-preprocessing>=1.1.1\n  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\nRequirement already satisfied: numpy>=1.20 in /databricks/python3/lib/python3.8/site-packages (from tensorflow) (1.20.1)\nCollecting typing-extensions>=3.6.6\n  Downloading typing_extensions-4.2.0-py3-none-any.whl (24 kB)\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.8/site-packages (from tensorflow) (20.9)\nCollecting grpcio<2.0,>=1.24.3\n  Downloading grpcio-1.46.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\nCollecting h5py>=2.9.0\n  Downloading h5py-3.7.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.5 MB)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /databricks/python3/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow) (0.36.2)\nCollecting werkzeug>=1.0.1\n  Downloading Werkzeug-2.1.2-py3-none-any.whl (224 kB)\nRequirement already satisfied: requests<3,>=2.21.0 in /databricks/python3/lib/python3.8/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.25.1)\nCollecting google-auth<3,>=1.6.3\n  Downloading google_auth-2.6.6-py2.py3-none-any.whl (156 kB)\nCollecting google-auth-oauthlib<0.5,>=0.4.1\n  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\nCollecting tensorboard-data-server<0.7.0,>=0.6.0\n  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\nCollecting tensorboard-plugin-wit>=1.6.0\n  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\nCollecting markdown>=2.6.8\n  Downloading Markdown-3.3.7-py3-none-any.whl (97 kB)\nCollecting pyasn1-modules>=0.2.1\n  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\nCollecting rsa<5,>=3.1.4\n  Downloading rsa-4.8-py3-none-any.whl (39 kB)\nCollecting cachetools<6.0,>=2.0.0\n  Downloading cachetools-5.2.0-py3-none-any.whl (9.3 kB)\nCollecting requests-oauthlib>=0.7.0\n  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\nCollecting importlib-metadata>=4.4\n  Downloading importlib_metadata-4.11.4-py3-none-any.whl (18 kB)\nCollecting zipp>=0.5\n  Downloading zipp-3.8.0-py3-none-any.whl (5.4 kB)\nCollecting pyasn1<0.5.0,>=0.4.6\n  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2020.12.5)\nRequirement already satisfied: chardet<5,>=3.0.2 in /databricks/python3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (4.0.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.25.11)\nRequirement already satisfied: idna<3,>=2.5 in /databricks/python3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.10)\nCollecting oauthlib>=3.0.0\n  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\nRequirement already satisfied: pyparsing>=2.0.2 in /databricks/python3/lib/python3.8/site-packages (from packaging->tensorflow) (2.4.7)\nBuilding wheels for collected packages: termcolor\n  Building wheel for termcolor (setup.py): started\n  Building wheel for termcolor (setup.py): finished with status 'done'\n  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=83fe61c005fcd16212e3210420fa9d4aac7f16bdc836dddf84f8f9b1c041f3e7\n  Stored in directory: /root/.cache/pip/wheels/a0/16/9c/5473df82468f958445479c59e784896fa24f4a5fc024b0f501\nSuccessfully built termcolor\nInstalling collected packages: pyasn1, zipp, rsa, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, importlib-metadata, google-auth, werkzeug, tensorboard-plugin-wit, tensorboard-data-server, markdown, grpcio, google-auth-oauthlib, absl-py, wrapt, typing-extensions, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, opt-einsum, libclang, keras-preprocessing, keras, h5py, google-pasta, gast, flatbuffers, astunparse, tensorflow\nSuccessfully installed absl-py-1.1.0 astunparse-1.6.3 cachetools-5.2.0 flatbuffers-1.12 gast-0.4.0 google-auth-2.6.6 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.46.3 h5py-3.7.0 importlib-metadata-4.11.4 keras-2.9.0 keras-preprocessing-1.1.2 libclang-14.0.1 markdown-3.3.7 oauthlib-3.2.0 opt-einsum-3.3.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.1 rsa-4.8 tensorboard-2.9.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.9.1 tensorflow-estimator-2.9.0 tensorflow-io-gcs-filesystem-0.26.0 termcolor-1.1.0 typing-extensions-4.2.0 werkzeug-2.1.2 wrapt-1.14.1 zipp-3.8.0\nPython interpreter will be restarted.\nPython interpreter will be restarted.\nCollecting category_encoders\n  Downloading category_encoders-2.5.0-py2.py3-none-any.whl (69 kB)\nRequirement already satisfied: pandas>=1.0.5 in /databricks/python3/lib/python3.8/site-packages (from category_encoders) (1.2.4)\nRequirement already satisfied: numpy>=1.14.0 in /databricks/python3/lib/python3.8/site-packages (from category_encoders) (1.20.1)\nRequirement already satisfied: patsy>=0.5.1 in /databricks/python3/lib/python3.8/site-packages (from category_encoders) (0.5.1)\nRequirement already satisfied: statsmodels>=0.9.0 in /databricks/python3/lib/python3.8/site-packages (from category_encoders) (0.12.2)\nRequirement already satisfied: scipy>=1.0.0 in /databricks/python3/lib/python3.8/site-packages (from category_encoders) (1.6.2)\nRequirement already satisfied: scikit-learn>=0.20.0 in /databricks/python3/lib/python3.8/site-packages (from category_encoders) (0.24.1)\nRequirement already satisfied: pytz>=2017.3 in /databricks/python3/lib/python3.8/site-packages (from pandas>=1.0.5->category_encoders) (2020.5)\nRequirement already satisfied: python-dateutil>=2.7.3 in /databricks/python3/lib/python3.8/site-packages (from pandas>=1.0.5->category_encoders) (2.8.1)\nRequirement already satisfied: six in /databricks/python3/lib/python3.8/site-packages (from patsy>=0.5.1->category_encoders) (1.15.0)\nRequirement already satisfied: joblib>=0.11 in /databricks/python3/lib/python3.8/site-packages (from scikit-learn>=0.20.0->category_encoders) (1.0.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.8/site-packages (from scikit-learn>=0.20.0->category_encoders) (2.1.0)\nInstalling collected packages: category-encoders\nSuccessfully installed category-encoders-2.5.0\nPython interpreter will be restarted.\nPython interpreter will be restarted.\nCollecting chess\n  Downloading chess-1.9.1-py3-none-any.whl (148 kB)\nInstalling collected packages: chess\nSuccessfully installed chess-1.9.1\nPython interpreter will be restarted.\n"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import *\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType, ArrayType, IntegerType\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom keras.layers import Embedding, LSTM, Dense, Dropout,Activation\nfrom keras.callbacks import EarlyStopping\nfrom keras.models import Sequential\nfrom keras.callbacks import ModelCheckpoint\nimport matplotlib.pyplot as plt\nimport json\nimport math\nimport os\nimport csv"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6eaade39-2eab-459e-830a-8bd4aff646c2"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Loading the txt of the dataset and store it in a pyspark dataframe. In this part the dataframe is split by ### so that there are two columns, one containing the match information and the other the moves"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7a6a1c4f-43cc-48d6-a9bc-826eddf140fe"}}},{"cell_type":"code","source":["df = spark.read.option(\"header\", \"false\") \\\n    .option(\"delimiter\", \"### \") \\\n    .option(\"inferSchema\", \"true\") \\\n    .csv(\"/bdc-2020-21/datasets/Chess.txt\")\n\ndf.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"56ca845c-d250-4eb6-968e-469f02fe03a0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- _c0: string (nullable = true)\n |-- _c1: string (nullable = true)\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- _c0: string (nullable = true)\n |-- _c1: string (nullable = true)\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d663ada0-8c74-41ee-a88f-4e164976c0f2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------------------+--------------------+\n|                 _c0|                 _c1|\n+--------------------+--------------------+\n|1 2000.03.14 1-0 ...|W1.d4 B1.d5 W2.c4...|\n|2 2000.03.14 1-0 ...|W1.e4 B1.d5 W2.ex...|\n|3 1999.11.20 1-0 ...|W1.e4 B1.e5 W2.Nf...|\n|4 1999.11.20 1-0 ...|W1.e4 B1.d5 W2.ex...|\n|5 2000.02.20 1/2-...|W1.e4 B1.e5 W2.Nf...|\n|6 2000.05.24 1/2-...|W1.d4 B1.e6 W2.Nf...|\n|7 2000.06.19 1-0 ...|W1.e4 B1.c5 W2.Nf...|\n|8 1999.11.20 1-0 ...|W1.d4 B1.Nf6 W2.c...|\n|9 2000.06.19 1/2-...|W1.c4 B1.e6 W2.g3...|\n|10 2000.01.18 1-0...|W1.e4 B1.c5 W2.Nf...|\n|11 2000.01.28 1/2...|W1.e4 B1.c5 W2.Nf...|\n|12 2000.02.12 1-0...|W1.d4 B1.d5 W2.c4...|\n|13 2000.06.29 1-0...|W1.b3 B1.c5 W2.Bb...|\n|14 2000.06.29 1-0...|W1.b3 B1.e5 W2.Bb...|\n|15 2000.06.23 1/2...|W1.d4 B1.e6 W2.c4...|\n|16 1999.11.20 1-0...|W1.e4 B1.e5 W2.Nf...|\n|17 1999.11.20 1-0...|W1.e4 B1.c6 W2.d4...|\n|18 2000.06.19 1-0...|W1.e4 B1.c5 W2.Nf...|\n|19 2000.06.24 0-1...|W1.d4 B1.Nf6 W2.N...|\n|20 2000.01.30 1-0...|W1.e4 B1.c5 W2.Nf...|\n+--------------------+--------------------+\nonly showing top 20 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------------------+--------------------+\n|                 _c0|                 _c1|\n+--------------------+--------------------+\n|1 2000.03.14 1-0 ...|W1.d4 B1.d5 W2.c4...|\n|2 2000.03.14 1-0 ...|W1.e4 B1.d5 W2.ex...|\n|3 1999.11.20 1-0 ...|W1.e4 B1.e5 W2.Nf...|\n|4 1999.11.20 1-0 ...|W1.e4 B1.d5 W2.ex...|\n|5 2000.02.20 1/2-...|W1.e4 B1.e5 W2.Nf...|\n|6 2000.05.24 1/2-...|W1.d4 B1.e6 W2.Nf...|\n|7 2000.06.19 1-0 ...|W1.e4 B1.c5 W2.Nf...|\n|8 1999.11.20 1-0 ...|W1.d4 B1.Nf6 W2.c...|\n|9 2000.06.19 1/2-...|W1.c4 B1.e6 W2.g3...|\n|10 2000.01.18 1-0...|W1.e4 B1.c5 W2.Nf...|\n|11 2000.01.28 1/2...|W1.e4 B1.c5 W2.Nf...|\n|12 2000.02.12 1-0...|W1.d4 B1.d5 W2.c4...|\n|13 2000.06.29 1-0...|W1.b3 B1.c5 W2.Bb...|\n|14 2000.06.29 1-0...|W1.b3 B1.e5 W2.Bb...|\n|15 2000.06.23 1/2...|W1.d4 B1.e6 W2.c4...|\n|16 1999.11.20 1-0...|W1.e4 B1.e5 W2.Nf...|\n|17 1999.11.20 1-0...|W1.e4 B1.c6 W2.d4...|\n|18 2000.06.19 1-0...|W1.e4 B1.c5 W2.Nf...|\n|19 2000.06.24 0-1...|W1.d4 B1.Nf6 W2.N...|\n|20 2000.01.30 1-0...|W1.e4 B1.c5 W2.Nf...|\n+--------------------+--------------------+\nonly showing top 20 rows\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["Once I read the dataset, In the _c1 column I have the game while in the second column I have all the data separated by space. I split that data by separating it by space and keep only the important columns. I kept the column that identifies the game with a number, the result obtained at the end of the game, the length of the game, and the set of moves played."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"034da0b9-a225-47f0-83c4-e62b7c92560b"}}},{"cell_type":"code","source":["split_col =  split(df._c0, '\\\\ ',)\n\ndf = df.withColumn(\"number\", split_col.getItem(0))\\\n    .withColumn(\"result\", split_col.getItem(2))\\\n    .withColumn(\"len\", split_col.getItem(5))\\\n    .drop(\"_c0\")\n    \ndf=df.withColumnRenamed(\"_c1\",\"game\")\ndf = df.withColumn(\"len\", df[\"len\"].cast(IntegerType()))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"149c40ae-65c5-4781-948f-44c094a948e2"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"77e3bb31-67d7-43a7-ac26-2597bc52a292"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- game: string (nullable = true)\n |-- number: string (nullable = true)\n |-- result: string (nullable = true)\n |-- len: integer (nullable = true)\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- game: string (nullable = true)\n |-- number: string (nullable = true)\n |-- result: string (nullable = true)\n |-- len: integer (nullable = true)\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9e2372e1-06e3-4ae1-acce-00b03f69f1a8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------------------+------+-------+---+\n|                game|number| result|len|\n+--------------------+------+-------+---+\n|W1.d4 B1.d5 W2.c4...|     1|    1-0| 67|\n|W1.e4 B1.d5 W2.ex...|     2|    1-0| 53|\n|W1.e4 B1.e5 W2.Nf...|     3|    1-0| 57|\n|W1.e4 B1.d5 W2.ex...|     4|    1-0| 49|\n|W1.e4 B1.e5 W2.Nf...|     5|1/2-1/2| 97|\n|W1.d4 B1.e6 W2.Nf...|     6|1/2-1/2| 52|\n|W1.e4 B1.c5 W2.Nf...|     7|    1-0| 79|\n|W1.d4 B1.Nf6 W2.c...|     8|    1-0| 71|\n|W1.c4 B1.e6 W2.g3...|     9|1/2-1/2| 72|\n|W1.e4 B1.c5 W2.Nf...|    10|    1-0| 49|\n|W1.e4 B1.c5 W2.Nf...|    11|1/2-1/2| 68|\n|W1.d4 B1.d5 W2.c4...|    12|    1-0|147|\n|W1.b3 B1.c5 W2.Bb...|    13|    1-0| 83|\n|W1.b3 B1.e5 W2.Bb...|    14|    1-0| 61|\n|W1.d4 B1.e6 W2.c4...|    15|1/2-1/2| 75|\n|W1.e4 B1.e5 W2.Nf...|    16|    1-0| 53|\n|W1.e4 B1.c6 W2.d4...|    17|    1-0| 51|\n|W1.e4 B1.c5 W2.Nf...|    18|    1-0| 73|\n|W1.d4 B1.Nf6 W2.N...|    19|    0-1| 94|\n|W1.e4 B1.c5 W2.Nf...|    20|    1-0| 87|\n+--------------------+------+-------+---+\nonly showing top 20 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------------------+------+-------+---+\n|                game|number| result|len|\n+--------------------+------+-------+---+\n|W1.d4 B1.d5 W2.c4...|     1|    1-0| 67|\n|W1.e4 B1.d5 W2.ex...|     2|    1-0| 53|\n|W1.e4 B1.e5 W2.Nf...|     3|    1-0| 57|\n|W1.e4 B1.d5 W2.ex...|     4|    1-0| 49|\n|W1.e4 B1.e5 W2.Nf...|     5|1/2-1/2| 97|\n|W1.d4 B1.e6 W2.Nf...|     6|1/2-1/2| 52|\n|W1.e4 B1.c5 W2.Nf...|     7|    1-0| 79|\n|W1.d4 B1.Nf6 W2.c...|     8|    1-0| 71|\n|W1.c4 B1.e6 W2.g3...|     9|1/2-1/2| 72|\n|W1.e4 B1.c5 W2.Nf...|    10|    1-0| 49|\n|W1.e4 B1.c5 W2.Nf...|    11|1/2-1/2| 68|\n|W1.d4 B1.d5 W2.c4...|    12|    1-0|147|\n|W1.b3 B1.c5 W2.Bb...|    13|    1-0| 83|\n|W1.b3 B1.e5 W2.Bb...|    14|    1-0| 61|\n|W1.d4 B1.e6 W2.c4...|    15|1/2-1/2| 75|\n|W1.e4 B1.e5 W2.Nf...|    16|    1-0| 53|\n|W1.e4 B1.c6 W2.d4...|    17|    1-0| 51|\n|W1.e4 B1.c5 W2.Nf...|    18|    1-0| 73|\n|W1.d4 B1.Nf6 W2.N...|    19|    0-1| 94|\n|W1.e4 B1.c5 W2.Nf...|    20|    1-0| 87|\n+--------------------+------+-------+---+\nonly showing top 20 rows\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["I check the distribution of the lengths of the matches in the dataframe"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e2193510-be70-40ba-a2e1-d988f6959a39"}}},{"cell_type":"code","source":["l=df.select('len').where(col(\"len\").isNotNull()).collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fd406554-5b64-4927-8e7f-61e6a7b2790f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["lung=[0 for i in range(0,16)]\nfor e in l:\n  e=e[0]\n  if e<=10:\n    lung[0]+=1\n  if e>10 and e<=20:\n    lung[1]+=1\n  if e>20 and e<=30:\n    lung[2]+=1\n  if e>30 and e<=40:\n    lung[3]+=1\n  if e>40 and e<=50:\n    lung[4]+=1\n  if e>50 and e<=60:\n    lung[5]+=1\n  if e>60 and e<=70:\n    lung[6]+=1\n  if e>70 and e<=80:\n    lung[7]+=1\n  if e>80 and e<=90:\n    lung[8]+=1\n  if e>90 and e<=100:\n    lung[9]+=1\n  if e>100 and e<=110:\n    lung[10]+=1\n  if e>110 and e<=120:\n    lung[11]+=1\n  if e>120 and e<=130:\n    lung[12]+=1\n  if e>130 and e<=140:\n    lung[13]+=1\n  if e>140 and e<=150:\n    lung[14]+=1\n  if e>150:\n    lung[15]+=1"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"30aec578-34f3-43b5-b837-0d1065312ee6"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n\ninds=range(len(lung))\nlabels=[\"0-10\",\"11-20\",\"21-30\",\"31-40\",\"41-50\",\"51-60\",\"61-70\",\"71-80\",\"81-90\",\"91-100\",\"101-110\",\"111-120\",\"121-130\",\"131-140\",\"141-150\",\"150+\"]\n\nfig,ax = plt.subplots()\nplt.xticks(rotation=90)\nrects = ax.bar(inds, lung)\nax.set_xticks([ind for ind in inds])\nax.set_xticklabels(labels)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"373c6424-99f9-459a-aa66-2d18ca04a8fc"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[70]: [Text(0, 0, '0-10'),\n Text(1, 0, '11-20'),\n Text(2, 0, '21-30'),\n Text(3, 0, '31-40'),\n Text(4, 0, '41-50'),\n Text(5, 0, '51-60'),\n Text(6, 0, '61-70'),\n Text(7, 0, '71-80'),\n Text(8, 0, '81-90'),\n Text(9, 0, '91-100'),\n Text(10, 0, '101-110'),\n Text(11, 0, '111-120'),\n Text(12, 0, '121-130'),\n Text(13, 0, '131-140'),\n Text(14, 0, '141-150'),\n Text(15, 0, '150+')]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[70]: [Text(0, 0, '0-10'),\n Text(1, 0, '11-20'),\n Text(2, 0, '21-30'),\n Text(3, 0, '31-40'),\n Text(4, 0, '41-50'),\n Text(5, 0, '51-60'),\n Text(6, 0, '61-70'),\n Text(7, 0, '71-80'),\n Text(8, 0, '81-90'),\n Text(9, 0, '91-100'),\n Text(10, 0, '101-110'),\n Text(11, 0, '111-120'),\n Text(12, 0, '121-130'),\n Text(13, 0, '131-140'),\n Text(14, 0, '141-150'),\n Text(15, 0, '150+')]"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"data:image/png;base64,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\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"image","arguments":{}}},"output_type":"display_data","data":{"image/png":"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\n"}}],"execution_count":0},{"cell_type":"markdown","source":["Most games are around 70 moves, let's see what is the exact average length of the games"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2678bf5a-ce9a-4713-bffb-71f94dce3119"}}},{"cell_type":"code","source":["medium=df.agg({\"len\": \"avg\"}).collect()[0][0]\nmedium=int(medium)\nprint(\"The average length of the matches in the dataset is\" , medium)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e4ed6140-6ea6-4c06-af81-a0d5b62a6b35"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"The average length of the matches in the dataset is 75\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["The average length of the matches in the dataset is 75\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["Calculation of the maximum match length present in the dataset"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"df5b2f8c-0add-48ff-8a70-8f677a684f4f"}}},{"cell_type":"code","source":["maxgame = df.agg({\"len\": \"max\"}).collect()[0][0]\nprint(\"the longest game is\", maxgame, \"moves\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"211e1214-62e5-499e-9b3f-1fe211739b6f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"the longest game is 600 moves\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["the longest game is 600 moves\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["Assign a minimum value for which a match can be considered. The average number of moves in a chess opening is 10 moves, 5 per player. So I take the games that have at least finished the opening phase"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f4d51d4c-5e66-4aee-97c6-71a50d889ce4"}}},{"cell_type":"code","source":["mingame=10"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"299f59a2-2b95-48cd-9bf1-023c351fa018"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Filtered the matches with minimum 10 moves and maximum the average length."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"82eb2318-6c96-4701-9249-2c07f7cc73f5"}}},{"cell_type":"code","source":["df=df[(df['len'] <= medium ) & (df['len'] >= mingame ) ]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"72766a00-a101-43bf-be16-04111c131a78"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["In the following cell all filtered matches have been collected and ready to be used"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"18e558f3-f8a9-4f64-9211-d7e84a72e392"}}},{"cell_type":"code","source":["game=df.select(\"game\").collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d92c1266-43ca-401a-b0f5-a8852c328d93"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# Section 2: First approach\n<p> In the first approach, the granularity of the moves used will be very raw. Moves are considered only for the piece in question and not for the entirety of the move. At the end of the train such a model must predict what kind of piece we need to move. </p>\n\n<p>To arrive at the sliding window suitable for this approach as first we take the <b>whole matches</b> as they are in the dataset and strip them of the information of the turns and the player in question. Once this is done we have the <b>parsed game</b>. Next we have to replace the moves only with the affected pieces and we have the <b>game with only the pieces</b>. Next we divide the game with only the pieces using the sliding window technique. we divide the game into vectors of 5 elements and use the sixth element as the label. We continue moving the vector to the right until the game ends.</p>\n\n\n<img src=\"https://www.dropbox.com/s/az8cudd0ifbl090/FirstApproach.jpg?dl=1\">\n\n\n<p>The approach used is this; strings are used in the figure for simplicity and clarity. In the actual approach, once we get the match with only the pieces, the pieces are encoded with integers according to a dictionary. Thus to have a list of integers.\nGiven a dictionary of this type: {\"p\" : 0, \"n\" : 1, \"b\" : 2, \"r\" : 3, \"q\" : 4, \"k\" : 5}, the match given in the example above will be: \"00041401\". The sliding window consequently will be considered with integers and not with strings.</p>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"60414eba-3ed1-4a1c-bb36-2d3025969989"}}},{"cell_type":"markdown","source":["In the next cell there is the dictionary used in this approach"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"406eb1e2-64dd-4f88-a885-f681aebaef45"}}},{"cell_type":"code","source":["dic = {\"p\" : 0, \"n\" : 1, \"b\" : 2, \"r\" : 3, \"q\" : 4, \"k\" : 5, \"fill\":6}"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ac71497d-ffc7-4caa-9c98-ee58d949e4e2"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Function for parse the moves and codify the games. This function take in input a string that identify a move and return the pice related"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5e1f2f2e-feb4-4017-b21f-497b3cd1739c"}}},{"cell_type":"code","source":["def piece(m):\n  if m.startswith('O-O'):\n    return \"k\"\n  if m[0].isupper():\n    if m[0].lower() in dic.keys():\n      return m[0].lower()\n  else:\n    return \"p\"\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c6d902e7-7ab3-4200-9c7c-a5038bc046d8"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Definition of n_steps. This value is used for define the timestep dimension for the slide window"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b82b530e-5913-472c-8658-886bae69db3c"}}},{"cell_type":"code","source":["n_steps=5"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"936df6f2-27c3-4c92-8383-70adc2a0a9f2"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["In the next cell we read the filtered matches and for each move we go to clean the data by removing in move number related to the match and then pass the cleaned move to the piece() function. <br>\n Then we padding the moves in the match by adding a null move until we get all the matches to the same length. With the padding added, all that remains is to divide the moves into slide windows of 5 timesteps. \nThe slide window division is saved within file \"X\". In file \"Y\" the related labels are stored."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b2b1070d-398c-433e-87f4-5833107f6d7a"}}},{"cell_type":"code","source":["X_file = open(\"/tmp/X_slide_windows_10-75_5steps.txt\", \"w+\")\nY_file = open(\"/tmp/Y_slide_windows_10-75_5steps.txt\", \"w+\")\nconta=0\nfor el in game:\n  lista=[]\n  if el[0]!=None:\n    li=el[0].split(\" \")\n    for y in li:\n      if y != '':\n        z=y.split('.')[1]\n        if z!='':\n          lista.append(int(dic[piece(z)]))\n  \n  n = medium-len(lista)\n  lfill=[int(dic[\"fill\"]) for i in range(0,n)]\n  lista=lista+lfill\n  seq=lista\n  for i in range(len(seq)):\n        #get the last index\n        lastIndex = i + n_steps\n        #if lastIndex is greater than length of sequence then break\n        if lastIndex > len(seq) - 1:\n            break\n        #Create input and output sequence\n        seq_X, seq_y = seq[i:lastIndex], seq[lastIndex]\n        X_file.write(str(seq_X))\n        X_file.write('\\n')\n        Y_file.write(str(seq_y))\n        Y_file.write('\\n')\n        pass\n  if conta%10000==0:\n    print(conta)\n  conta+=1\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4321831f-a2bc-4f42-8b43-029c8f89ed30"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"0\n10000\n20000\n30000\n40000\n50000\n60000\n70000\n80000\n90000\n100000\n110000\n120000\n130000\n140000\n150000\n160000\n170000\n180000\n190000\n200000\n210000\n220000\n230000\n240000\n250000\n260000\n270000\n280000\n290000\n300000\n310000\n320000\n330000\n340000\n350000\n360000\n370000\n380000\n390000\n400000\n410000\n420000\n430000\n440000\n450000\n460000\n470000\n480000\n490000\n500000\n510000\n520000\n530000\n540000\n550000\n560000\n570000\n580000\n590000\n600000\n610000\n620000\n630000\n640000\n650000\n660000\n670000\n680000\n690000\n700000\n710000\n720000\n730000\n740000\n750000\n760000\n770000\n780000\n790000\n800000\n810000\n820000\n830000\n840000\n850000\n860000\n870000\n880000\n890000\n900000\n910000\n920000\n930000\n940000\n950000\n960000\n970000\n980000\n990000\n1000000\n1010000\n1020000\n1030000\n1040000\n1050000\n1060000\n1070000\n1080000\n1090000\n1100000\n1110000\n1120000\n1130000\n1140000\n1150000\n1160000\n1170000\n1180000\n1190000\n1200000\n1210000\n1220000\n1230000\n1240000\n1250000\n1260000\n1270000\n1280000\n1290000\n1300000\n1310000\n1320000\n1330000\n1340000\n1350000\n1360000\n1370000\n1380000\n1390000\n1400000\n1410000\n1420000\n1430000\n1440000\n1450000\n1460000\n1470000\n1480000\n1490000\n1500000\n1510000\n1520000\n1530000\n1540000\n1550000\n1560000\n1570000\n1580000\n1590000\n1600000\n1610000\n1620000\n1630000\n1640000\n1650000\n1660000\n1670000\n1680000\n1690000\n1700000\n1710000\n1720000\n1730000\n1740000\n1750000\n1760000\n1770000\n1780000\n1790000\n1800000\n1810000\n1820000\n1830000\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["0\n10000\n20000\n30000\n40000\n50000\n60000\n70000\n80000\n90000\n100000\n110000\n120000\n130000\n140000\n150000\n160000\n170000\n180000\n190000\n200000\n210000\n220000\n230000\n240000\n250000\n260000\n270000\n280000\n290000\n300000\n310000\n320000\n330000\n340000\n350000\n360000\n370000\n380000\n390000\n400000\n410000\n420000\n430000\n440000\n450000\n460000\n470000\n480000\n490000\n500000\n510000\n520000\n530000\n540000\n550000\n560000\n570000\n580000\n590000\n600000\n610000\n620000\n630000\n640000\n650000\n660000\n670000\n680000\n690000\n700000\n710000\n720000\n730000\n740000\n750000\n760000\n770000\n780000\n790000\n800000\n810000\n820000\n830000\n840000\n850000\n860000\n870000\n880000\n890000\n900000\n910000\n920000\n930000\n940000\n950000\n960000\n970000\n980000\n990000\n1000000\n1010000\n1020000\n1030000\n1040000\n1050000\n1060000\n1070000\n1080000\n1090000\n1100000\n1110000\n1120000\n1130000\n1140000\n1150000\n1160000\n1170000\n1180000\n1190000\n1200000\n1210000\n1220000\n1230000\n1240000\n1250000\n1260000\n1270000\n1280000\n1290000\n1300000\n1310000\n1320000\n1330000\n1340000\n1350000\n1360000\n1370000\n1380000\n1390000\n1400000\n1410000\n1420000\n1430000\n1440000\n1450000\n1460000\n1470000\n1480000\n1490000\n1500000\n1510000\n1520000\n1530000\n1540000\n1550000\n1560000\n1570000\n1580000\n1590000\n1600000\n1610000\n1620000\n1630000\n1640000\n1650000\n1660000\n1670000\n1680000\n1690000\n1700000\n1710000\n1720000\n1730000\n1740000\n1750000\n1760000\n1770000\n1780000\n1790000\n1800000\n1810000\n1820000\n1830000\n"]}}],"execution_count":0},{"cell_type":"code","source":["X_file.close()\nY_file.close()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"92320f5e-f248-4bb0-9e7b-1d6a1a357c44"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["In the following cells, the X and Y failes are saved in the DBFS for exporting"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b0d3ace3-60a2-444c-921b-968d23eb5997"}}},{"cell_type":"code","source":["dbutils.fs.cp(\"file:/tmp/X_slide_windows_10-75_5steps.txt\", \"dbfs:/tmp/X_slide_windows_10-75_5steps.txt\")\ndisplay(dbutils.fs.ls(\"/tmp/X_slide_windows_10-75_5steps.txt\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7a5c2f23-ad86-4b28-a981-dc68159d7e03"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["dbfs:/tmp/X_slide_windows_10-75_5steps.txt","X_slide_windows_10-75_5steps.txt",2053496832,1652371564000]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"size","type":"\"long\"","metadata":"{}"},{"name":"modificationTime","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/tmp/X_slide_windows_10-75_5steps.txt</td><td>X_slide_windows_10-75_5steps.txt</td><td>2053496832</td><td>1652371564000</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.cp(\"file:/tmp/Y_slide_windows_10-75_5steps.txt\", \"dbfs:/tmp/Y_slide_windows_10-75_5steps.txt\")\ndisplay(dbutils.fs.ls(\"/tmp/Y_slide_windows_10-75_5steps.txt\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f70467a6-1509-47ef-853b-1a985caf48dd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["dbfs:/tmp/Y_slide_windows_10-75_5steps.txt","Y_slide_windows_10-75_5steps.txt",256679936,1652371600000]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"size","type":"\"long\"","metadata":"{}"},{"name":"modificationTime","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/tmp/Y_slide_windows_10-75_5steps.txt</td><td>Y_slide_windows_10-75_5steps.txt</td><td>256679936</td><td>1652371600000</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.cp(\"dbfs:/tmp/X_slide_windows_10-75_5steps.txt\", \"/FileStore/X_slide_windows_10-75_5steps.txt\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"db815833-6cdf-4705-b2db-b234cc98c745"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[18]: True","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[18]: True"]}}],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.cp(\"dbfs:/tmp/Y_slide_windows_10-75_5steps.txt\", \"/FileStore/Y_slide_windows_10-75_5steps.txt\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2757fb46-bc5e-4047-918f-206240fb2250"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[17]: True","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[17]: True"]}}],"execution_count":0},{"cell_type":"markdown","source":["Let's read the saved files"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7af5cf0d-79b1-4e70-a397-b8d460bf8845"}}},{"cell_type":"code","source":["'''\nThe first 300000 matches have been considered for this approach. the dataset was limited in order not to get into memory problems with databricks. In fact, by trying to use more matches, the memory of dabricks exhausted.\nUsing 300000 and dividing them into 5 timesteps we have 21000000 vectors and 21000000 labels.\n'''\nmatches=300000\nsteps_per_match=70\nn_steps=5"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b090401e-47c1-4d71-9259-c76fa5c68f74"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.cp(\"dbfs:/tmp/X_slide_windows_10-75_5steps.txt\", \"file:/tmp/X_slide_windows_10-75_5steps.txt\")\nx=[]\nc=0\nfor line in open(\"/tmp/X_slide_windows_10-75_5steps.txt\", \"r\"):\n  stripped_line = line.strip()\n  st=stripped_line.replace(\"[\",\"\").replace(\"]\",\"\")\n  s=st.split(\",\")\n  s=[int(x) for x in s]\n  x.append(s)\n  if c==(matches*steps_per_match)-1:\n    break\n  c+=1\nx = np.array(x, dtype=int)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0557d6f2-d7e6-44d8-91a7-e2a25685f09a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.cp(\"dbfs:/tmp/Y_slide_windows_10-75_5steps.txt\", \"file:/tmp/Y_slide_windows_10-75_5steps.txt\")\ny=[]\nc=0\nfor line in open(\"/tmp/Y_slide_windows_10-75_5steps.txt\", \"r\"):\n  stripped_line = line.strip()\n  y.append(int(stripped_line))\n  if c==(matches*steps_per_match)-1:\n    break\n  c+=1\ny = np.array(y, dtype=int)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"650d06b3-5e2b-4c43-ab82-89081b19e543"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["print(len(x))\nprint(len(y))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"03458b59-8942-4d3b-aa78-8592b8809227"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"21000000\n21000000\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["21000000\n21000000\n"]}}],"execution_count":0},{"cell_type":"code","source":["nmoves=len(dic)-1\nprint(\"In the dictionary there are\", nmoves, \"moves\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d788eb5b-bb8b-4e77-b47a-1b5bf07ff946"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"In the dictionary there are 6 moves\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["In the dictionary there are 6 moves\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["In the following cells, the train and test set are created. 20% of the matches are used for the test set."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"911db9b8-ed13-4279-9997-e808b4712254"}}},{"cell_type":"code","source":["print(\"TRAIN X\")\ntrain_x= x[:int(len(x)*0.8)]\nprint(\"The number of elements in train_x is\",len(train_x), \"(80% of the dataset)\")\n\nprint(\"TRAIN Y\")\ntrain_y = y[:int(len(y)*0.8)]\nprint(\"The number of elements in train_y is\",len(train_y), \"(80% of the dataset)\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aaa895b2-4f54-4f38-9c9f-e8c1fe62f5f2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"TRAIN X\nThe number of elements in train_x is 16800000 (80% of the dataset)\nTRAIN Y\nThe number of elements in train_y is 16800000 (80% of the dataset)\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["TRAIN X\nThe number of elements in train_x is 16800000 (80% of the dataset)\nTRAIN Y\nThe number of elements in train_y is 16800000 (80% of the dataset)\n"]}}],"execution_count":0},{"cell_type":"code","source":["print(\"TEST X\")\ntest_x = x[-int(len(x)*0.2):] \nprint(\"The number of elements in test_x is\",len(test_x), \"(20% of the dataset)\")\n\nprint(\"TEST Y\")\ntest_y= y[-int(len(y)*0.2):] \nprint(\"The number of elements in test_y is\",len(test_y), \"(20% of the dataset)\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4c9dc3eb-e341-4b77-8732-b919e0d0802a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"TEST X\nThe number of elements in test_x is 4200000 (20% of the dataset)\nTEST Y\nThe number of elements in test_y is 4200000 (20% of the dataset)\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["TEST X\nThe number of elements in test_x is 4200000 (20% of the dataset)\nTEST Y\nThe number of elements in test_y is 4200000 (20% of the dataset)\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["In this cell we see how the data are structured"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"59a10e86-b595-43f1-8d6d-c44f513c8344"}}},{"cell_type":"code","source":["for i in range(0,len(test_x[:70])):\n  print(test_x[i], \"  \" , test_y[i])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"26beb8d5-1774-4256-8f02-01991c2f160c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"[0 0 0 0 0]    0\n[0 0 0 0 0]    0\n[0 0 0 0 0]    4\n[0 0 0 0 4]    1\n[0 0 0 4 1]    1\n[0 0 4 1 1]    0\n[0 4 1 1 0]    0\n[4 1 1 0 0]    1\n[1 1 0 0 1]    1\n[1 0 0 1 1]    2\n[0 0 1 1 2]    2\n[0 1 1 2 2]    5\n[1 1 2 2 5]    1\n[1 2 2 5 1]    3\n[2 2 5 1 3]    4\n[2 5 1 3 4]    3\n[5 1 3 4 3]    1\n[1 3 4 3 1]    1\n[3 4 3 1 1]    1\n[4 3 1 1 1]    2\n[3 1 1 1 2]    1\n[1 1 1 2 1]    1\n[1 1 2 1 1]    1\n[1 2 1 1 1]    1\n[2 1 1 1 1]    1\n[1 1 1 1 1]    1\n[1 1 1 1 1]    2\n[1 1 1 1 2]    2\n[1 1 1 2 2]    0\n[1 1 2 2 0]    0\n[1 2 2 0 0]    2\n[2 2 0 0 2]    0\n[2 0 0 2 0]    5\n[0 0 2 0 5]    4\n[0 2 0 5 4]    5\n[2 0 5 4 5]    0\n[0 5 4 5 0]    5\n[5 4 5 0 5]    2\n[4 5 0 5 2]    3\n[5 0 5 2 3]    2\n[0 5 2 3 2]    4\n[5 2 3 2 4]    1\n[2 3 2 4 1]    2\n[3 2 4 1 2]    1\n[2 4 1 2 1]    0\n[4 1 2 1 0]    0\n[1 2 1 0 0]    0\n[2 1 0 0 0]    0\n[1 0 0 0 0]    0\n[0 0 0 0 0]    1\n[0 0 0 0 1]    0\n[0 0 0 1 0]    1\n[0 0 1 0 1]    0\n[0 1 0 1 0]    2\n[1 0 1 0 2]    0\n[0 1 0 2 0]    2\n[1 0 2 0 2]    2\n[0 2 0 2 2]    5\n[2 0 2 2 5]    2\n[0 2 2 5 2]    3\n[2 2 5 2 3]    2\n[2 5 2 3 2]    3\n[5 2 3 2 3]    2\n[2 3 2 3 2]    1\n[3 2 3 2 1]    4\n[2 3 2 1 4]    5\n[3 2 1 4 5]    1\n[2 1 4 5 1]    2\n[1 4 5 1 2]    3\n[4 5 1 2 3]    6\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["[0 0 0 0 0]    0\n[0 0 0 0 0]    0\n[0 0 0 0 0]    4\n[0 0 0 0 4]    1\n[0 0 0 4 1]    1\n[0 0 4 1 1]    0\n[0 4 1 1 0]    0\n[4 1 1 0 0]    1\n[1 1 0 0 1]    1\n[1 0 0 1 1]    2\n[0 0 1 1 2]    2\n[0 1 1 2 2]    5\n[1 1 2 2 5]    1\n[1 2 2 5 1]    3\n[2 2 5 1 3]    4\n[2 5 1 3 4]    3\n[5 1 3 4 3]    1\n[1 3 4 3 1]    1\n[3 4 3 1 1]    1\n[4 3 1 1 1]    2\n[3 1 1 1 2]    1\n[1 1 1 2 1]    1\n[1 1 2 1 1]    1\n[1 2 1 1 1]    1\n[2 1 1 1 1]    1\n[1 1 1 1 1]    1\n[1 1 1 1 1]    2\n[1 1 1 1 2]    2\n[1 1 1 2 2]    0\n[1 1 2 2 0]    0\n[1 2 2 0 0]    2\n[2 2 0 0 2]    0\n[2 0 0 2 0]    5\n[0 0 2 0 5]    4\n[0 2 0 5 4]    5\n[2 0 5 4 5]    0\n[0 5 4 5 0]    5\n[5 4 5 0 5]    2\n[4 5 0 5 2]    3\n[5 0 5 2 3]    2\n[0 5 2 3 2]    4\n[5 2 3 2 4]    1\n[2 3 2 4 1]    2\n[3 2 4 1 2]    1\n[2 4 1 2 1]    0\n[4 1 2 1 0]    0\n[1 2 1 0 0]    0\n[2 1 0 0 0]    0\n[1 0 0 0 0]    0\n[0 0 0 0 0]    1\n[0 0 0 0 1]    0\n[0 0 0 1 0]    1\n[0 0 1 0 1]    0\n[0 1 0 1 0]    2\n[1 0 1 0 2]    0\n[0 1 0 2 0]    2\n[1 0 2 0 2]    2\n[0 2 0 2 2]    5\n[2 0 2 2 5]    2\n[0 2 2 5 2]    3\n[2 2 5 2 3]    2\n[2 5 2 3 2]    3\n[5 2 3 2 3]    2\n[2 3 2 3 2]    1\n[3 2 3 2 1]    4\n[2 3 2 1 4]    5\n[3 2 1 4 5]    1\n[2 1 4 5 1]    2\n[1 4 5 1 2]    3\n[4 5 1 2 3]    6\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["In the following cell, One Hot Encoding is applied on the labels of the train set."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d337aa94-0737-4066-91bc-9dd55b5fb7c9"}}},{"cell_type":"code","source":["train_y = to_categorical(train_y)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0d75c6f4-e88b-4b7e-afb4-5267d1136041"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["In this cell we see how the data are structured after the One Hot Encodind"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b16fe411-4140-4d26-998f-4d57da979518"}}},{"cell_type":"code","source":["for i in range(0,len(train_x[:70])):\n  print(train_x[i], \"  \" , train_y[i])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"98e38282-ca38-436c-8f12-6257b5c65f43"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"[0 0 0 0 1]    [0. 1. 0. 0. 0. 0. 0.]\n[0 0 0 1 1]    [1. 0. 0. 0. 0. 0. 0.]\n[0 0 1 1 0]    [1. 0. 0. 0. 0. 0. 0.]\n[0 1 1 0 0]    [0. 0. 1. 0. 0. 0. 0.]\n[1 1 0 0 2]    [0. 0. 1. 0. 0. 0. 0.]\n[1 0 0 2 2]    [1. 0. 0. 0. 0. 0. 0.]\n[0 0 2 2 0]    [0. 1. 0. 0. 0. 0. 0.]\n[0 2 2 0 1]    [0. 0. 1. 0. 0. 0. 0.]\n[2 2 0 1 2]    [0. 1. 0. 0. 0. 0. 0.]\n[2 0 1 2 1]    [0. 0. 1. 0. 0. 0. 0.]\n[0 1 2 1 2]    [0. 1. 0. 0. 0. 0. 0.]\n[1 2 1 2 1]    [0. 0. 1. 0. 0. 0. 0.]\n[2 1 2 1 2]    [0. 1. 0. 0. 0. 0. 0.]\n[1 2 1 2 1]    [0. 0. 0. 1. 0. 0. 0.]\n[2 1 2 1 3]    [0. 1. 0. 0. 0. 0. 0.]\n[1 2 1 3 1]    [0. 0. 1. 0. 0. 0. 0.]\n[2 1 3 1 2]    [1. 0. 0. 0. 0. 0. 0.]\n[1 3 1 2 0]    [0. 1. 0. 0. 0. 0. 0.]\n[3 1 2 0 1]    [0. 0. 0. 0. 0. 1. 0.]\n[1 2 0 1 5]    [0. 1. 0. 0. 0. 0. 0.]\n[2 0 1 5 1]    [1. 0. 0. 0. 0. 0. 0.]\n[0 1 5 1 0]    [1. 0. 0. 0. 0. 0. 0.]\n[1 5 1 0 0]    [0. 1. 0. 0. 0. 0. 0.]\n[5 1 0 0 1]    [0. 0. 1. 0. 0. 0. 0.]\n[1 0 0 1 2]    [0. 0. 0. 1. 0. 0. 0.]\n[0 0 1 2 3]    [0. 0. 1. 0. 0. 0. 0.]\n[0 1 2 3 2]    [0. 0. 1. 0. 0. 0. 0.]\n[1 2 3 2 2]    [1. 0. 0. 0. 0. 0. 0.]\n[2 3 2 2 0]    [1. 0. 0. 0. 0. 0. 0.]\n[3 2 2 0 0]    [0. 0. 0. 0. 0. 1. 0.]\n[2 2 0 0 5]    [0. 1. 0. 0. 0. 0. 0.]\n[2 0 0 5 1]    [0. 0. 0. 1. 0. 0. 0.]\n[0 0 5 1 3]    [0. 0. 1. 0. 0. 0. 0.]\n[0 5 1 3 2]    [0. 0. 0. 0. 0. 1. 0.]\n[5 1 3 2 5]    [0. 0. 0. 0. 0. 1. 0.]\n[1 3 2 5 5]    [1. 0. 0. 0. 0. 0. 0.]\n[3 2 5 5 0]    [1. 0. 0. 0. 0. 0. 0.]\n[2 5 5 0 0]    [1. 0. 0. 0. 0. 0. 0.]\n[5 5 0 0 0]    [1. 0. 0. 0. 0. 0. 0.]\n[5 0 0 0 0]    [0. 0. 0. 1. 0. 0. 0.]\n[0 0 0 0 3]    [0. 0. 0. 1. 0. 0. 0.]\n[0 0 0 3 3]    [1. 0. 0. 0. 0. 0. 0.]\n[0 0 3 3 0]    [0. 0. 1. 0. 0. 0. 0.]\n[0 3 3 0 2]    [1. 0. 0. 0. 0. 0. 0.]\n[3 3 0 2 0]    [1. 0. 0. 0. 0. 0. 0.]\n[3 0 2 0 0]    [1. 0. 0. 0. 0. 0. 0.]\n[0 2 0 0 0]    [0. 1. 0. 0. 0. 0. 0.]\n[2 0 0 0 1]    [0. 1. 0. 0. 0. 0. 0.]\n[0 0 0 1 1]    [0. 0. 1. 0. 0. 0. 0.]\n[0 0 1 1 2]    [0. 0. 0. 1. 0. 0. 0.]\n[0 1 1 2 3]    [1. 0. 0. 0. 0. 0. 0.]\n[1 1 2 3 0]    [1. 0. 0. 0. 0. 0. 0.]\n[1 2 3 0 0]    [0. 0. 0. 1. 0. 0. 0.]\n[2 3 0 0 3]    [0. 0. 0. 1. 0. 0. 0.]\n[3 0 0 3 3]    [0. 0. 0. 0. 0. 1. 0.]\n[0 0 3 3 5]    [0. 0. 0. 1. 0. 0. 0.]\n[0 3 3 5 3]    [0. 0. 1. 0. 0. 0. 0.]\n[3 3 5 3 2]    [0. 0. 0. 1. 0. 0. 0.]\n[3 5 3 2 3]    [0. 0. 0. 0. 0. 1. 0.]\n[5 3 2 3 5]    [0. 0. 0. 1. 0. 0. 0.]\n[3 2 3 5 3]    [0. 0. 1. 0. 0. 0. 0.]\n[2 3 5 3 2]    [0. 0. 0. 1. 0. 0. 0.]\n[3 5 3 2 3]    [0. 0. 0. 0. 0. 0. 1.]\n[5 3 2 3 6]    [0. 0. 0. 0. 0. 0. 1.]\n[3 2 3 6 6]    [0. 0. 0. 0. 0. 0. 1.]\n[2 3 6 6 6]    [0. 0. 0. 0. 0. 0. 1.]\n[3 6 6 6 6]    [0. 0. 0. 0. 0. 0. 1.]\n[6 6 6 6 6]    [0. 0. 0. 0. 0. 0. 1.]\n[6 6 6 6 6]    [0. 0. 0. 0. 0. 0. 1.]\n[6 6 6 6 6]    [0. 0. 0. 0. 0. 0. 1.]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["[0 0 0 0 1]    [0. 1. 0. 0. 0. 0. 0.]\n[0 0 0 1 1]    [1. 0. 0. 0. 0. 0. 0.]\n[0 0 1 1 0]    [1. 0. 0. 0. 0. 0. 0.]\n[0 1 1 0 0]    [0. 0. 1. 0. 0. 0. 0.]\n[1 1 0 0 2]    [0. 0. 1. 0. 0. 0. 0.]\n[1 0 0 2 2]    [1. 0. 0. 0. 0. 0. 0.]\n[0 0 2 2 0]    [0. 1. 0. 0. 0. 0. 0.]\n[0 2 2 0 1]    [0. 0. 1. 0. 0. 0. 0.]\n[2 2 0 1 2]    [0. 1. 0. 0. 0. 0. 0.]\n[2 0 1 2 1]    [0. 0. 1. 0. 0. 0. 0.]\n[0 1 2 1 2]    [0. 1. 0. 0. 0. 0. 0.]\n[1 2 1 2 1]    [0. 0. 1. 0. 0. 0. 0.]\n[2 1 2 1 2]    [0. 1. 0. 0. 0. 0. 0.]\n[1 2 1 2 1]    [0. 0. 0. 1. 0. 0. 0.]\n[2 1 2 1 3]    [0. 1. 0. 0. 0. 0. 0.]\n[1 2 1 3 1]    [0. 0. 1. 0. 0. 0. 0.]\n[2 1 3 1 2]    [1. 0. 0. 0. 0. 0. 0.]\n[1 3 1 2 0]    [0. 1. 0. 0. 0. 0. 0.]\n[3 1 2 0 1]    [0. 0. 0. 0. 0. 1. 0.]\n[1 2 0 1 5]    [0. 1. 0. 0. 0. 0. 0.]\n[2 0 1 5 1]    [1. 0. 0. 0. 0. 0. 0.]\n[0 1 5 1 0]    [1. 0. 0. 0. 0. 0. 0.]\n[1 5 1 0 0]    [0. 1. 0. 0. 0. 0. 0.]\n[5 1 0 0 1]    [0. 0. 1. 0. 0. 0. 0.]\n[1 0 0 1 2]    [0. 0. 0. 1. 0. 0. 0.]\n[0 0 1 2 3]    [0. 0. 1. 0. 0. 0. 0.]\n[0 1 2 3 2]    [0. 0. 1. 0. 0. 0. 0.]\n[1 2 3 2 2]    [1. 0. 0. 0. 0. 0. 0.]\n[2 3 2 2 0]    [1. 0. 0. 0. 0. 0. 0.]\n[3 2 2 0 0]    [0. 0. 0. 0. 0. 1. 0.]\n[2 2 0 0 5]    [0. 1. 0. 0. 0. 0. 0.]\n[2 0 0 5 1]    [0. 0. 0. 1. 0. 0. 0.]\n[0 0 5 1 3]    [0. 0. 1. 0. 0. 0. 0.]\n[0 5 1 3 2]    [0. 0. 0. 0. 0. 1. 0.]\n[5 1 3 2 5]    [0. 0. 0. 0. 0. 1. 0.]\n[1 3 2 5 5]    [1. 0. 0. 0. 0. 0. 0.]\n[3 2 5 5 0]    [1. 0. 0. 0. 0. 0. 0.]\n[2 5 5 0 0]    [1. 0. 0. 0. 0. 0. 0.]\n[5 5 0 0 0]    [1. 0. 0. 0. 0. 0. 0.]\n[5 0 0 0 0]    [0. 0. 0. 1. 0. 0. 0.]\n[0 0 0 0 3]    [0. 0. 0. 1. 0. 0. 0.]\n[0 0 0 3 3]    [1. 0. 0. 0. 0. 0. 0.]\n[0 0 3 3 0]    [0. 0. 1. 0. 0. 0. 0.]\n[0 3 3 0 2]    [1. 0. 0. 0. 0. 0. 0.]\n[3 3 0 2 0]    [1. 0. 0. 0. 0. 0. 0.]\n[3 0 2 0 0]    [1. 0. 0. 0. 0. 0. 0.]\n[0 2 0 0 0]    [0. 1. 0. 0. 0. 0. 0.]\n[2 0 0 0 1]    [0. 1. 0. 0. 0. 0. 0.]\n[0 0 0 1 1]    [0. 0. 1. 0. 0. 0. 0.]\n[0 0 1 1 2]    [0. 0. 0. 1. 0. 0. 0.]\n[0 1 1 2 3]    [1. 0. 0. 0. 0. 0. 0.]\n[1 1 2 3 0]    [1. 0. 0. 0. 0. 0. 0.]\n[1 2 3 0 0]    [0. 0. 0. 1. 0. 0. 0.]\n[2 3 0 0 3]    [0. 0. 0. 1. 0. 0. 0.]\n[3 0 0 3 3]    [0. 0. 0. 0. 0. 1. 0.]\n[0 0 3 3 5]    [0. 0. 0. 1. 0. 0. 0.]\n[0 3 3 5 3]    [0. 0. 1. 0. 0. 0. 0.]\n[3 3 5 3 2]    [0. 0. 0. 1. 0. 0. 0.]\n[3 5 3 2 3]    [0. 0. 0. 0. 0. 1. 0.]\n[5 3 2 3 5]    [0. 0. 0. 1. 0. 0. 0.]\n[3 2 3 5 3]    [0. 0. 1. 0. 0. 0. 0.]\n[2 3 5 3 2]    [0. 0. 0. 1. 0. 0. 0.]\n[3 5 3 2 3]    [0. 0. 0. 0. 0. 0. 1.]\n[5 3 2 3 6]    [0. 0. 0. 0. 0. 0. 1.]\n[3 2 3 6 6]    [0. 0. 0. 0. 0. 0. 1.]\n[2 3 6 6 6]    [0. 0. 0. 0. 0. 0. 1.]\n[3 6 6 6 6]    [0. 0. 0. 0. 0. 0. 1.]\n[6 6 6 6 6]    [0. 0. 0. 0. 0. 0. 1.]\n[6 6 6 6 6]    [0. 0. 0. 0. 0. 0. 1.]\n[6 6 6 6 6]    [0. 0. 0. 0. 0. 0. 1.]\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["In the following cell we define values that will be used for embedding layer of the model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e6c7e932-ea42-44c7-936a-8d30fb5441e6"}}},{"cell_type":"code","source":["seq_len=train_x.shape[1]\nvocab_size=len(dic)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1bf760c5-3051-4642-8c9e-7b17a7c95b1c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Defining a callback to prevent overfitting"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"401f6170-f7c2-4b10-848d-9d01980c9bc3"}}},{"cell_type":"code","source":["from tensorflow.keras.callbacks import EarlyStopping\ncallback = EarlyStopping(monitor='loss', patience=2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3772f35d-8946-4bca-ac86-30f20ee76778"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["In the next cell we have the model used for the train. The train of this model was performed on kaggle. The model uses two hidden LSTM layers with 100 memory cells each and a fully connected dense layer with 100 neurons connects to the hidden LSTM layers to interpret features extracted from the sequence. After the first lstm layer there is a 20% dropout layer to prevent overfitting of the data. The output layer predicts the next word as a single vector of the vocabulary size with a probability for each word in the vocabulary. A softmax activation function is used to ensure that the outputs have the normalized probability features."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5516d66b-6dd2-4496-8a51-dc9480fb3246"}}},{"cell_type":"code","source":["model = Sequential()\nmodel.add(Embedding(vocab_size, n_steps, input_length=seq_len))\nmodel.add(LSTM(100, return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(100))\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dense(vocab_size, activation='softmax'))\nprint(model.summary())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"41291b1f-615f-45cd-b751-2bc9b69df7f4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Model: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n embedding (Embedding)       (None, 5, 5)              35        \n                                                                 \n lstm (LSTM)                 (None, 5, 100)            42400     \n                                                                 \n dropout (Dropout)           (None, 5, 100)            0         \n                                                                 \n lstm_1 (LSTM)               (None, 100)               80400     \n                                                                 \n dense (Dense)               (None, 100)               10100     \n                                                                 \n dense_1 (Dense)             (None, 7)                 707       \n                                                                 \n=================================================================\nTotal params: 133,642\nTrainable params: 133,642\nNon-trainable params: 0\n_________________________________________________________________\nNone\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Model: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n embedding (Embedding)       (None, 5, 5)              35        \n                                                                 \n lstm (LSTM)                 (None, 5, 100)            42400     \n                                                                 \n dropout (Dropout)           (None, 5, 100)            0         \n                                                                 \n lstm_1 (LSTM)               (None, 100)               80400     \n                                                                 \n dense (Dense)               (None, 100)               10100     \n                                                                 \n dense_1 (Dense)             (None, 7)                 707       \n                                                                 \n=================================================================\nTotal params: 133,642\nTrainable params: 133,642\nNon-trainable params: 0\n_________________________________________________________________\nNone\n"]}}],"execution_count":0},{"cell_type":"code","source":["# compile model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n# fit model\nhistory=model.fit(train_x, train_y, batch_size=128, epochs=50, callbacks=[callback])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"029eaac8-d471-4e23-8b9b-e14f1b2e7174"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Epoch 1/50\n\r     1/131250 [..............................] - ETA: 197:53:59 - loss: 1.9457 - accuracy: 0.3438\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r     2/131250 [..............................] - ETA: 2:27:12 - loss: 1.9441 - accuracy: 0.3398  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r     3/131250 [..............................] - ETA: 2:16:13 - loss: 1.9424 - accuracy: 0.3281\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r     5/131250 [..............................] - ETA: 1:50:30 - loss: 1.9385 - accuracy: 0.3266\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r     7/131250 [..............................] - ETA: 1:45:10 - loss: 1.9336 - accuracy: 0.3315\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r     9/131250 [..............................] - ETA: 1:36:57 - loss: 1.9264 - accuracy: 0.3446\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    11/131250 [..............................] - ETA: 1:33:14 - loss: 1.9182 - accuracy: 0.3530\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    12/131250 [..............................] - ETA: 1:35:29 - loss: 1.9134 - accuracy: 0.3548\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    13/131250 [..............................] - ETA: 1:38:27 - loss: 1.9097 - accuracy: 0.3540\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    14/131250 [..............................] - ETA: 1:42:06 - loss: 1.9064 - accuracy: 0.3499\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    15/131250 [..............................] - ETA: 1:44:03 - loss: 1.9014 - accuracy: 0.3495","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Epoch 1/50\n\r     1/131250 [..............................] - ETA: 197:53:59 - loss: 1.9457 - accuracy: 0.3438\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r     2/131250 [..............................] - ETA: 2:27:12 - loss: 1.9441 - accuracy: 0.3398  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r     3/131250 [..............................] - ETA: 2:16:13 - loss: 1.9424 - accuracy: 0.3281\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r     5/131250 [..............................] - ETA: 1:50:30 - loss: 1.9385 - accuracy: 0.3266\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r     7/131250 [..............................] - ETA: 1:45:10 - loss: 1.9336 - accuracy: 0.3315\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r     9/131250 [..............................] - ETA: 1:36:57 - loss: 1.9264 - accuracy: 0.3446\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    11/131250 [..............................] - ETA: 1:33:14 - loss: 1.9182 - accuracy: 0.3530\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    12/131250 [..............................] - ETA: 1:35:29 - loss: 1.9134 - accuracy: 0.3548\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    13/131250 [..............................] - ETA: 1:38:27 - loss: 1.9097 - accuracy: 0.3540\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    14/131250 [..............................] - ETA: 1:42:06 - loss: 1.9064 - accuracy: 0.3499\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    15/131250 [..............................] - ETA: 1:44:03 - loss: 1.9014 - accuracy: 0.3495"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Cancelled","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["We upload the trained model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3f1e3e07-19c1-4b00-ae07-94002a7e5946"}}},{"cell_type":"code","source":["%sh wget https://www.dropbox.com/s/k7thce9p1ed2zi6/model_kaggle_300kgames_50epoch.h5?dl=1 -O /tmp/Kaggle_onlyPiece.csv"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"693c22f4-847d-4c7e-a686-21a564549185"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"--2022-06-04 17:15:41--  https://www.dropbox.com/s/k7thce9p1ed2zi6/model_kaggle_300kgames_50epoch.h5?dl=1\nResolving www.dropbox.com (www.dropbox.com)... 162.125.8.18, 2620:100:6016:18::a27d:112\nConnecting to www.dropbox.com (www.dropbox.com)|162.125.8.18|:443... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: /s/dl/k7thce9p1ed2zi6/model_kaggle_300kgames_50epoch.h5 [following]\n--2022-06-04 17:15:41--  https://www.dropbox.com/s/dl/k7thce9p1ed2zi6/model_kaggle_300kgames_50epoch.h5\nReusing existing connection to www.dropbox.com:443.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://uc31463aa2454811b76e74aa4565.dl.dropboxusercontent.com/cd/0/get/Bmn--rzDH7_zJ5nKIi9fnzpXR1S38DSxzGEmzJ-hiPoQ-ItUqtEUh_XgGzYzXCr2goNBRta2LFcKeKQNReXO6MtjRAdqBpLeDt-Z5cdhUyTbqbuldybHRiMC4W7OiVAQZkzXjqnyn6xx-1W1M3uanbkuYK2BKI09NYXAD0r7uufNWshBiWTs8ZyMr_4Zz0DIIa0/file?dl=1# [following]\n--2022-06-04 17:15:42--  https://uc31463aa2454811b76e74aa4565.dl.dropboxusercontent.com/cd/0/get/Bmn--rzDH7_zJ5nKIi9fnzpXR1S38DSxzGEmzJ-hiPoQ-ItUqtEUh_XgGzYzXCr2goNBRta2LFcKeKQNReXO6MtjRAdqBpLeDt-Z5cdhUyTbqbuldybHRiMC4W7OiVAQZkzXjqnyn6xx-1W1M3uanbkuYK2BKI09NYXAD0r7uufNWshBiWTs8ZyMr_4Zz0DIIa0/file?dl=1\nResolving uc31463aa2454811b76e74aa4565.dl.dropboxusercontent.com (uc31463aa2454811b76e74aa4565.dl.dropboxusercontent.com)... 162.125.8.15, 2620:100:6016:15::a27d:10f\nConnecting to uc31463aa2454811b76e74aa4565.dl.dropboxusercontent.com (uc31463aa2454811b76e74aa4565.dl.dropboxusercontent.com)|162.125.8.15|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1663376 (1.6M) [application/binary]\nSaving to: /tmp/Kaggle_onlyPiece.csv\n\n     0K .......... .......... .......... .......... ..........  3%  501K 3s\n    50K .......... .......... .......... .......... ..........  6% 1007K 2s\n   100K .......... .......... .......... .......... ..........  9% 1.21M 2s\n   150K .......... .......... .......... .......... .......... 12% 1.53M 2s\n   200K .......... .......... .......... .......... .......... 15% 2.93M 1s\n   250K .......... .......... .......... .......... .......... 18% 2.69M 1s\n   300K .......... .......... .......... .......... .......... 21% 3.09M 1s\n   350K .......... .......... .......... .......... .......... 24% 3.16M 1s\n   400K .......... .......... .......... .......... .......... 27% 5.01M 1s\n   450K .......... .......... .......... .......... .......... 30% 5.89M 1s\n   500K .......... .......... .......... .......... .......... 33% 6.59M 1s\n   550K .......... .......... .......... .......... .......... 36% 7.83M 1s\n   600K .......... .......... .......... .......... .......... 40% 7.74M 0s\n   650K .......... .......... .......... .......... .......... 43% 9.72M 0s\n   700K .......... .......... .......... .......... .......... 46% 8.32M 0s\n   750K .......... .......... .......... .......... .......... 49% 6.86M 0s\n   800K .......... .......... .......... .......... .......... 52% 9.81M 0s\n   850K .......... .......... .......... .......... .......... 55% 10.1M 0s\n   900K .......... .......... .......... .......... .......... 58% 9.56M 0s\n   950K .......... .......... .......... .......... .......... 61% 13.3M 0s\n  1000K .......... .......... .......... .......... .......... 64% 12.8M 0s\n  1050K .......... .......... .......... .......... .......... 67% 11.7M 0s\n  1100K .......... .......... .......... .......... .......... 70% 13.1M 0s\n  1150K .......... .......... .......... .......... .......... 73% 11.1M 0s\n  1200K .......... .......... .......... .......... .......... 76% 19.0M 0s\n  1250K .......... .......... .......... .......... .......... 80% 15.3M 0s\n  1300K .......... .......... .......... .......... .......... 83% 16.7M 0s\n  1350K .......... .......... .......... .......... .......... 86% 17.3M 0s\n  1400K .......... .......... .......... .......... .......... 89% 18.6M 0s\n  1450K .......... .......... .......... .......... .......... 92% 26.3M 0s\n  1500K .......... .......... .......... .......... .......... 95% 16.1M 0s\n  1550K .......... .......... .......... .......... .......... 98% 15.5M 0s\n  1600K .......... .......... ....                            100% 24.0M=0.4s\n\n2022-06-04 17:15:43 (3.95 MB/s) - /tmp/Kaggle_onlyPiece.csv saved [1663376/1663376]\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["--2022-06-04 17:15:41--  https://www.dropbox.com/s/k7thce9p1ed2zi6/model_kaggle_300kgames_50epoch.h5?dl=1\nResolving www.dropbox.com (www.dropbox.com)... 162.125.8.18, 2620:100:6016:18::a27d:112\nConnecting to www.dropbox.com (www.dropbox.com)|162.125.8.18|:443... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: /s/dl/k7thce9p1ed2zi6/model_kaggle_300kgames_50epoch.h5 [following]\n--2022-06-04 17:15:41--  https://www.dropbox.com/s/dl/k7thce9p1ed2zi6/model_kaggle_300kgames_50epoch.h5\nReusing existing connection to www.dropbox.com:443.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://uc31463aa2454811b76e74aa4565.dl.dropboxusercontent.com/cd/0/get/Bmn--rzDH7_zJ5nKIi9fnzpXR1S38DSxzGEmzJ-hiPoQ-ItUqtEUh_XgGzYzXCr2goNBRta2LFcKeKQNReXO6MtjRAdqBpLeDt-Z5cdhUyTbqbuldybHRiMC4W7OiVAQZkzXjqnyn6xx-1W1M3uanbkuYK2BKI09NYXAD0r7uufNWshBiWTs8ZyMr_4Zz0DIIa0/file?dl=1# [following]\n--2022-06-04 17:15:42--  https://uc31463aa2454811b76e74aa4565.dl.dropboxusercontent.com/cd/0/get/Bmn--rzDH7_zJ5nKIi9fnzpXR1S38DSxzGEmzJ-hiPoQ-ItUqtEUh_XgGzYzXCr2goNBRta2LFcKeKQNReXO6MtjRAdqBpLeDt-Z5cdhUyTbqbuldybHRiMC4W7OiVAQZkzXjqnyn6xx-1W1M3uanbkuYK2BKI09NYXAD0r7uufNWshBiWTs8ZyMr_4Zz0DIIa0/file?dl=1\nResolving uc31463aa2454811b76e74aa4565.dl.dropboxusercontent.com (uc31463aa2454811b76e74aa4565.dl.dropboxusercontent.com)... 162.125.8.15, 2620:100:6016:15::a27d:10f\nConnecting to uc31463aa2454811b76e74aa4565.dl.dropboxusercontent.com (uc31463aa2454811b76e74aa4565.dl.dropboxusercontent.com)|162.125.8.15|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1663376 (1.6M) [application/binary]\nSaving to: /tmp/Kaggle_onlyPiece.csv\n\n     0K .......... .......... .......... .......... ..........  3%  501K 3s\n    50K .......... .......... .......... .......... ..........  6% 1007K 2s\n   100K .......... .......... .......... .......... ..........  9% 1.21M 2s\n   150K .......... .......... .......... .......... .......... 12% 1.53M 2s\n   200K .......... .......... .......... .......... .......... 15% 2.93M 1s\n   250K .......... .......... .......... .......... .......... 18% 2.69M 1s\n   300K .......... .......... .......... .......... .......... 21% 3.09M 1s\n   350K .......... .......... .......... .......... .......... 24% 3.16M 1s\n   400K .......... .......... .......... .......... .......... 27% 5.01M 1s\n   450K .......... .......... .......... .......... .......... 30% 5.89M 1s\n   500K .......... .......... .......... .......... .......... 33% 6.59M 1s\n   550K .......... .......... .......... .......... .......... 36% 7.83M 1s\n   600K .......... .......... .......... .......... .......... 40% 7.74M 0s\n   650K .......... .......... .......... .......... .......... 43% 9.72M 0s\n   700K .......... .......... .......... .......... .......... 46% 8.32M 0s\n   750K .......... .......... .......... .......... .......... 49% 6.86M 0s\n   800K .......... .......... .......... .......... .......... 52% 9.81M 0s\n   850K .......... .......... .......... .......... .......... 55% 10.1M 0s\n   900K .......... .......... .......... .......... .......... 58% 9.56M 0s\n   950K .......... .......... .......... .......... .......... 61% 13.3M 0s\n  1000K .......... .......... .......... .......... .......... 64% 12.8M 0s\n  1050K .......... .......... .......... .......... .......... 67% 11.7M 0s\n  1100K .......... .......... .......... .......... .......... 70% 13.1M 0s\n  1150K .......... .......... .......... .......... .......... 73% 11.1M 0s\n  1200K .......... .......... .......... .......... .......... 76% 19.0M 0s\n  1250K .......... .......... .......... .......... .......... 80% 15.3M 0s\n  1300K .......... .......... .......... .......... .......... 83% 16.7M 0s\n  1350K .......... .......... .......... .......... .......... 86% 17.3M 0s\n  1400K .......... .......... .......... .......... .......... 89% 18.6M 0s\n  1450K .......... .......... .......... .......... .......... 92% 26.3M 0s\n  1500K .......... .......... .......... .......... .......... 95% 16.1M 0s\n  1550K .......... .......... .......... .......... .......... 98% 15.5M 0s\n  1600K .......... .......... ....                            100% 24.0M=0.4s\n\n2022-06-04 17:15:43 (3.95 MB/s) - /tmp/Kaggle_onlyPiece.csv saved [1663376/1663376]\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["model1_onlyPiece = keras.models.load_model(\"/tmp/Kaggle_onlyPiece.csv\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"68a37c66-0116-40d0-9231-be580676cdb5"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Plotting of the loss and accuracy values during the train"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bdd83636-6cdf-44cd-854b-ef7bd32624b9"}}},{"cell_type":"code","source":["plt.plot(history.history['loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train'], loc='upper right')\nplt.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c9361c51-6942-473a-8fa3-f4c797574484"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["<img src=\"https://www.dropbox.com/s/d2n3ur4fjvx0ety/model1_loss.png?dl=1\">"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ead82058-e2a6-41dd-b13c-2b4d3dad04f2"}}},{"cell_type":"code","source":["plt.plot(history.history['accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train'], loc='upper left')\nplt.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f50fc42c-f347-4a19-815a-e81043d9e8ad"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["<img src=\"https://www.dropbox.com/s/aa8tk4tw5u5fg42/model1_accuracy.png?dl=1\">"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f5205c7d-427e-485e-866f-d8a9efb88084"}}},{"cell_type":"markdown","source":["##### Model trained on paperspace with a slightly different network\nIn the next cell we have the model used for the train performed on Paperspace. The model was trained on a slightly different net from the previous model to try to increase performance and not have all models trained with the same Neural Network. This model uses two hidden LSTM layers with 256 memory cells each a dense layer with 100 neurons to interpret features extracted from the sequence. After each lstm layer there is a 40% dropout layer to prevent overfitting of the data. The output layer predicts the next word as a single vector of the vocabulary size with a probability for each word in the vocabulary. A softmax activation function is used to ensure that the outputs have the normalized probability features."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ae193dab-0e55-4a6b-bca2-2198f3104162"}}},{"cell_type":"code","source":["model = Sequential()\nmodel.add(Embedding(vocab_size, n_steps, input_length=seq_len))\nmodel.add(LSTM(256, return_sequences=True))\nmodel.add(Dropout(0.4))\nmodel.add(LSTM(256))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dense(vocab_size, activation='softmax'))\nprint(model.summary())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c42a3be2-734c-4fa1-9313-097f0019100e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Model: \"sequential_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n embedding_1 (Embedding)     (None, 5, 5)              35        \n                                                                 \n lstm_2 (LSTM)               (None, 5, 256)            268288    \n                                                                 \n dropout (Dropout)           (None, 5, 256)            0         \n                                                                 \n lstm_3 (LSTM)               (None, 256)               525312    \n                                                                 \n dropout_1 (Dropout)         (None, 256)               0         \n                                                                 \n dense_2 (Dense)             (None, 100)               25700     \n                                                                 \n dense_3 (Dense)             (None, 7)                 707       \n                                                                 \n=================================================================\nTotal params: 820,042\nTrainable params: 820,042\nNon-trainable params: 0\n_________________________________________________________________\nNone\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Model: \"sequential_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n embedding_1 (Embedding)     (None, 5, 5)              35        \n                                                                 \n lstm_2 (LSTM)               (None, 5, 256)            268288    \n                                                                 \n dropout (Dropout)           (None, 5, 256)            0         \n                                                                 \n lstm_3 (LSTM)               (None, 256)               525312    \n                                                                 \n dropout_1 (Dropout)         (None, 256)               0         \n                                                                 \n dense_2 (Dense)             (None, 100)               25700     \n                                                                 \n dense_3 (Dense)             (None, 7)                 707       \n                                                                 \n=================================================================\nTotal params: 820,042\nTrainable params: 820,042\nNon-trainable params: 0\n_________________________________________________________________\nNone\n"]}}],"execution_count":0},{"cell_type":"code","source":["# compile model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n# fit model\nhistory=model.fit(train_x, train_y, batch_size=256, epochs=50, callbacks=[callback] )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"794bad4d-58d4-4a4c-97c2-57ff5d27845b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Epoch 1/50\n\r    1/65625 [..............................] - ETA: 99:55:21 - loss: 1.9459 - accuracy: 0.1406\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    2/65625 [..............................] - ETA: 5:13:09 - loss: 1.9433 - accuracy: 0.2480 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    3/65625 [..............................] - ETA: 5:00:31 - loss: 1.9410 - accuracy: 0.2695\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    4/65625 [..............................] - ETA: 5:06:40 - loss: 1.9380 - accuracy: 0.2822\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    5/65625 [..............................] - ETA: 5:30:42 - loss: 1.9342 - accuracy: 0.2914","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Epoch 1/50\n\r    1/65625 [..............................] - ETA: 99:55:21 - loss: 1.9459 - accuracy: 0.1406\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    2/65625 [..............................] - ETA: 5:13:09 - loss: 1.9433 - accuracy: 0.2480 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    3/65625 [..............................] - ETA: 5:00:31 - loss: 1.9410 - accuracy: 0.2695\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    4/65625 [..............................] - ETA: 5:06:40 - loss: 1.9380 - accuracy: 0.2822\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    5/65625 [..............................] - ETA: 5:30:42 - loss: 1.9342 - accuracy: 0.2914"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Cancelled","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["We upload the trained model on Paperspace"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"57b2fbf9-b9d4-4381-8507-66c115009c06"}}},{"cell_type":"code","source":["%sh wget https://www.dropbox.com/s/nipbobspz4skcu1/model_paperspace_100kgames_50epoch.h5?dl=1 -O /tmp/Paperspace_onlyPiece.csv"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8eb0f6f6-3791-48c4-b20f-590d76f2b337"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"--2022-06-04 17:16:16--  https://www.dropbox.com/s/nipbobspz4skcu1/model_paperspace_100kgames_50epoch.h5?dl=1\nResolving www.dropbox.com (www.dropbox.com)... 162.125.8.18, 2620:100:6016:18::a27d:112\nConnecting to www.dropbox.com (www.dropbox.com)|162.125.8.18|:443... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: /s/dl/nipbobspz4skcu1/model_paperspace_100kgames_50epoch.h5 [following]\n--2022-06-04 17:16:17--  https://www.dropbox.com/s/dl/nipbobspz4skcu1/model_paperspace_100kgames_50epoch.h5\nReusing existing connection to www.dropbox.com:443.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://uc80dbbad726932aa42756d45c81.dl.dropboxusercontent.com/cd/0/get/BmkxWDHGhArXBp0d5QeETE1u7rEPweQMzI5GsWWQ_Lmy1zvHfT9H0vY5GuY4h6L0m7BOSvg97UbNu6ways-rrruhyM-ztjIQFqb8aMibFSginy5SXuIvIrs1hPjMBTJ8dl7z0reY9erjfR2EKtFOTEoATf2wccDtPHuF-P7MYAbRZYDO6-SNImdclzPl9MgNrQU/file?dl=1# [following]\n--2022-06-04 17:16:17--  https://uc80dbbad726932aa42756d45c81.dl.dropboxusercontent.com/cd/0/get/BmkxWDHGhArXBp0d5QeETE1u7rEPweQMzI5GsWWQ_Lmy1zvHfT9H0vY5GuY4h6L0m7BOSvg97UbNu6ways-rrruhyM-ztjIQFqb8aMibFSginy5SXuIvIrs1hPjMBTJ8dl7z0reY9erjfR2EKtFOTEoATf2wccDtPHuF-P7MYAbRZYDO6-SNImdclzPl9MgNrQU/file?dl=1\nResolving uc80dbbad726932aa42756d45c81.dl.dropboxusercontent.com (uc80dbbad726932aa42756d45c81.dl.dropboxusercontent.com)... 162.125.8.15, 2620:100:6016:15::a27d:10f\nConnecting to uc80dbbad726932aa42756d45c81.dl.dropboxusercontent.com (uc80dbbad726932aa42756d45c81.dl.dropboxusercontent.com)|162.125.8.15|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 9900128 (9.4M) [application/binary]\nSaving to: /tmp/Paperspace_onlyPiece.csv\n\n     0K .......... .......... .......... .......... ..........  0%  496K 19s\n    50K .......... .......... .......... .......... ..........  1%  995K 14s\n   100K .......... .......... .......... .......... ..........  1% 1.20M 12s\n   150K .......... .......... .......... .......... ..........  2% 1.50M 11s\n   200K .......... .......... .......... .......... ..........  2% 2.91M 9s\n   250K .......... .......... .......... .......... ..........  3% 2.68M 8s\n   300K .......... .......... .......... .......... ..........  3% 3.06M 7s\n   350K .......... .......... .......... .......... ..........  4% 3.04M 7s\n   400K .......... .......... .......... .......... ..........  4% 5.30M 6s\n   450K .......... .......... .......... .......... ..........  5% 5.95M 6s\n   500K .......... .......... .......... .......... ..........  5% 6.12M 5s\n   550K .......... .......... .......... .......... ..........  6% 7.97M 5s\n   600K .......... .......... .......... .......... ..........  6% 7.64M 5s\n   650K .......... .......... .......... .......... ..........  7% 8.31M 4s\n   700K .......... .......... .......... .......... ..........  7% 9.62M 4s\n   750K .......... .......... .......... .......... ..........  8% 6.95M 4s\n   800K .......... .......... .......... .......... ..........  8% 8.11M 4s\n   850K .......... .......... .......... .......... ..........  9% 10.5M 3s\n   900K .......... .......... .......... .......... ..........  9% 10.4M 3s\n   950K .......... .......... .......... .......... .......... 10% 11.8M 3s\n  1000K .......... .......... .......... .......... .......... 10% 12.5M 3s\n  1050K .......... .......... .......... .......... .......... 11% 11.9M 3s\n  1100K .......... .......... .......... .......... .......... 11% 17.4M 3s\n  1150K .......... .......... .......... .......... .......... 12% 10.3M 3s\n  1200K .......... .......... .......... .......... .......... 12% 14.6M 3s\n  1250K .......... .......... .......... .......... .......... 13% 20.6M 2s\n  1300K .......... .......... .......... .......... .......... 13% 17.5M 2s\n  1350K .......... .......... .......... .......... .......... 14% 4.05M 2s\n  1400K .......... .......... .......... .......... .......... 14%  125M 2s\n  1450K .......... .......... .......... .......... .......... 15%  124M 2s\n  1500K .......... .......... .......... .......... .......... 16% 89.4M 2s\n  1550K .......... .......... .......... .......... .......... 16% 75.5M 2s\n  1600K .......... .......... .......... .......... .......... 17% 18.0M 2s\n  1650K .......... .......... .......... .......... .......... 17% 26.5M 2s\n  1700K .......... .......... .......... .......... .......... 18% 21.0M 2s\n  1750K .......... .......... .......... .......... .......... 18% 23.7M 2s\n  1800K .......... .......... .......... .......... .......... 19% 19.8M 2s\n  1850K .......... .......... .......... .......... .......... 19% 21.2M 2s\n  1900K .......... .......... .......... .......... .......... 20% 34.4M 2s\n  1950K .......... .......... .......... .......... .......... 20% 19.4M 2s\n  2000K .......... .......... .......... .......... .......... 21% 27.4M 2s\n  2050K .......... .......... .......... .......... .......... 21% 22.3M 2s\n  2100K .......... .......... .......... .......... .......... 22% 22.8M 1s\n  2150K .......... .......... .......... .......... .......... 22% 20.4M 1s\n  2200K .......... .......... .......... .......... .......... 23%  119M 1s\n  2250K .......... .......... .......... .......... .......... 23% 17.1M 1s\n  2300K .......... .......... .......... .......... .......... 24% 60.1M 1s\n  2350K .......... .......... .......... .......... .......... 24% 22.5M 1s\n  2400K .......... .......... .......... .......... .......... 25% 23.0M 1s\n  2450K .......... .......... .......... .......... .......... 25% 65.9M 1s\n  2500K .......... .......... .......... .......... .......... 26% 24.0M 1s\n  2550K .......... .......... .......... .......... .......... 26% 40.8M 1s\n  2600K .......... .......... .......... .......... .......... 27% 26.9M 1s\n  2650K .......... .......... .......... .......... .......... 27% 44.4M 1s\n  2700K .......... .......... .......... .......... .......... 28% 26.8M 1s\n  2750K .......... .......... .......... .......... .......... 28% 21.1M 1s\n  2800K .......... .......... .......... .......... .......... 29% 97.4M 1s\n  2850K .......... .......... .......... .......... .......... 29% 19.8M 1s\n  2900K .......... .......... .......... .......... .......... 30%  133M 1s\n  2950K .......... .......... .......... .......... .......... 31% 21.8M 1s\n  3000K .......... .......... .......... .......... .......... 31%  130M 1s\n  3050K .......... .......... .......... .......... .......... 32% 21.7M 1s\n  3100K .......... .......... .......... .......... .......... 32% 25.8M 1s\n  3150K .......... .......... .......... .......... .......... 33% 18.2M 1s\n  3200K .......... .......... .......... .......... .......... 33% 17.8M 1s\n  3250K .......... .......... .......... .......... .......... 34% 42.9M 1s\n  3300K .......... .......... .......... .......... .......... 34% 16.7M 1s\n  3350K .......... .......... .......... .......... .......... 35% 21.1M 1s\n  3400K .......... .......... .......... .......... .......... 35% 25.1M 1s\n  3450K .......... .......... .......... .......... .......... 36% 28.8M 1s\n  3500K .......... .......... .......... .......... .......... 36% 23.0M 1s\n  3550K .......... .......... .......... .......... .......... 37% 24.0M 1s\n  3600K .......... .......... .......... .......... .......... 37% 19.6M 1s\n  3650K .......... .......... .......... .......... .......... 38% 28.0M 1s\n  3700K .......... .......... .......... .......... .......... 38% 83.1M 1s\n  3750K .......... .......... .......... .......... .......... 39% 23.8M 1s\n  3800K .......... .......... .......... .......... .......... 39% 20.5M 1s\n  3850K .......... .......... .......... .......... .......... 40% 25.7M 1s\n  3900K .......... .......... .......... .......... .......... 40% 19.1M 1s\n  3950K .......... .......... .......... .......... .......... 41% 27.8M 1s\n  4000K .......... .......... .......... .......... .......... 41%  136M 1s\n  4050K .......... .......... .......... .......... .......... 42% 24.9M 1s\n  4100K .......... .......... .......... .......... .......... 42% 27.5M 1s\n  4150K .......... .......... .......... .......... .......... 43% 24.9M 1s\n  4200K .......... .......... .......... .......... .......... 43%  143M 1s\n  4250K .......... .......... .......... .......... .......... 44% 26.3M 1s\n  4300K .......... .......... .......... .......... .......... 44% 26.3M 1s\n  4350K .......... .......... .......... .......... .......... 45% 24.9M 1s\n  4400K .......... .......... .......... .......... .......... 46% 47.6M 1s\n  4450K .......... .......... .......... .......... .......... 46%  110M 1s\n  4500K .......... .......... .......... .......... .......... 47% 33.7M 1s\n  4550K .......... .......... .......... .......... .......... 47% 24.6M 1s\n  4600K .......... .......... .......... .......... .......... 48% 33.3M 1s\n  4650K .......... .......... .......... .......... .......... 48% 29.0M 1s\n  4700K .......... .......... .......... .......... .......... 49% 22.5M 1s\n  4750K .......... .......... .......... .......... .......... 49% 20.4M 1s\n  4800K .......... .......... .......... .......... .......... 50% 16.8M 1s\n  4850K .......... .......... .......... .......... .......... 50% 19.2M 1s\n  4900K .......... .......... .......... .......... .......... 51% 63.3M 1s\n  4950K .......... .......... .......... .......... .......... 51% 15.4M 0s\n  5000K .......... .......... .......... .......... .......... 52% 24.4M 0s\n  5050K .......... .......... .......... .......... .......... 52% 20.4M 0s\n  5100K .......... .......... .......... .......... .......... 53%  137M 0s\n  5150K .......... .......... .......... .......... .......... 53% 23.0M 0s\n  5200K .......... .......... .......... .......... .......... 54% 17.7M 0s\n  5250K .......... .......... .......... .......... .......... 54% 24.3M 0s\n  5300K .......... .......... .......... .......... .......... 55% 26.3M 0s\n  5350K .......... .......... .......... .......... .......... 55% 69.8M 0s\n  5400K .......... .......... .......... .......... .......... 56% 25.2M 0s\n  5450K .......... .......... .......... .......... .......... 56% 18.0M 0s\n  5500K .......... .......... .......... .......... .......... 57% 32.6M 0s\n  5550K .......... .......... .......... .......... .......... 57% 30.6M 0s\n  5600K .......... .......... .......... .......... .......... 58%  102M 0s\n  5650K .......... .......... .......... .......... .......... 58% 26.3M 0s\n  5700K .......... .......... .......... .......... .......... 59% 24.0M 0s\n  5750K .......... .......... .......... .......... .......... 59% 30.3M 0s\n  5800K .......... .......... .......... .......... .......... 60%  113M 0s\n  5850K .......... .......... .......... .......... .......... 61% 29.0M 0s\n  5900K .......... .......... .......... .......... .......... 61% 21.7M 0s\n  5950K .......... .......... .......... .......... .......... 62% 28.7M 0s\n  6000K .......... .......... .......... .......... .......... 62% 55.9M 0s\n  6050K .......... .......... .......... .......... .......... 63%  103M 0s\n  6100K .......... .......... .......... .......... .......... 63% 27.5M 0s\n  6150K .......... .......... .......... .......... .......... 64% 25.9M 0s\n  6200K .......... .......... .......... .......... .......... 64% 18.5M 0s\n  6250K .......... .......... .......... .......... .......... 65% 16.0M 0s\n  6300K .......... .......... .......... .......... .......... 65% 39.0M 0s\n  6350K .......... .......... .......... .......... .......... 66% 17.8M 0s\n  6400K .......... .......... .......... .......... .......... 66% 27.4M 0s\n  6450K .......... .......... .......... .......... .......... 67% 15.5M 0s\n  6500K .......... .......... .......... .......... .......... 67% 24.5M 0s\n  6550K .......... .......... .......... .......... .......... 68% 31.2M 0s\n  6600K .......... .......... .......... .......... .......... 68% 28.5M 0s\n  6650K .......... .......... .......... .......... .......... 69% 26.3M 0s\n  6700K .......... .......... .......... .......... .......... 69% 18.7M 0s\n  6750K .......... .......... .......... .......... .......... 70% 18.0M 0s\n  6800K .......... .......... .......... .......... .......... 70%  120M 0s\n  6850K .......... .......... .......... .......... .......... 71% 28.0M 0s\n  6900K .......... .......... .......... .......... .......... 71% 22.9M 0s\n  6950K .......... .......... .......... .......... .......... 72% 19.3M 0s\n  7000K .......... .......... .......... .......... .......... 72% 71.8M 0s\n  7050K .......... .......... .......... .......... .......... 73% 32.9M 0s\n  7100K .......... .......... .......... .......... .......... 73% 29.3M 0s\n  7150K .......... .......... .......... .......... .......... 74% 29.5M 0s\n  7200K .......... .......... .......... .......... .......... 74% 27.7M 0s\n  7250K .......... .......... .......... .......... .......... 75% 68.1M 0s\n  7300K .......... .......... .......... .......... .......... 76% 28.8M 0s\n  7350K .......... .......... .......... .......... .......... 76% 28.2M 0s\n  7400K .......... .......... .......... .......... .......... 77% 19.7M 0s\n  7450K .......... .......... .......... .......... .......... 77%  116M 0s\n  7500K .......... .......... .......... .......... .......... 78% 64.8M 0s\n  7550K .......... .......... .......... .......... .......... 78% 27.1M 0s\n  7600K .......... .......... .......... .......... .......... 79% 33.6M 0s\n  7650K .......... .......... .......... .......... .......... 79% 28.4M 0s\n  7700K .......... .......... .......... .......... .......... 80% 40.3M 0s\n  7750K .......... .......... .......... .......... .......... 80% 18.6M 0s\n  7800K .......... .......... .......... .......... .......... 81% 20.4M 0s\n  7850K .......... .......... .......... .......... .......... 81% 17.2M 0s\n  7900K .......... .......... .......... .......... .......... 82% 34.1M 0s\n  7950K .......... .......... .......... .......... .......... 82% 22.1M 0s\n  8000K .......... .......... .......... .......... .......... 83% 15.6M 0s\n  8050K .......... .......... .......... .......... .......... 83% 39.8M 0s\n  8100K .......... .......... .......... .......... .......... 84% 17.9M 0s\n  8150K .......... .......... .......... .......... .......... 84% 70.8M 0s\n  8200K .......... .......... .......... .......... .......... 85% 19.9M 0s\n  8250K .......... .......... .......... .......... .......... 85% 22.1M 0s\n  8300K .......... .......... .......... .......... .......... 86% 21.2M 0s\n  8350K .......... .......... .......... .......... .......... 86% 26.4M 0s\n  8400K .......... .......... .......... .......... .......... 87% 22.5M 0s\n  8450K .......... .......... .......... .......... .......... 87% 99.4M 0s\n  8500K .......... .......... .......... .......... .......... 88% 14.7M 0s\n  8550K .......... .......... .......... .......... .......... 88% 65.7M 0s\n  8600K .......... .......... .......... .......... .......... 89% 28.0M 0s\n  8650K .......... .......... .......... .......... .......... 89%  115M 0s\n  8700K .......... .......... .......... .......... .......... 90% 25.5M 0s\n  8750K .......... .......... .......... .......... .......... 91% 21.4M 0s\n  8800K .......... .......... .......... .......... .......... 91% 33.2M 0s\n  8850K .......... .......... .......... .......... .......... 92% 32.8M 0s\n  8900K .......... .......... .......... .......... .......... 92%  132M 0s\n  8950K .......... .......... .......... .......... .......... 93% 16.9M 0s\n  9000K .......... .......... .......... .......... .......... 93% 69.2M 0s\n  9050K .......... .......... .......... .......... .......... 94% 35.6M 0s\n  9100K .......... .......... .......... .......... .......... 94% 81.1M 0s\n  9150K .......... .......... .......... .......... .......... 95% 29.8M 0s\n  9200K .......... .......... .......... .......... .......... 95% 24.1M 0s\n  9250K .......... .......... .......... .......... .......... 96% 22.3M 0s\n  9300K .......... .......... .......... .......... .......... 96% 14.5M 0s\n  9350K .......... .......... .......... .......... .......... 97% 30.3M 0s\n  9400K .......... .......... .......... .......... .......... 97% 18.7M 0s\n  9450K .......... .......... .......... .......... .......... 98% 32.6M 0s\n  9500K .......... .......... .......... .......... .......... 98% 16.1M 0s\n  9550K .......... .......... .......... .......... .......... 99% 22.9M 0s\n  9600K .......... .......... .......... .......... .......... 99% 18.9M 0s\n  9650K .......... ........                                   100%  132M=0.7s\n\n2022-06-04 17:16:18 (13.5 MB/s) - /tmp/Paperspace_onlyPiece.csv saved [9900128/9900128]\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["--2022-06-04 17:16:16--  https://www.dropbox.com/s/nipbobspz4skcu1/model_paperspace_100kgames_50epoch.h5?dl=1\nResolving www.dropbox.com (www.dropbox.com)... 162.125.8.18, 2620:100:6016:18::a27d:112\nConnecting to www.dropbox.com (www.dropbox.com)|162.125.8.18|:443... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: /s/dl/nipbobspz4skcu1/model_paperspace_100kgames_50epoch.h5 [following]\n--2022-06-04 17:16:17--  https://www.dropbox.com/s/dl/nipbobspz4skcu1/model_paperspace_100kgames_50epoch.h5\nReusing existing connection to www.dropbox.com:443.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://uc80dbbad726932aa42756d45c81.dl.dropboxusercontent.com/cd/0/get/BmkxWDHGhArXBp0d5QeETE1u7rEPweQMzI5GsWWQ_Lmy1zvHfT9H0vY5GuY4h6L0m7BOSvg97UbNu6ways-rrruhyM-ztjIQFqb8aMibFSginy5SXuIvIrs1hPjMBTJ8dl7z0reY9erjfR2EKtFOTEoATf2wccDtPHuF-P7MYAbRZYDO6-SNImdclzPl9MgNrQU/file?dl=1# [following]\n--2022-06-04 17:16:17--  https://uc80dbbad726932aa42756d45c81.dl.dropboxusercontent.com/cd/0/get/BmkxWDHGhArXBp0d5QeETE1u7rEPweQMzI5GsWWQ_Lmy1zvHfT9H0vY5GuY4h6L0m7BOSvg97UbNu6ways-rrruhyM-ztjIQFqb8aMibFSginy5SXuIvIrs1hPjMBTJ8dl7z0reY9erjfR2EKtFOTEoATf2wccDtPHuF-P7MYAbRZYDO6-SNImdclzPl9MgNrQU/file?dl=1\nResolving uc80dbbad726932aa42756d45c81.dl.dropboxusercontent.com (uc80dbbad726932aa42756d45c81.dl.dropboxusercontent.com)... 162.125.8.15, 2620:100:6016:15::a27d:10f\nConnecting to uc80dbbad726932aa42756d45c81.dl.dropboxusercontent.com (uc80dbbad726932aa42756d45c81.dl.dropboxusercontent.com)|162.125.8.15|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 9900128 (9.4M) [application/binary]\nSaving to: /tmp/Paperspace_onlyPiece.csv\n\n     0K .......... .......... .......... .......... ..........  0%  496K 19s\n    50K .......... .......... .......... .......... ..........  1%  995K 14s\n   100K .......... .......... .......... .......... ..........  1% 1.20M 12s\n   150K .......... .......... .......... .......... ..........  2% 1.50M 11s\n   200K .......... .......... .......... .......... ..........  2% 2.91M 9s\n   250K .......... .......... .......... .......... ..........  3% 2.68M 8s\n   300K .......... .......... .......... .......... ..........  3% 3.06M 7s\n   350K .......... .......... .......... .......... ..........  4% 3.04M 7s\n   400K .......... .......... .......... .......... ..........  4% 5.30M 6s\n   450K .......... .......... .......... .......... ..........  5% 5.95M 6s\n   500K .......... .......... .......... .......... ..........  5% 6.12M 5s\n   550K .......... .......... .......... .......... ..........  6% 7.97M 5s\n   600K .......... .......... .......... .......... ..........  6% 7.64M 5s\n   650K .......... .......... .......... .......... ..........  7% 8.31M 4s\n   700K .......... .......... .......... .......... ..........  7% 9.62M 4s\n   750K .......... .......... .......... .......... ..........  8% 6.95M 4s\n   800K .......... .......... .......... .......... ..........  8% 8.11M 4s\n   850K .......... .......... .......... .......... ..........  9% 10.5M 3s\n   900K .......... .......... .......... .......... ..........  9% 10.4M 3s\n   950K .......... .......... .......... .......... .......... 10% 11.8M 3s\n  1000K .......... .......... .......... .......... .......... 10% 12.5M 3s\n  1050K .......... .......... .......... .......... .......... 11% 11.9M 3s\n  1100K .......... .......... .......... .......... .......... 11% 17.4M 3s\n  1150K .......... .......... .......... .......... .......... 12% 10.3M 3s\n  1200K .......... .......... .......... .......... .......... 12% 14.6M 3s\n  1250K .......... .......... .......... .......... .......... 13% 20.6M 2s\n  1300K .......... .......... .......... .......... .......... 13% 17.5M 2s\n  1350K .......... .......... .......... .......... .......... 14% 4.05M 2s\n  1400K .......... .......... .......... .......... .......... 14%  125M 2s\n  1450K .......... .......... .......... .......... .......... 15%  124M 2s\n  1500K .......... .......... .......... .......... .......... 16% 89.4M 2s\n  1550K .......... .......... .......... .......... .......... 16% 75.5M 2s\n  1600K .......... .......... .......... .......... .......... 17% 18.0M 2s\n  1650K .......... .......... .......... .......... .......... 17% 26.5M 2s\n  1700K .......... .......... .......... .......... .......... 18% 21.0M 2s\n  1750K .......... .......... .......... .......... .......... 18% 23.7M 2s\n  1800K .......... .......... .......... .......... .......... 19% 19.8M 2s\n  1850K .......... .......... .......... .......... .......... 19% 21.2M 2s\n  1900K .......... .......... .......... .......... .......... 20% 34.4M 2s\n  1950K .......... .......... .......... .......... .......... 20% 19.4M 2s\n  2000K .......... .......... .......... .......... .......... 21% 27.4M 2s\n  2050K .......... .......... .......... .......... .......... 21% 22.3M 2s\n  2100K .......... .......... .......... .......... .......... 22% 22.8M 1s\n  2150K .......... .......... .......... .......... .......... 22% 20.4M 1s\n  2200K .......... .......... .......... .......... .......... 23%  119M 1s\n  2250K .......... .......... .......... .......... .......... 23% 17.1M 1s\n  2300K .......... .......... .......... .......... .......... 24% 60.1M 1s\n  2350K .......... .......... .......... .......... .......... 24% 22.5M 1s\n  2400K .......... .......... .......... .......... .......... 25% 23.0M 1s\n  2450K .......... .......... .......... .......... .......... 25% 65.9M 1s\n  2500K .......... .......... .......... .......... .......... 26% 24.0M 1s\n  2550K .......... .......... .......... .......... .......... 26% 40.8M 1s\n  2600K .......... .......... .......... .......... .......... 27% 26.9M 1s\n  2650K .......... .......... .......... .......... .......... 27% 44.4M 1s\n  2700K .......... .......... .......... .......... .......... 28% 26.8M 1s\n  2750K .......... .......... .......... .......... .......... 28% 21.1M 1s\n  2800K .......... .......... .......... .......... .......... 29% 97.4M 1s\n  2850K .......... .......... .......... .......... .......... 29% 19.8M 1s\n  2900K .......... .......... .......... .......... .......... 30%  133M 1s\n  2950K .......... .......... .......... .......... .......... 31% 21.8M 1s\n  3000K .......... .......... .......... .......... .......... 31%  130M 1s\n  3050K .......... .......... .......... .......... .......... 32% 21.7M 1s\n  3100K .......... .......... .......... .......... .......... 32% 25.8M 1s\n  3150K .......... .......... .......... .......... .......... 33% 18.2M 1s\n  3200K .......... .......... .......... .......... .......... 33% 17.8M 1s\n  3250K .......... .......... .......... .......... .......... 34% 42.9M 1s\n  3300K .......... .......... .......... .......... .......... 34% 16.7M 1s\n  3350K .......... .......... .......... .......... .......... 35% 21.1M 1s\n  3400K .......... .......... .......... .......... .......... 35% 25.1M 1s\n  3450K .......... .......... .......... .......... .......... 36% 28.8M 1s\n  3500K .......... .......... .......... .......... .......... 36% 23.0M 1s\n  3550K .......... .......... .......... .......... .......... 37% 24.0M 1s\n  3600K .......... .......... .......... .......... .......... 37% 19.6M 1s\n  3650K .......... .......... .......... .......... .......... 38% 28.0M 1s\n  3700K .......... .......... .......... .......... .......... 38% 83.1M 1s\n  3750K .......... .......... .......... .......... .......... 39% 23.8M 1s\n  3800K .......... .......... .......... .......... .......... 39% 20.5M 1s\n  3850K .......... .......... .......... .......... .......... 40% 25.7M 1s\n  3900K .......... .......... .......... .......... .......... 40% 19.1M 1s\n  3950K .......... .......... .......... .......... .......... 41% 27.8M 1s\n  4000K .......... .......... .......... .......... .......... 41%  136M 1s\n  4050K .......... .......... .......... .......... .......... 42% 24.9M 1s\n  4100K .......... .......... .......... .......... .......... 42% 27.5M 1s\n  4150K .......... .......... .......... .......... .......... 43% 24.9M 1s\n  4200K .......... .......... .......... .......... .......... 43%  143M 1s\n  4250K .......... .......... .......... .......... .......... 44% 26.3M 1s\n  4300K .......... .......... .......... .......... .......... 44% 26.3M 1s\n  4350K .......... .......... .......... .......... .......... 45% 24.9M 1s\n  4400K .......... .......... .......... .......... .......... 46% 47.6M 1s\n  4450K .......... .......... .......... .......... .......... 46%  110M 1s\n  4500K .......... .......... .......... .......... .......... 47% 33.7M 1s\n  4550K .......... .......... .......... .......... .......... 47% 24.6M 1s\n  4600K .......... .......... .......... .......... .......... 48% 33.3M 1s\n  4650K .......... .......... .......... .......... .......... 48% 29.0M 1s\n  4700K .......... .......... .......... .......... .......... 49% 22.5M 1s\n  4750K .......... .......... .......... .......... .......... 49% 20.4M 1s\n  4800K .......... .......... .......... .......... .......... 50% 16.8M 1s\n  4850K .......... .......... .......... .......... .......... 50% 19.2M 1s\n  4900K .......... .......... .......... .......... .......... 51% 63.3M 1s\n  4950K .......... .......... .......... .......... .......... 51% 15.4M 0s\n  5000K .......... .......... .......... .......... .......... 52% 24.4M 0s\n  5050K .......... .......... .......... .......... .......... 52% 20.4M 0s\n  5100K .......... .......... .......... .......... .......... 53%  137M 0s\n  5150K .......... .......... .......... .......... .......... 53% 23.0M 0s\n  5200K .......... .......... .......... .......... .......... 54% 17.7M 0s\n  5250K .......... .......... .......... .......... .......... 54% 24.3M 0s\n  5300K .......... .......... .......... .......... .......... 55% 26.3M 0s\n  5350K .......... .......... .......... .......... .......... 55% 69.8M 0s\n  5400K .......... .......... .......... .......... .......... 56% 25.2M 0s\n  5450K .......... .......... .......... .......... .......... 56% 18.0M 0s\n  5500K .......... .......... .......... .......... .......... 57% 32.6M 0s\n  5550K .......... .......... .......... .......... .......... 57% 30.6M 0s\n  5600K .......... .......... .......... .......... .......... 58%  102M 0s\n  5650K .......... .......... .......... .......... .......... 58% 26.3M 0s\n  5700K .......... .......... .......... .......... .......... 59% 24.0M 0s\n  5750K .......... .......... .......... .......... .......... 59% 30.3M 0s\n  5800K .......... .......... .......... .......... .......... 60%  113M 0s\n  5850K .......... .......... .......... .......... .......... 61% 29.0M 0s\n  5900K .......... .......... .......... .......... .......... 61% 21.7M 0s\n  5950K .......... .......... .......... .......... .......... 62% 28.7M 0s\n  6000K .......... .......... .......... .......... .......... 62% 55.9M 0s\n  6050K .......... .......... .......... .......... .......... 63%  103M 0s\n  6100K .......... .......... .......... .......... .......... 63% 27.5M 0s\n  6150K .......... .......... .......... .......... .......... 64% 25.9M 0s\n  6200K .......... .......... .......... .......... .......... 64% 18.5M 0s\n  6250K .......... .......... .......... .......... .......... 65% 16.0M 0s\n  6300K .......... .......... .......... .......... .......... 65% 39.0M 0s\n  6350K .......... .......... .......... .......... .......... 66% 17.8M 0s\n  6400K .......... .......... .......... .......... .......... 66% 27.4M 0s\n  6450K .......... .......... .......... .......... .......... 67% 15.5M 0s\n  6500K .......... .......... .......... .......... .......... 67% 24.5M 0s\n  6550K .......... .......... .......... .......... .......... 68% 31.2M 0s\n  6600K .......... .......... .......... .......... .......... 68% 28.5M 0s\n  6650K .......... .......... .......... .......... .......... 69% 26.3M 0s\n  6700K .......... .......... .......... .......... .......... 69% 18.7M 0s\n  6750K .......... .......... .......... .......... .......... 70% 18.0M 0s\n  6800K .......... .......... .......... .......... .......... 70%  120M 0s\n  6850K .......... .......... .......... .......... .......... 71% 28.0M 0s\n  6900K .......... .......... .......... .......... .......... 71% 22.9M 0s\n  6950K .......... .......... .......... .......... .......... 72% 19.3M 0s\n  7000K .......... .......... .......... .......... .......... 72% 71.8M 0s\n  7050K .......... .......... .......... .......... .......... 73% 32.9M 0s\n  7100K .......... .......... .......... .......... .......... 73% 29.3M 0s\n  7150K .......... .......... .......... .......... .......... 74% 29.5M 0s\n  7200K .......... .......... .......... .......... .......... 74% 27.7M 0s\n  7250K .......... .......... .......... .......... .......... 75% 68.1M 0s\n  7300K .......... .......... .......... .......... .......... 76% 28.8M 0s\n  7350K .......... .......... .......... .......... .......... 76% 28.2M 0s\n  7400K .......... .......... .......... .......... .......... 77% 19.7M 0s\n  7450K .......... .......... .......... .......... .......... 77%  116M 0s\n  7500K .......... .......... .......... .......... .......... 78% 64.8M 0s\n  7550K .......... .......... .......... .......... .......... 78% 27.1M 0s\n  7600K .......... .......... .......... .......... .......... 79% 33.6M 0s\n  7650K .......... .......... .......... .......... .......... 79% 28.4M 0s\n  7700K .......... .......... .......... .......... .......... 80% 40.3M 0s\n  7750K .......... .......... .......... .......... .......... 80% 18.6M 0s\n  7800K .......... .......... .......... .......... .......... 81% 20.4M 0s\n  7850K .......... .......... .......... .......... .......... 81% 17.2M 0s\n  7900K .......... .......... .......... .......... .......... 82% 34.1M 0s\n  7950K .......... .......... .......... .......... .......... 82% 22.1M 0s\n  8000K .......... .......... .......... .......... .......... 83% 15.6M 0s\n  8050K .......... .......... .......... .......... .......... 83% 39.8M 0s\n  8100K .......... .......... .......... .......... .......... 84% 17.9M 0s\n  8150K .......... .......... .......... .......... .......... 84% 70.8M 0s\n  8200K .......... .......... .......... .......... .......... 85% 19.9M 0s\n  8250K .......... .......... .......... .......... .......... 85% 22.1M 0s\n  8300K .......... .......... .......... .......... .......... 86% 21.2M 0s\n  8350K .......... .......... .......... .......... .......... 86% 26.4M 0s\n  8400K .......... .......... .......... .......... .......... 87% 22.5M 0s\n  8450K .......... .......... .......... .......... .......... 87% 99.4M 0s\n  8500K .......... .......... .......... .......... .......... 88% 14.7M 0s\n  8550K .......... .......... .......... .......... .......... 88% 65.7M 0s\n  8600K .......... .......... .......... .......... .......... 89% 28.0M 0s\n  8650K .......... .......... .......... .......... .......... 89%  115M 0s\n  8700K .......... .......... .......... .......... .......... 90% 25.5M 0s\n  8750K .......... .......... .......... .......... .......... 91% 21.4M 0s\n  8800K .......... .......... .......... .......... .......... 91% 33.2M 0s\n  8850K .......... .......... .......... .......... .......... 92% 32.8M 0s\n  8900K .......... .......... .......... .......... .......... 92%  132M 0s\n  8950K .......... .......... .......... .......... .......... 93% 16.9M 0s\n  9000K .......... .......... .......... .......... .......... 93% 69.2M 0s\n  9050K .......... .......... .......... .......... .......... 94% 35.6M 0s\n  9100K .......... .......... .......... .......... .......... 94% 81.1M 0s\n  9150K .......... .......... .......... .......... .......... 95% 29.8M 0s\n  9200K .......... .......... .......... .......... .......... 95% 24.1M 0s\n  9250K .......... .......... .......... .......... .......... 96% 22.3M 0s\n  9300K .......... .......... .......... .......... .......... 96% 14.5M 0s\n  9350K .......... .......... .......... .......... .......... 97% 30.3M 0s\n  9400K .......... .......... .......... .......... .......... 97% 18.7M 0s\n  9450K .......... .......... .......... .......... .......... 98% 32.6M 0s\n  9500K .......... .......... .......... .......... .......... 98% 16.1M 0s\n  9550K .......... .......... .......... .......... .......... 99% 22.9M 0s\n  9600K .......... .......... .......... .......... .......... 99% 18.9M 0s\n  9650K .......... ........                                   100%  132M=0.7s\n\n2022-06-04 17:16:18 (13.5 MB/s) - /tmp/Paperspace_onlyPiece.csv saved [9900128/9900128]\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["model2_onlyPiece = keras.models.load_model(\"/tmp/Paperspace_onlyPiece.csv\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"191eb10e-e833-4924-a25f-301098482418"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Plotting of the loss and accuracy values during the train"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cb9c7784-23c9-4970-bd2a-328a79f3cad7"}}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\nplt.plot(history.history['loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train'], loc='upper right')\nplt.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0cd3aac3-470d-49e2-a6c2-f1d0a46b33c9"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["<img src=\"https://www.dropbox.com/s/8rzpwv7myc4im79/Model2_loss.png?dl=1\">"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c70b6ce5-a5e9-4c12-9339-545f49ba815d"}}},{"cell_type":"code","source":["plt.plot(history.history['accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train'], loc='upper left')\nplt.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5ba9bbd4-e484-4103-b1dc-3d81fc9aa610"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["<img src=\"https://www.dropbox.com/s/gm0kei8ae4dyp4f/Model2_accuracy.png?dl=1\">"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7218cb0a-cbce-4d0c-9fc3-8fb9ad3c71ac"}}},{"cell_type":"markdown","source":["### Evaluation of first approach models\nIn this section the evaluation of models trained with this approach is performed. For more information on the evaluation conducted the reader is invited to read the section \"Evaluation Backgorund\" in the Introduction."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"259825ac-c58f-4c05-aea6-0e05b04eb27c"}}},{"cell_type":"markdown","source":["In the following cell, we check how many times the model exactly predicts the moves of the matches in the test set. We also calculate the perplexity of the models for each match."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ad07408a-54f2-4753-9c21-e1aba5528e7c"}}},{"cell_type":"code","source":["#How many times do the models predict correctly\nm1=0\nm2=0\n#Multiplication of probabilities for final calculation of perplexity\np_m1=1\np_m2=1\n#List for saving perplexities divided by match \np_model1=[]\np_model2=[]\ngam_count=0\n\nfor i in range(0,len(test_x[:35000])):\n  el = np.asarray(test_x[i])\n  el = np.reshape(el, (1, len(el), 1))\n  \n  #MODEL 1 \n  prediction = model1_onlyPiece.predict(el, verbose=0)\n  index = np.argmax(prediction)\n  \n  p_m1=p_m1 * prediction[0][index]\n  \n  #MODEL 2\n  \n  prediction2 = model2_onlyPiece.predict(el, verbose=0)\n  index2 = np.argmax(prediction2)\n  p_m2=p_m2 * prediction2[0][index2]\n  \n  if gam_count==69:\n    p_model1.append(p_m1**(-(1/70)))\n    p_model2.append(p_m2**(-(1/70)))\n    p_m1=1\n    p_m2=1\n    gam_count=0\n\n  if index==test_y[i]:\n    m1+=1\n  if index2==test_y[i]:\n    m2+=1\n  if i%100==0:\n    print(i)\n  gam_count+=1\n  \nprint(\"-----------------------\")\nprint(\"model1 -> \", m1)\nprint(\"model2 -> \", m2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d5617399-ba58-4245-82bb-052be89af9e5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"0\n100\n200\n300\n400\n500\n600\n700\n800\n900\n1000\n1100\n1200\n1300\n1400\n1500\n1600\n1700\n1800\n1900\n2000\n2100\n2200\n2300\n2400\n2500\n2600\n2700\n2800\n2900\n3000\n3100\n3200\n3300\n3400\n3500\n3600\n3700\n3800\n3900\n4000\n4100\n4200\n4300\n4400\n4500\n4600\n4700\n4800\n4900\n5000\n5100\n5200\n5300\n5400\n5500\n5600\n5700\n5800\n5900\n6000\n6100\n6200\n6300\n6400\n6500\n6600\n6700\n6800\n6900\n7000\n7100\n7200\n7300\n7400\n7500\n7600\n7700\n7800\n7900\n8000\n8100\n8200\n8300\n8400\n8500\n8600\n8700\n8800\n8900\n9000\n9100\n9200\n9300\n9400\n9500\n9600\n9700\n9800\n9900\n10000\n10100\n10200\n10300\n10400\n10500\n10600\n10700\n10800\n10900\n11000\n11100\n11200\n11300\n11400\n11500\n11600\n11700\n11800\n11900\n12000\n12100\n12200\n12300\n12400\n12500\n12600\n12700\n12800\n12900\n13000\n13100\n13200\n13300\n13400\n13500\n13600\n13700\n13800\n13900\n14000\n14100\n14200\n14300\n14400\n14500\n14600\n14700\n14800\n14900\n15000\n15100\n15200\n15300\n15400\n15500\n15600\n15700\n15800\n15900\n16000\n16100\n16200\n16300\n16400\n16500\n16600\n16700\n16800\n16900\n17000\n17100\n17200\n17300\n17400\n17500\n17600\n17700\n17800\n17900\n18000\n18100\n18200\n18300\n18400\n18500\n18600\n18700\n18800\n18900\n19000\n19100\n19200\n19300\n19400\n19500\n19600\n19700\n19800\n19900\n20000\n20100\n20200\n20300\n20400\n20500\n20600\n20700\n20800\n20900\n21000\n21100\n21200\n21300\n21400\n21500\n21600\n21700\n21800\n21900\n22000\n22100\n22200\n22300\n22400\n22500\n22600\n22700\n22800\n22900\n23000\n23100\n23200\n23300\n23400\n23500\n23600\n23700\n23800\n23900\n24000\n24100\n24200\n24300\n24400\n24500\n24600\n24700\n24800\n24900\n25000\n25100\n25200\n25300\n25400\n25500\n25600\n25700\n25800\n25900\n26000\n26100\n26200\n26300\n26400\n26500\n26600\n26700\n26800\n26900\n27000\n27100\n27200\n27300\n27400\n27500\n27600\n27700\n27800\n27900\n28000\n28100\n28200\n28300\n28400\n28500\n28600\n28700\n28800\n28900\n29000\n29100\n29200\n29300\n29400\n29500\n29600\n29700\n29800\n29900\n30000\n30100\n30200\n30300\n30400\n30500\n30600\n30700\n30800\n30900\n31000\n31100\n31200\n31300\n31400\n31500\n31600\n31700\n31800\n31900\n32000\n32100\n32200\n32300\n32400\n32500\n32600\n32700\n32800\n32900\n33000\n33100\n33200\n33300\n33400\n33500\n33600\n33700\n33800\n33900\n34000\n34100\n34200\n34300\n34400\n34500\n34600\n34700\n34800\n34900\n-----------------------\nmodel1 ->  19116\nmodel2 ->  19115\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["0\n100\n200\n300\n400\n500\n600\n700\n800\n900\n1000\n1100\n1200\n1300\n1400\n1500\n1600\n1700\n1800\n1900\n2000\n2100\n2200\n2300\n2400\n2500\n2600\n2700\n2800\n2900\n3000\n3100\n3200\n3300\n3400\n3500\n3600\n3700\n3800\n3900\n4000\n4100\n4200\n4300\n4400\n4500\n4600\n4700\n4800\n4900\n5000\n5100\n5200\n5300\n5400\n5500\n5600\n5700\n5800\n5900\n6000\n6100\n6200\n6300\n6400\n6500\n6600\n6700\n6800\n6900\n7000\n7100\n7200\n7300\n7400\n7500\n7600\n7700\n7800\n7900\n8000\n8100\n8200\n8300\n8400\n8500\n8600\n8700\n8800\n8900\n9000\n9100\n9200\n9300\n9400\n9500\n9600\n9700\n9800\n9900\n10000\n10100\n10200\n10300\n10400\n10500\n10600\n10700\n10800\n10900\n11000\n11100\n11200\n11300\n11400\n11500\n11600\n11700\n11800\n11900\n12000\n12100\n12200\n12300\n12400\n12500\n12600\n12700\n12800\n12900\n13000\n13100\n13200\n13300\n13400\n13500\n13600\n13700\n13800\n13900\n14000\n14100\n14200\n14300\n14400\n14500\n14600\n14700\n14800\n14900\n15000\n15100\n15200\n15300\n15400\n15500\n15600\n15700\n15800\n15900\n16000\n16100\n16200\n16300\n16400\n16500\n16600\n16700\n16800\n16900\n17000\n17100\n17200\n17300\n17400\n17500\n17600\n17700\n17800\n17900\n18000\n18100\n18200\n18300\n18400\n18500\n18600\n18700\n18800\n18900\n19000\n19100\n19200\n19300\n19400\n19500\n19600\n19700\n19800\n19900\n20000\n20100\n20200\n20300\n20400\n20500\n20600\n20700\n20800\n20900\n21000\n21100\n21200\n21300\n21400\n21500\n21600\n21700\n21800\n21900\n22000\n22100\n22200\n22300\n22400\n22500\n22600\n22700\n22800\n22900\n23000\n23100\n23200\n23300\n23400\n23500\n23600\n23700\n23800\n23900\n24000\n24100\n24200\n24300\n24400\n24500\n24600\n24700\n24800\n24900\n25000\n25100\n25200\n25300\n25400\n25500\n25600\n25700\n25800\n25900\n26000\n26100\n26200\n26300\n26400\n26500\n26600\n26700\n26800\n26900\n27000\n27100\n27200\n27300\n27400\n27500\n27600\n27700\n27800\n27900\n28000\n28100\n28200\n28300\n28400\n28500\n28600\n28700\n28800\n28900\n29000\n29100\n29200\n29300\n29400\n29500\n29600\n29700\n29800\n29900\n30000\n30100\n30200\n30300\n30400\n30500\n30600\n30700\n30800\n30900\n31000\n31100\n31200\n31300\n31400\n31500\n31600\n31700\n31800\n31900\n32000\n32100\n32200\n32300\n32400\n32500\n32600\n32700\n32800\n32900\n33000\n33100\n33200\n33300\n33400\n33500\n33600\n33700\n33800\n33900\n34000\n34100\n34200\n34300\n34400\n34500\n34600\n34700\n34800\n34900\n-----------------------\nmodel1 ->  19116\nmodel2 ->  19115\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["Once we have calculated the model perplexity for each match we calculate an average of these perplexities to compare the models"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5911755f-dc0a-4f7e-8ed4-a59a67aa0dcb"}}},{"cell_type":"code","source":["s_m1=0\ns_m2=0\nfor i in range(0,len(p_model1)):\n  s_m1+=p_model1[i]\n  s_m2+=p_model2[i]\n  \navg_m1=s_m1/len(p_model1)\navg_m2=s_m2/len(p_model2)\n\nprint(\"The average perplexity of model 1 is\", avg_m1)\nprint(\"The average perplexity of model 2 is\", avg_m2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e21637bf-494f-4d33-85bd-c37d4c66ae08"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"The average perplexity of model 1 is 2.251866545819963\nThe average perplexity of model 2 is 2.248328845399468\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["The average perplexity of model 1 is 2.251866545819963\nThe average perplexity of model 2 is 2.248328845399468\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Discussion of evaluation results\nThe models created were evaluated in the same way and provided very similar results. In the extrinsic evaluation on 35000 predictions, model1 correctly predicted 19116 moves while model 2 correctly predicted 19115 moves. So the models behaved in the same way having also almost identical perplexities. We note, however, that although model 2 missed one more prediction than model 1, it had a slightly lower perplexity in fact model 2 had an average perplexity of 2.248 while model 1 had an average perplexity of 2.251."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"111ae66e-c080-4e15-a8fc-089e8c72756b"}}},{"cell_type":"markdown","source":["# Section 3: Second Approach\n<p>In the second approach, the granularity of the moves used is extended. In this approach I consider as a move the whole string except piece captures (\"x\" in the move), check (\"+\" at the end of the move), checkmate(\"#\" at the end of the move) and piece disambiguations. The process of arriving at sliding windows is the same as in the first approach but considering the moves differently. For clarity, let's look at the same example given in the first approach.</p> <br>\n\n\n<img src=\"https://www.dropbox.com/s/zji4zjp2965md9p/SecondApproach.jpg?dl=1\">\n\n\n<p>The same thing said in the first approach applies to this. The model is trained with integers, not strings. So once we have the match with the move formed by piece and cell I need a dictionary to encode the moves in integer. For simplicity this step is not included in figure.<br>\nIn the example given there is no case where a move contains a check or a checkmate. In this case a move such as Nf3+ (or Nf3#) becomes Nf3. In the case of disambiguations between pieces a move such as N4f6 becomes Nf6.</p>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dcbc7dd7-9b0c-463e-a467-20d45b0e7128"}}},{"cell_type":"markdown","source":["Function for parse the moves and codify the games. This function take in input a string that identify a move and return the moves without takes (\"x\"), disambiguation, check and checkmate."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6e9b3297-d859-4999-9989-e54fea3b4cf5"}}},{"cell_type":"code","source":["import re\n\ndef move_onlyInitial(m):\n  if m.startswith('O-O') or len(m)<=3:\n    if m[-1]==\"#\" or m[-1]==\"+\":\n      m=m[:-1]\n    return m\n  else:\n    z=\"\"\n    number = re.search(r\"\\d\", m)\n    z = m[0] + m[number.start()-1] + m[number.start()]\n    return z"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8629af89-7a29-4d76-99b6-ddb3e169a7e5"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["In the next cell we read the filtered matches and for each move we go to clean the data by removing in move number related to the match and then pass the cleaned move to the move_onlyInitial function. <br>\nWhile cleaning the data, a set is also defined with the moves that occurred."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7a83b02e-0604-4d89-a66e-90aa181e0742"}}},{"cell_type":"code","source":["game_man=[]\nconta=0\nset_moves=set()\nfor el in game:\n  lista=[]\n  if el[0]!=None:\n    li=el[0].split(\" \")\n    for y in li:\n      if y != '':\n        z=y.split('.')[1]\n        if z!='':\n          m=move_onlyInitial(z)\n          set_moves.add(m)\n          lista.append(m)\n          \n  game_man.append(lista)\n          \n  if conta%100000==0:\n      print(conta)\n  conta+=1"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0d95282b-977c-4b61-9339-bf1c7811df52"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"0\n100000\n200000\n300000\n400000\n500000\n600000\n700000\n800000\n900000\n1000000\n1100000\n1200000\n1300000\n1400000\n1500000\n1600000\n1700000\n1800000\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["0\n100000\n200000\n300000\n400000\n500000\n600000\n700000\n800000\n900000\n1000000\n1100000\n1200000\n1300000\n1400000\n1500000\n1600000\n1700000\n1800000\n"]}}],"execution_count":0},{"cell_type":"code","source":["print(\"The number of different moves is:\",len(set_moves))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9b25965b-d1b7-421a-83fe-d935f97a5c19"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"The number of different moves is: 522\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["The number of different moves is: 522\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["We create a dictionary given the move set. The dictionary will have the moves as keys and an integer used for encoding as a value. A null move was added to the dictionary for padding. After that the dictionary was saved in the DFBS."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"01c28399-8b66-487a-ba19-5f0ec97b0037"}}},{"cell_type":"code","source":["int_moves=dict((c, i) for i, c in enumerate(set_moves))\nint_moves[\"fill\"]=len(int_moves)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cad1ee08-a256-49c9-bce5-956ad3956ae4"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["with open(\"/tmp/dic_moves_onlyInitial.txt\", \"w+\") as fp:\n    json.dump(int_moves, fp)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"72e770c7-d10c-4a60-aeb4-61f806a9516f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.cp(\"file:/tmp/dic_moves_onlyInitial.txt\", \"dbfs:/tmp/dic_moves_onlyInitial.txt\")\ndisplay(dbutils.fs.ls(\"/tmp/dic_moves_onlyInitial.txt\"))\ndbutils.fs.cp(\"dbfs:/tmp/dic_moves_onlyInitial.txt\", \"/FileStore/dic_moves_onlyInitial.txt\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"84aeaeaa-13bf-4f7c-89e9-be16a2bb0623"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.cp(\"dbfs:/tmp/dic_moves_onlyInitial.txt\", \"file:/tmp/dic_moves_onlyInitial.txt\")\nwith open('/tmp/dic_moves_onlyInitial.txt') as json_file:\n    int_moves = json.load(json_file)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ed28cba2-df05-4aaf-8773-d7353ac2beeb"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Once the dictionary is calculated, we create x and y divided by 5 timesteps and within them we put the integer coding of the moves"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f6e82158-5b94-4293-ae82-f3d70d58f76e"}}},{"cell_type":"code","source":["X_file = open(\"/tmp/X_slide_windows_10-75_5steps_games.txt\", \"w+\")\nY_file = open(\"/tmp/Y_slide_windows_10-75_5steps_games.txt\", \"w+\")\nconta=0\nfor el in game_man:\n    el2=[int(int_moves[e]) for e in el]\n    n = medium-len(el2)\n    lfill=[int(int_moves[\"fill\"]) for i in range(0,n)]\n    lista=el2+lfill\n    seq=lista\n    for i in range(len(seq)):\n          #get the last index\n          lastIndex = i + n_steps\n          #if lastIndex is greater than length of sequence then break\n          if lastIndex > len(seq) - 1:\n              break\n          #Create input and output sequence\n          seq_X, seq_y = seq[i:lastIndex], seq[lastIndex]\n          X_file.write(str(seq_X))\n          X_file.write('\\n')\n          Y_file.write(str(seq_y))\n          Y_file.write('\\n')\n          pass\n    if conta%500==0:\n      print(conta)\n    conta+=1"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7d7514ce-a0e5-4ee9-a09a-a94b48803807"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Save \"X\" and \"Y\" in the DBFS"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d210a1da-2ecc-49aa-8031-b15f26b3b20b"}}},{"cell_type":"code","source":["dbutils.fs.cp(\"file:/tmp/X_slide_windows_10-75_5steps_games.txt\", \"dbfs:/tmp/X_slide_windows_10-75_5steps_games.txt\")\ndisplay(dbutils.fs.ls(\"/tmp/X_slide_windows_10-75_5steps_games.txt\"))\ndbutils.fs.cp(\"dbfs:/tmp/X_slide_windows_10-75_5steps_games.txt\", \"/FileStore/X_slide_windows_10-75_5steps_games.txt\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"019d042c-8a30-4315-bc9c-770b8f9537f4"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.cp(\"file:/tmp/Y_slide_windows_10-75_5steps_games.txt\", \"dbfs:/tmp/Y_slide_windows_10-75_5steps_games.txt\")\ndisplay(dbutils.fs.ls(\"/tmp/Y_slide_windows_10-75_5steps_games.txt\"))\ndbutils.fs.cp(\"dbfs:/tmp/Y_slide_windows_10-75_5steps_games.txt\", \"/FileStore/Y_slide_windows_10-75_5steps_games.txt\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3033f593-5035-486a-ac62-7ebaa079516a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Load X and Y <br>\nFor the following two approaches, 15000 matches were considered. Because the size of the dictionary is much larger than the first approach and memory problems were encountered when computing the One Hot Encoder. Considering 15000 matches means having 1050000 vectors given by dividing by timesteps in x and y."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8e31b18f-9051-446b-8be1-838b84b4b9d5"}}},{"cell_type":"code","source":["matches=15000\nsteps_per_match=70\nn_steps=5\nnmoves=len(int_moves)-1"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"24be96cf-416a-457c-8cfc-95ad6567ce37"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.cp(\"dbfs:/tmp/Y_slide_windows_10-75_5steps_games.txt\", \"file:/tmp/Y_slide_windows_10-75_5steps_games.txt\")\ny=[]\nc=0\nfor line in open(\"/tmp/Y_slide_windows_10-75_5steps_games.txt\",\"r\"):\n  stripped_line = line.strip()\n  y.append(int(stripped_line))\n  if c==(matches*steps_per_match)-1:\n    break\n  c+=1\n  \ny = np.array(y, dtype=int)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c2ac607a-ff3f-4062-bb63-31a9be85f9b2"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.cp(\"dbfs:/tmp/X_slide_windows_10-75_5steps_games.txt\", \"file:/tmp/X_slide_windows_10-75_5steps_games.txt\")\nx=[]\nc=0\nfor line in open(\"/tmp/X_slide_windows_10-75_5steps_games.txt\",\"r\"):\n  stripped_line = line.strip()\n  st=stripped_line.replace(\"[\",\"\").replace(\"]\",\"\")\n  s=st.split(\",\")\n  s=[int(x) for x in s]\n  x.append(s)\n  if c==(matches*steps_per_match)-1:\n    break\n  c+=1\n\nx = np.array(x, dtype=int)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"907a7854-9be2-43aa-80b2-2baec82e24ec"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["In the following cells, the train and test set are created. 20% of the matches are used for the test set."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eb2e37a4-4e88-4281-b1fd-b6308f4c735a"}}},{"cell_type":"code","source":["print(\"TRAIN X\")\ntrain_x= x[:int(len(x)*0.8)]\nprint(\"The number of elements in train_x is\",len(train_x), \"(80% of the dataset)\")\n\nprint(\"TRAIN Y\")\ntrain_y = y[:int(len(y)*0.8)]\nprint(\"The number of elements in train_y is\",len(train_y), \"(80% of the dataset)\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"16e96dde-66f3-44bc-9f1d-fc4bb4e7094f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"TRAIN X\nThe number of elements in train_x is 840000 (80% of the dataset)\nTRAIN Y\nThe number of elements in train_y is 840000 (80% of the dataset)\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["TRAIN X\nThe number of elements in train_x is 840000 (80% of the dataset)\nTRAIN Y\nThe number of elements in train_y is 840000 (80% of the dataset)\n"]}}],"execution_count":0},{"cell_type":"code","source":["print(\"TEST X\")\ntest_x = x[-int(len(x)*0.2):] \nprint(\"The number of elements in test_x is\",len(test_x), \"(20% of the dataset)\")\n\nprint(\"TEST Y\")\ntest_y= y[-int(len(y)*0.2):] \nprint(\"The number of elements in test_y is\",len(test_y), \"(20% of the dataset)\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"32263186-73c3-48fa-8f4f-5625513ba88e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"TEST X\nThe number of elements in test_x is 210000 (20% of the dataset)\nTEST Y\nThe number of elements in test_y is 210000 (20% of the dataset)\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["TEST X\nThe number of elements in test_x is 210000 (20% of the dataset)\nTEST Y\nThe number of elements in test_y is 210000 (20% of the dataset)\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["In this cell we see how the data are structured"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"51de810b-95ad-46ff-b3d0-470d1147ed5f"}}},{"cell_type":"code","source":["for i in range(0,len(test_x[:70])):\n  print(test_x[i], \"  \" , test_y[i])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"40f3bd26-fcc6-40b8-9adc-1b48219d7f70"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"[337 218 438 322 244]    464\n[218 438 322 244 464]    420\n[438 322 244 464 420]    219\n[322 244 464 420 219]    159\n[244 464 420 219 159]    167\n[464 420 219 159 167]    17\n[420 219 159 167  17]    225\n[219 159 167  17 225]    270\n[159 167  17 225 270]    33\n[167  17 225 270  33]    261\n[ 17 225 270  33 261]    377\n[225 270  33 261 377]    485\n[270  33 261 377 485]    182\n[ 33 261 377 485 182]    21\n[261 377 485 182  21]    303\n[377 485 182  21 303]    441\n[485 182  21 303 441]    265\n[182  21 303 441 265]    138\n[ 21 303 441 265 138]    478\n[303 441 265 138 478]    190\n[441 265 138 478 190]    286\n[265 138 478 190 286]    98\n[138 478 190 286  98]    325\n[478 190 286  98 325]    110\n[190 286  98 325 110]    337\n[286  98 325 110 337]    96\n[ 98 325 110 337  96]    218\n[325 110 337  96 218]    280\n[110 337  96 218 280]    408\n[337  96 218 280 408]    447\n[ 96 218 280 408 447]    247\n[218 280 408 447 247]    301\n[280 408 447 247 301]    338\n[408 447 247 301 338]    522\n[447 247 301 338 522]    522\n[247 301 338 522 522]    522\n[301 338 522 522 522]    522\n[338 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["[337 218 438 322 244]    464\n[218 438 322 244 464]    420\n[438 322 244 464 420]    219\n[322 244 464 420 219]    159\n[244 464 420 219 159]    167\n[464 420 219 159 167]    17\n[420 219 159 167  17]    225\n[219 159 167  17 225]    270\n[159 167  17 225 270]    33\n[167  17 225 270  33]    261\n[ 17 225 270  33 261]    377\n[225 270  33 261 377]    485\n[270  33 261 377 485]    182\n[ 33 261 377 485 182]    21\n[261 377 485 182  21]    303\n[377 485 182  21 303]    441\n[485 182  21 303 441]    265\n[182  21 303 441 265]    138\n[ 21 303 441 265 138]    478\n[303 441 265 138 478]    190\n[441 265 138 478 190]    286\n[265 138 478 190 286]    98\n[138 478 190 286  98]    325\n[478 190 286  98 325]    110\n[190 286  98 325 110]    337\n[286  98 325 110 337]    96\n[ 98 325 110 337  96]    218\n[325 110 337  96 218]    280\n[110 337  96 218 280]    408\n[337  96 218 280 408]    447\n[ 96 218 280 408 447]    247\n[218 280 408 447 247]    301\n[280 408 447 247 301]    338\n[408 447 247 301 338]    522\n[447 247 301 338 522]    522\n[247 301 338 522 522]    522\n[301 338 522 522 522]    522\n[338 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["In the following cell, One Hot Encoding is applied on the labels of the train set."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f879e96e-20b2-4ffc-a9d2-2d485bf1b046"}}},{"cell_type":"code","source":["train_y = to_categorical(train_y)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e3889983-7d9c-4050-b83f-4302196ab3ae"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["In the following cell we define values that will be used for embedding layer of the model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1a9afbbb-14e6-4e75-9abb-6744f04de21c"}}},{"cell_type":"code","source":["seq_len=train_x.shape[1]\nvocab_size=len(int_moves)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ac7e266b-60ed-4a84-8b79-40b6f6f735fe"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Defining a callback to prevent overfitting"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dd3ad423-5a0d-4803-ab60-ccd1befa24a8"}}},{"cell_type":"code","source":["from tensorflow.keras.callbacks import EarlyStopping\ncallback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3286f26a-510a-4ee5-80f0-2029cba54993"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["In the next cell we have the model used for the train. The train of this model was performed on kaggle. The model uses two hidden LSTM layers with 100 memory cells each and a fully connected dense layer with 100 neurons connects to the hidden LSTM layers to interpret features extracted from the sequence. After the first lstm layer there is a 20% dropout layer to prevent overfitting of the data. The output layer predicts the next word as a single vector of the vocabulary size with a probability for each word in the vocabulary. A softmax activation function is used to ensure that the outputs have the normalized probability features."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e9887f22-5caa-4f76-91f1-f68074c22985"}}},{"cell_type":"code","source":["model = Sequential()\nmodel.add(Embedding(vocab_size, n_steps, input_length=seq_len))\nmodel.add(LSTM(100, return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(100))\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dense(vocab_size, activation='softmax'))\nprint(model.summary())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c833ab9e-d772-4ef7-bb97-3c993b01efb8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Model: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n embedding (Embedding)       (None, 5, 5)              2615      \n                                                                 \n lstm (LSTM)                 (None, 5, 100)            42400     \n                                                                 \n dropout (Dropout)           (None, 5, 100)            0         \n                                                                 \n lstm_1 (LSTM)               (None, 100)               80400     \n                                                                 \n dense (Dense)               (None, 100)               10100     \n                                                                 \n dense_1 (Dense)             (None, 523)               52823     \n                                                                 \n=================================================================\nTotal params: 188,338\nTrainable params: 188,338\nNon-trainable params: 0\n_________________________________________________________________\nNone\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Model: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n embedding (Embedding)       (None, 5, 5)              2615      \n                                                                 \n lstm (LSTM)                 (None, 5, 100)            42400     \n                                                                 \n dropout (Dropout)           (None, 5, 100)            0         \n                                                                 \n lstm_1 (LSTM)               (None, 100)               80400     \n                                                                 \n dense (Dense)               (None, 100)               10100     \n                                                                 \n dense_1 (Dense)             (None, 523)               52823     \n                                                                 \n=================================================================\nTotal params: 188,338\nTrainable params: 188,338\nNon-trainable params: 0\n_________________________________________________________________\nNone\n"]}}],"execution_count":0},{"cell_type":"code","source":["# compile model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n# fit model\nhistory=model.fit(train_x, train_y, batch_size=64, epochs=50, callbacks=[callback] )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3494c739-d6a7-4a3d-aae0-7f6b812744d7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Epoch 1/50\n\r    1/13125 [..............................] - ETA: 13:32:03 - loss: 6.2595 - accuracy: 0.0000e+00\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    3/13125 [..............................] - ETA: 6:14 - loss: 6.2575 - accuracy: 0.1927        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    6/13125 [..............................] - ETA: 5:19 - loss: 6.2546 - accuracy: 0.2214\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    9/13125 [..............................] - ETA: 5:05 - loss: 6.2490 - accuracy: 0.2552\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   12/13125 [..............................] - ETA: 5:02 - loss: 6.2411 - accuracy: 0.2773\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   15/13125 [..............................] - ETA: 4:56 - loss: 6.2303 - accuracy: 0.2812\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   18/13125 [..............................] - ETA: 4:53 - loss: 6.2141 - accuracy: 0.2795\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   20/13125 [..............................] - ETA: 4:57 - loss: 6.1979 - accuracy: 0.2805\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   23/13125 [..............................] - ETA: 4:54 - loss: 6.1504 - accuracy: 0.2874\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   26/13125 [..............................] - ETA: 4:53 - loss: 6.0811 - accuracy: 0.2885\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   29/13125 [..............................] - ETA: 4:50 - loss: 5.9863 - accuracy: 0.2861\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   32/13125 [..............................] - ETA: 4:49 - loss: 5.8799 - accuracy: 0.2827\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   35/13125 [..............................] - ETA: 4:47 - loss: 5.7930 - accuracy: 0.2826\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   38/13125 [..............................] - ETA: 4:45 - loss: 5.6974 - accuracy: 0.2829\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   41/13125 [..............................] - ETA: 4:44 - loss: 5.5863 - accuracy: 0.2858\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   44/13125 [..............................] - ETA: 4:43 - loss: 5.5098 - accuracy: 0.2855\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   47/13125 [..............................] - ETA: 4:44 - loss: 5.4559 - accuracy: 0.2819\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   50/13125 [..............................] - ETA: 4:44 - loss: 5.4227 - accuracy: 0.2763\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   53/13125 [..............................] - ETA: 4:45 - loss: 5.3769 - accuracy: 0.2751\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   55/13125 [..............................] - ETA: 4:49 - loss: 5.3385 - accuracy: 0.2741\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   57/13125 [..............................] - ETA: 4:53 - loss: 5.3053 - accuracy: 0.2747\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   59/13125 [..............................] - ETA: 4:56 - loss: 5.2859 - accuracy: 0.2725\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   62/13125 [..............................] - ETA: 4:56 - loss: 5.2520 - accuracy: 0.2702\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   65/13125 [..............................] - ETA: 4:55 - loss: 5.2157 - accuracy: 0.2690\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   68/13125 [..............................] - ETA: 4:53 - loss: 5.1654 - accuracy: 0.2714\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   71/13125 [..............................] - ETA: 4:53 - loss: 5.1278 - accuracy: 0.2711\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   74/13125 [..............................] - ETA: 4:52 - loss: 5.0942 - accuracy: 0.2713\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   77/13125 [..............................] - ETA: 4:52 - loss: 5.0531 - accuracy: 0.2729\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   80/13125 [..............................] - ETA: 4:51 - loss: 5.0172 - accuracy: 0.2732\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   83/13125 [..............................] - ETA: 4:50 - loss: 4.9856 - accuracy: 0.2733\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   86/13125 [..............................] - ETA: 4:50 - loss: 4.9563 - accuracy: 0.2736\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   89/13125 [..............................] - ETA: 4:49 - loss: 4.9341 - accuracy: 0.2733\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   92/13125 [..............................] - ETA: 4:48 - loss: 4.9150 - accuracy: 0.2724\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   95/13125 [..............................] - ETA: 4:48 - loss: 4.9026 - accuracy: 0.2712\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   98/13125 [..............................] - ETA: 4:47 - loss: 4.8842 - accuracy: 0.2707\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  101/13125 [..............................] - ETA: 4:46 - loss: 4.8630 - accuracy: 0.2709\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  104/13125 [..............................] - ETA: 4:46 - loss: 4.8390 - accuracy: 0.2719\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  107/13125 [..............................] - ETA: 4:45 - loss: 4.8237 - accuracy: 0.2716\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  110/13125 [..............................] - ETA: 4:45 - loss: 4.8111 - accuracy: 0.2709\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  113/13125 [..............................] - ETA: 4:45 - loss: 4.7918 - accuracy: 0.2716\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  116/13125 [..............................] - ETA: 4:46 - loss: 4.7881 - accuracy: 0.2697\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  118/13125 [..............................] - ETA: 4:47 - loss: 4.7717 - accuracy: 0.2712\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  121/13125 [..............................] - ETA: 4:47 - loss: 4.7584 - accuracy: 0.2716\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  124/13125 [..............................] - ETA: 4:46 - loss: 4.7404 - accuracy: 0.2724\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  127/13125 [..............................] - ETA: 4:45 - loss: 4.7202 - accuracy: 0.2730\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  130/13125 [..............................] - ETA: 4:45 - loss: 4.7127 - accuracy: 0.2724\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  133/13125 [..............................] - ETA: 4:44 - loss: 4.7001 - accuracy: 0.2726\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  135/13125 [..............................] - ETA: 4:46 - loss: 4.6821 - accuracy: 0.2744\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  137/13125 [..............................] - ETA: 4:47 - loss: 4.6757 - accuracy: 0.2741\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  139/13125 [..............................] - ETA: 4:48 - loss: 4.6697 - accuracy: 0.2741\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  141/13125 [..............................] - ETA: 4:50 - loss: 4.6622 - accuracy: 0.2743\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  143/13125 [..............................] - ETA: 4:51 - loss: 4.6561 - accuracy: 0.2738\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  146/13125 [..............................] - ETA: 4:50 - loss: 4.6454 - accuracy: 0.2744\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  149/13125 [..............................] - ETA: 4:50 - loss: 4.6348 - accuracy: 0.2751\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  152/13125 [..............................] - ETA: 4:50 - loss: 4.6233 - accuracy: 0.2752\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  154/13125 [..............................] - ETA: 4:50 - loss: 4.6182 - accuracy: 0.2749\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  156/13125 [..............................] - ETA: 4:51 - loss: 4.6065 - accuracy: 0.2757\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  158/13125 [..............................] - ETA: 4:54 - loss: 4.5959 - accuracy: 0.2764\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  160/13125 [..............................] - ETA: 4:55 - loss: 4.5893 - accuracy: 0.2765\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  162/13125 [..............................] - ETA: 4:57 - loss: 4.5863 - accuracy: 0.2758\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  164/13125 [..............................] - ETA: 4:58 - loss: 4.5856 - accuracy: 0.2752\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  167/13125 [..............................] - ETA: 4:58 - loss: 4.5748 - accuracy: 0.2757\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  170/13125 [..............................] - ETA: 4:58 - loss: 4.5645 - accuracy: 0.2765\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  173/13125 [..............................] - ETA: 4:57 - loss: 4.5607 - accuracy: 0.2756\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  176/13125 [..............................] - ETA: 4:57 - loss: 4.5543 - accuracy: 0.2753\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  179/13125 [..............................] - ETA: 4:56 - loss: 4.5440 - accuracy: 0.2760\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  181/13125 [..............................] - ETA: 4:57 - loss: 4.5424 - accuracy: 0.2756\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  183/13125 [..............................] - ETA: 4:58 - loss: 4.5300 - accuracy: 0.2771\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  185/13125 [..............................] - ETA: 5:00 - loss: 4.5228 - accuracy: 0.2772\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  188/13125 [..............................] - ETA: 5:01 - loss: 4.5193 - accuracy: 0.2768\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  191/13125 [..............................] - ETA: 5:01 - loss: 4.5160 - accuracy: 0.2759\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  194/13125 [..............................] - ETA: 5:01 - loss: 4.5080 - accuracy: 0.2763","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Epoch 1/50\n\r    1/13125 [..............................] - ETA: 13:32:03 - loss: 6.2595 - accuracy: 0.0000e+00\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    3/13125 [..............................] - ETA: 6:14 - loss: 6.2575 - accuracy: 0.1927        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    6/13125 [..............................] - ETA: 5:19 - loss: 6.2546 - accuracy: 0.2214\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    9/13125 [..............................] - ETA: 5:05 - loss: 6.2490 - accuracy: 0.2552\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   12/13125 [..............................] - ETA: 5:02 - loss: 6.2411 - accuracy: 0.2773\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   15/13125 [..............................] - ETA: 4:56 - loss: 6.2303 - accuracy: 0.2812\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   18/13125 [..............................] - ETA: 4:53 - loss: 6.2141 - accuracy: 0.2795\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   20/13125 [..............................] - ETA: 4:57 - loss: 6.1979 - accuracy: 0.2805\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   23/13125 [..............................] - ETA: 4:54 - loss: 6.1504 - accuracy: 0.2874\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   26/13125 [..............................] - ETA: 4:53 - loss: 6.0811 - accuracy: 0.2885\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   29/13125 [..............................] - ETA: 4:50 - loss: 5.9863 - accuracy: 0.2861\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   32/13125 [..............................] - ETA: 4:49 - loss: 5.8799 - accuracy: 0.2827\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   35/13125 [..............................] - ETA: 4:47 - loss: 5.7930 - accuracy: 0.2826\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   38/13125 [..............................] - ETA: 4:45 - loss: 5.6974 - accuracy: 0.2829\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   41/13125 [..............................] - ETA: 4:44 - loss: 5.5863 - accuracy: 0.2858\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   44/13125 [..............................] - ETA: 4:43 - loss: 5.5098 - accuracy: 0.2855\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   47/13125 [..............................] - ETA: 4:44 - loss: 5.4559 - accuracy: 0.2819\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   50/13125 [..............................] - ETA: 4:44 - loss: 5.4227 - accuracy: 0.2763\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   53/13125 [..............................] - ETA: 4:45 - loss: 5.3769 - accuracy: 0.2751\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   55/13125 [..............................] - ETA: 4:49 - loss: 5.3385 - accuracy: 0.2741\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   57/13125 [..............................] - ETA: 4:53 - loss: 5.3053 - accuracy: 0.2747\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   59/13125 [..............................] - ETA: 4:56 - loss: 5.2859 - accuracy: 0.2725\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   62/13125 [..............................] - ETA: 4:56 - loss: 5.2520 - accuracy: 0.2702\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   65/13125 [..............................] - ETA: 4:55 - loss: 5.2157 - accuracy: 0.2690\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   68/13125 [..............................] - ETA: 4:53 - loss: 5.1654 - accuracy: 0.2714\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   71/13125 [..............................] - ETA: 4:53 - loss: 5.1278 - accuracy: 0.2711\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   74/13125 [..............................] - ETA: 4:52 - loss: 5.0942 - accuracy: 0.2713\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   77/13125 [..............................] - ETA: 4:52 - loss: 5.0531 - accuracy: 0.2729\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   80/13125 [..............................] - ETA: 4:51 - loss: 5.0172 - accuracy: 0.2732\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   83/13125 [..............................] - ETA: 4:50 - loss: 4.9856 - accuracy: 0.2733\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   86/13125 [..............................] - ETA: 4:50 - loss: 4.9563 - accuracy: 0.2736\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   89/13125 [..............................] - ETA: 4:49 - loss: 4.9341 - accuracy: 0.2733\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   92/13125 [..............................] - ETA: 4:48 - loss: 4.9150 - accuracy: 0.2724\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   95/13125 [..............................] - ETA: 4:48 - loss: 4.9026 - accuracy: 0.2712\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   98/13125 [..............................] - ETA: 4:47 - loss: 4.8842 - accuracy: 0.2707\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  101/13125 [..............................] - ETA: 4:46 - loss: 4.8630 - accuracy: 0.2709\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  104/13125 [..............................] - ETA: 4:46 - loss: 4.8390 - accuracy: 0.2719\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  107/13125 [..............................] - ETA: 4:45 - loss: 4.8237 - accuracy: 0.2716\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  110/13125 [..............................] - ETA: 4:45 - loss: 4.8111 - accuracy: 0.2709\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  113/13125 [..............................] - ETA: 4:45 - loss: 4.7918 - accuracy: 0.2716\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  116/13125 [..............................] - ETA: 4:46 - loss: 4.7881 - accuracy: 0.2697\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  118/13125 [..............................] - ETA: 4:47 - loss: 4.7717 - accuracy: 0.2712\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  121/13125 [..............................] - ETA: 4:47 - loss: 4.7584 - accuracy: 0.2716\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  124/13125 [..............................] - ETA: 4:46 - loss: 4.7404 - accuracy: 0.2724\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  127/13125 [..............................] - ETA: 4:45 - loss: 4.7202 - accuracy: 0.2730\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  130/13125 [..............................] - ETA: 4:45 - loss: 4.7127 - accuracy: 0.2724\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  133/13125 [..............................] - ETA: 4:44 - loss: 4.7001 - accuracy: 0.2726\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  135/13125 [..............................] - ETA: 4:46 - loss: 4.6821 - accuracy: 0.2744\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  137/13125 [..............................] - ETA: 4:47 - loss: 4.6757 - accuracy: 0.2741\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  139/13125 [..............................] - ETA: 4:48 - loss: 4.6697 - accuracy: 0.2741\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  141/13125 [..............................] - ETA: 4:50 - loss: 4.6622 - accuracy: 0.2743\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  143/13125 [..............................] - ETA: 4:51 - loss: 4.6561 - accuracy: 0.2738\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  146/13125 [..............................] - ETA: 4:50 - loss: 4.6454 - accuracy: 0.2744\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  149/13125 [..............................] - ETA: 4:50 - loss: 4.6348 - accuracy: 0.2751\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  152/13125 [..............................] - ETA: 4:50 - loss: 4.6233 - accuracy: 0.2752\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  154/13125 [..............................] - ETA: 4:50 - loss: 4.6182 - accuracy: 0.2749\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  156/13125 [..............................] - ETA: 4:51 - loss: 4.6065 - accuracy: 0.2757\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  158/13125 [..............................] - ETA: 4:54 - loss: 4.5959 - accuracy: 0.2764\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  160/13125 [..............................] - ETA: 4:55 - loss: 4.5893 - accuracy: 0.2765\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  162/13125 [..............................] - ETA: 4:57 - loss: 4.5863 - accuracy: 0.2758\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  164/13125 [..............................] - ETA: 4:58 - loss: 4.5856 - accuracy: 0.2752\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  167/13125 [..............................] - ETA: 4:58 - loss: 4.5748 - accuracy: 0.2757\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  170/13125 [..............................] - ETA: 4:58 - loss: 4.5645 - accuracy: 0.2765\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  173/13125 [..............................] - ETA: 4:57 - loss: 4.5607 - accuracy: 0.2756\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  176/13125 [..............................] - ETA: 4:57 - loss: 4.5543 - accuracy: 0.2753\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  179/13125 [..............................] - ETA: 4:56 - loss: 4.5440 - accuracy: 0.2760\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  181/13125 [..............................] - ETA: 4:57 - loss: 4.5424 - accuracy: 0.2756\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  183/13125 [..............................] - ETA: 4:58 - loss: 4.5300 - accuracy: 0.2771\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  185/13125 [..............................] - ETA: 5:00 - loss: 4.5228 - accuracy: 0.2772\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  188/13125 [..............................] - ETA: 5:01 - loss: 4.5193 - accuracy: 0.2768\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  191/13125 [..............................] - ETA: 5:01 - loss: 4.5160 - accuracy: 0.2759\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  194/13125 [..............................] - ETA: 5:01 - loss: 4.5080 - accuracy: 0.2763"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Cancelled","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Loading the trained model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"936d7d54-7f23-4b76-a7e4-5288f7f56cf9"}}},{"cell_type":"code","source":["%sh wget https://www.dropbox.com/s/o4axduv8xpoblgh/OnlyInitialCell_NOFREQ_Kaggle_model.h5?dl=1 -O /tmp/Kaggle_NoFreq_100Units_onlyInitialCell.csv"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bf6d9c8c-170d-4fe6-b52b-b79420461e06"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"--2022-06-04 11:55:42--  https://www.dropbox.com/s/o4axduv8xpoblgh/OnlyInitialCell_NOFREQ_Kaggle_model.h5?dl=1\nResolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6016:18::a27d:112\nConnecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: /s/dl/o4axduv8xpoblgh/OnlyInitialCell_NOFREQ_Kaggle_model.h5 [following]\n--2022-06-04 11:55:43--  https://www.dropbox.com/s/dl/o4axduv8xpoblgh/OnlyInitialCell_NOFREQ_Kaggle_model.h5\nReusing existing connection to www.dropbox.com:443.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://uc4ec578d802b7d8ef0b1e0b4392.dl.dropboxusercontent.com/cd/0/get/Bmi9Bi2v6ZuTE8cTaXZAjfDhCfCGmGH3pjdkinMxevjMFqfwLMQgcNgcfQ89SaO5f1KQyqXuewy5J81kzJsg7vqPJTgG4fQ489Y4bDNpgPacONU0Zr0EgS-0E5Edq0uRD0GHDFj2Rkw-Iq-t8W4DsAfPyg5nuGBXOlydkvLTIS1wkIXGlO_nJMGRGBNl_YA3yTY/file?dl=1# [following]\n--2022-06-04 11:55:43--  https://uc4ec578d802b7d8ef0b1e0b4392.dl.dropboxusercontent.com/cd/0/get/Bmi9Bi2v6ZuTE8cTaXZAjfDhCfCGmGH3pjdkinMxevjMFqfwLMQgcNgcfQ89SaO5f1KQyqXuewy5J81kzJsg7vqPJTgG4fQ489Y4bDNpgPacONU0Zr0EgS-0E5Edq0uRD0GHDFj2Rkw-Iq-t8W4DsAfPyg5nuGBXOlydkvLTIS1wkIXGlO_nJMGRGBNl_YA3yTY/file?dl=1\nResolving uc4ec578d802b7d8ef0b1e0b4392.dl.dropboxusercontent.com (uc4ec578d802b7d8ef0b1e0b4392.dl.dropboxusercontent.com)... 162.125.8.15, 2620:100:6016:15::a27d:10f\nConnecting to uc4ec578d802b7d8ef0b1e0b4392.dl.dropboxusercontent.com (uc4ec578d802b7d8ef0b1e0b4392.dl.dropboxusercontent.com)|162.125.8.15|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2318200 (2.2M) [application/binary]\nSaving to: /tmp/Kaggle_NoFreq_100Units_onlyInitialCell.csv\n\n     0K .......... .......... .......... .......... ..........  2%  490K 5s\n    50K .......... .......... .......... .......... ..........  4%  983K 3s\n   100K .......... .......... .......... .......... ..........  6% 1.19M 3s\n   150K .......... .......... .......... .......... ..........  8% 1.48M 2s\n   200K .......... .......... .......... .......... .......... 11% 2.87M 2s\n   250K .......... .......... .......... .......... .......... 13% 2.61M 2s\n   300K .......... .......... .......... .......... .......... 15% 3.02M 2s\n   350K .......... .......... .......... .......... .......... 17% 3.09M 1s\n   400K .......... .......... .......... .......... .......... 19% 5.19M 1s\n   450K .......... .......... .......... .......... .......... 22% 5.41M 1s\n   500K .......... .......... .......... .......... .......... 24% 6.39M 1s\n   550K .......... .......... .......... .......... .......... 26% 7.71M 1s\n   600K .......... .......... .......... .......... .......... 28% 7.94M 1s\n   650K .......... .......... .......... .......... .......... 30% 7.94M 1s\n   700K .......... .......... .......... .......... .......... 33% 9.49M 1s\n   750K .......... .......... .......... .......... .......... 35% 6.24M 1s\n   800K .......... .......... .......... .......... .......... 37% 9.11M 1s\n   850K .......... .......... .......... .......... .......... 39% 10.2M 1s\n   900K .......... .......... .......... .......... .......... 41% 11.1M 1s\n   950K .......... .......... .......... .......... .......... 44% 10.8M 0s\n  1000K .......... .......... .......... .......... .......... 46% 12.4M 0s\n  1050K .......... .......... .......... .......... .......... 48% 11.5M 0s\n  1100K .......... .......... .......... .......... .......... 50% 13.5M 0s\n  1150K .......... .......... .......... .......... .......... 53% 12.5M 0s\n  1200K .......... .......... .......... .......... .......... 55% 13.6M 0s\n  1250K .......... .......... .......... .......... .......... 57% 19.0M 0s\n  1300K .......... .......... .......... .......... .......... 59% 19.8M 0s\n  1350K .......... .......... .......... .......... .......... 61% 16.9M 0s\n  1400K .......... .......... .......... .......... .......... 64% 16.7M 0s\n  1450K .......... .......... .......... .......... .......... 66% 20.2M 0s\n  1500K .......... .......... .......... .......... .......... 68% 19.5M 0s\n  1550K .......... .......... .......... .......... .......... 70% 15.6M 0s\n  1600K .......... .......... .......... .......... .......... 72% 15.9M 0s\n  1650K .......... .......... .......... .......... .......... 75% 22.6M 0s\n  1700K .......... .......... .......... .......... .......... 77% 27.6M 0s\n  1750K .......... .......... .......... .......... .......... 79% 22.1M 0s\n  1800K .......... .......... .......... .......... .......... 81% 17.8M 0s\n  1850K .......... .......... .......... .......... .......... 83% 27.6M 0s\n  1900K .......... .......... .......... .......... .......... 86% 24.8M 0s\n  1950K .......... .......... .......... .......... .......... 88% 19.6M 0s\n  2000K .......... .......... .......... .......... .......... 90% 27.4M 0s\n  2050K .......... .......... .......... .......... .......... 92% 15.5M 0s\n  2100K .......... .......... .......... .......... .......... 94% 42.1M 0s\n  2150K .......... .......... .......... .......... .......... 97% 30.2M 0s\n  2200K .......... .......... .......... .......... .......... 99% 33.2M 0s\n  2250K .......... ...                                        100% 16.1M=0.4s\n\n2022-06-04 11:55:44 (5.05 MB/s) - /tmp/Kaggle_NoFreq_100Units_onlyInitialCell.csv saved [2318200/2318200]\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["--2022-06-04 11:55:42--  https://www.dropbox.com/s/o4axduv8xpoblgh/OnlyInitialCell_NOFREQ_Kaggle_model.h5?dl=1\nResolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6016:18::a27d:112\nConnecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: /s/dl/o4axduv8xpoblgh/OnlyInitialCell_NOFREQ_Kaggle_model.h5 [following]\n--2022-06-04 11:55:43--  https://www.dropbox.com/s/dl/o4axduv8xpoblgh/OnlyInitialCell_NOFREQ_Kaggle_model.h5\nReusing existing connection to www.dropbox.com:443.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://uc4ec578d802b7d8ef0b1e0b4392.dl.dropboxusercontent.com/cd/0/get/Bmi9Bi2v6ZuTE8cTaXZAjfDhCfCGmGH3pjdkinMxevjMFqfwLMQgcNgcfQ89SaO5f1KQyqXuewy5J81kzJsg7vqPJTgG4fQ489Y4bDNpgPacONU0Zr0EgS-0E5Edq0uRD0GHDFj2Rkw-Iq-t8W4DsAfPyg5nuGBXOlydkvLTIS1wkIXGlO_nJMGRGBNl_YA3yTY/file?dl=1# [following]\n--2022-06-04 11:55:43--  https://uc4ec578d802b7d8ef0b1e0b4392.dl.dropboxusercontent.com/cd/0/get/Bmi9Bi2v6ZuTE8cTaXZAjfDhCfCGmGH3pjdkinMxevjMFqfwLMQgcNgcfQ89SaO5f1KQyqXuewy5J81kzJsg7vqPJTgG4fQ489Y4bDNpgPacONU0Zr0EgS-0E5Edq0uRD0GHDFj2Rkw-Iq-t8W4DsAfPyg5nuGBXOlydkvLTIS1wkIXGlO_nJMGRGBNl_YA3yTY/file?dl=1\nResolving uc4ec578d802b7d8ef0b1e0b4392.dl.dropboxusercontent.com (uc4ec578d802b7d8ef0b1e0b4392.dl.dropboxusercontent.com)... 162.125.8.15, 2620:100:6016:15::a27d:10f\nConnecting to uc4ec578d802b7d8ef0b1e0b4392.dl.dropboxusercontent.com (uc4ec578d802b7d8ef0b1e0b4392.dl.dropboxusercontent.com)|162.125.8.15|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2318200 (2.2M) [application/binary]\nSaving to: /tmp/Kaggle_NoFreq_100Units_onlyInitialCell.csv\n\n     0K .......... .......... .......... .......... ..........  2%  490K 5s\n    50K .......... .......... .......... .......... ..........  4%  983K 3s\n   100K .......... .......... .......... .......... ..........  6% 1.19M 3s\n   150K .......... .......... .......... .......... ..........  8% 1.48M 2s\n   200K .......... .......... .......... .......... .......... 11% 2.87M 2s\n   250K .......... .......... .......... .......... .......... 13% 2.61M 2s\n   300K .......... .......... .......... .......... .......... 15% 3.02M 2s\n   350K .......... .......... .......... .......... .......... 17% 3.09M 1s\n   400K .......... .......... .......... .......... .......... 19% 5.19M 1s\n   450K .......... .......... .......... .......... .......... 22% 5.41M 1s\n   500K .......... .......... .......... .......... .......... 24% 6.39M 1s\n   550K .......... .......... .......... .......... .......... 26% 7.71M 1s\n   600K .......... .......... .......... .......... .......... 28% 7.94M 1s\n   650K .......... .......... .......... .......... .......... 30% 7.94M 1s\n   700K .......... .......... .......... .......... .......... 33% 9.49M 1s\n   750K .......... .......... .......... .......... .......... 35% 6.24M 1s\n   800K .......... .......... .......... .......... .......... 37% 9.11M 1s\n   850K .......... .......... .......... .......... .......... 39% 10.2M 1s\n   900K .......... .......... .......... .......... .......... 41% 11.1M 1s\n   950K .......... .......... .......... .......... .......... 44% 10.8M 0s\n  1000K .......... .......... .......... .......... .......... 46% 12.4M 0s\n  1050K .......... .......... .......... .......... .......... 48% 11.5M 0s\n  1100K .......... .......... .......... .......... .......... 50% 13.5M 0s\n  1150K .......... .......... .......... .......... .......... 53% 12.5M 0s\n  1200K .......... .......... .......... .......... .......... 55% 13.6M 0s\n  1250K .......... .......... .......... .......... .......... 57% 19.0M 0s\n  1300K .......... .......... .......... .......... .......... 59% 19.8M 0s\n  1350K .......... .......... .......... .......... .......... 61% 16.9M 0s\n  1400K .......... .......... .......... .......... .......... 64% 16.7M 0s\n  1450K .......... .......... .......... .......... .......... 66% 20.2M 0s\n  1500K .......... .......... .......... .......... .......... 68% 19.5M 0s\n  1550K .......... .......... .......... .......... .......... 70% 15.6M 0s\n  1600K .......... .......... .......... .......... .......... 72% 15.9M 0s\n  1650K .......... .......... .......... .......... .......... 75% 22.6M 0s\n  1700K .......... .......... .......... .......... .......... 77% 27.6M 0s\n  1750K .......... .......... .......... .......... .......... 79% 22.1M 0s\n  1800K .......... .......... .......... .......... .......... 81% 17.8M 0s\n  1850K .......... .......... .......... .......... .......... 83% 27.6M 0s\n  1900K .......... .......... .......... .......... .......... 86% 24.8M 0s\n  1950K .......... .......... .......... .......... .......... 88% 19.6M 0s\n  2000K .......... .......... .......... .......... .......... 90% 27.4M 0s\n  2050K .......... .......... .......... .......... .......... 92% 15.5M 0s\n  2100K .......... .......... .......... .......... .......... 94% 42.1M 0s\n  2150K .......... .......... .......... .......... .......... 97% 30.2M 0s\n  2200K .......... .......... .......... .......... .......... 99% 33.2M 0s\n  2250K .......... ...                                        100% 16.1M=0.4s\n\n2022-06-04 11:55:44 (5.05 MB/s) - /tmp/Kaggle_NoFreq_100Units_onlyInitialCell.csv saved [2318200/2318200]\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["model1_Initialmoves = keras.models.load_model(\"/tmp/Kaggle_NoFreq_100Units_onlyInitialCell.csv\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2cc564ec-ab26-4960-a5c3-a0bf2da97a3a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Plotting of the loss and accuracy values during the train"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"de317d96-7211-48a4-8e0e-3e10d255cf86"}}},{"cell_type":"code","source":["plt.plot(history.history['loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train'], loc='upper right')\nplt.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"14aa4115-cad4-44f9-934e-b4bafee95730"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["<img src=\"https://www.dropbox.com/s/gz0vh95qjw6rta7/model3_losss.png?dl=1\">"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bbb16589-df0a-4877-a3ff-e401ca1e58be"}}},{"cell_type":"code","source":["plt.plot(history.history['accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train'], loc='upper left')\nplt.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c2541a3d-cc15-49a5-aebb-0be68e8a40b7"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["<img src=\"https://www.dropbox.com/s/3jrbjcwr1u7dp92/model3_accuracy.png?dl=1\">"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"997362f9-8cb5-4780-8546-bb161741bb38"}}},{"cell_type":"markdown","source":["##### Second Model trained with a slightly different network\nIn the next cell we have the model used for the train performed on kaggle. The model uses two hidden LSTM layers with 256 memory cells each and a fully connected dense layer with 256 neurons connects to the hidden LSTM layers to interpret features extracted from the sequence. After the first lstm layer there is a 20% dropout layer to prevent overfitting of the data. The output layer predicts the next word as a single vector of the vocabulary size with a probability for each word in the vocabulary. A softmax activation function is used to ensure that the outputs have the normalized probability features."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a77d48ee-9a11-4f9f-a696-76a6cad8d49f"}}},{"cell_type":"code","source":["model = Sequential()\nmodel.add(Embedding(vocab_size, n_steps, input_length=seq_len))\nmodel.add(LSTM(256, return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(256))\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dense(vocab_size, activation='softmax'))\nprint(model.summary())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"61f89ab1-9144-4240-81c7-707361fc7a79"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Model: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n embedding_1 (Embedding)     (None, 5, 5)              2615      \n                                                                 \n lstm_2 (LSTM)               (None, 5, 256)            268288    \n                                                                 \n dropout_1 (Dropout)         (None, 5, 256)            0         \n                                                                 \n lstm_3 (LSTM)               (None, 256)               525312    \n                                                                 \n dense_2 (Dense)             (None, 256)               65792     \n                                                                 \n dense_3 (Dense)             (None, 523)               134411    \n                                                                 \n=================================================================\nTotal params: 996,418\nTrainable params: 996,418\nNon-trainable params: 0\n_________________________________________________________________\nNone\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Model: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n embedding_1 (Embedding)     (None, 5, 5)              2615      \n                                                                 \n lstm_2 (LSTM)               (None, 5, 256)            268288    \n                                                                 \n dropout_1 (Dropout)         (None, 5, 256)            0         \n                                                                 \n lstm_3 (LSTM)               (None, 256)               525312    \n                                                                 \n dense_2 (Dense)             (None, 256)               65792     \n                                                                 \n dense_3 (Dense)             (None, 523)               134411    \n                                                                 \n=================================================================\nTotal params: 996,418\nTrainable params: 996,418\nNon-trainable params: 0\n_________________________________________________________________\nNone\n"]}}],"execution_count":0},{"cell_type":"code","source":["# compile model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n# fit model\nhistory=model.fit(train_x, train_y, batch_size=64, epochs=50, callbacks=[callback] )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0100df0c-d237-4219-b66b-72c07880fc18"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Epoch 1/50\n\r    1/13125 [..............................] - ETA: 14:21:00 - loss: 6.2597 - accuracy: 0.0000e+00\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    2/13125 [..............................] - ETA: 21:59 - loss: 6.2580 - accuracy: 0.1172       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    3/13125 [..............................] - ETA: 21:04 - loss: 6.2562 - accuracy: 0.1615\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    4/13125 [..............................] - ETA: 19:16 - loss: 6.2539 - accuracy: 0.1797\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    5/13125 [..............................] - ETA: 18:10 - loss: 6.2507 - accuracy: 0.1969\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    6/13125 [..............................] - ETA: 17:48 - loss: 6.2467 - accuracy: 0.2083\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    7/13125 [..............................] - ETA: 17:16 - loss: 6.2407 - accuracy: 0.2210\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    8/13125 [..............................] - ETA: 16:55 - loss: 6.2316 - accuracy: 0.2383\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    9/13125 [..............................] - ETA: 17:03 - loss: 6.2258 - accuracy: 0.2326\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   10/13125 [..............................] - ETA: 16:44 - loss: 6.2090 - accuracy: 0.2469\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   11/13125 [..............................] - ETA: 16:30 - loss: 6.1921 - accuracy: 0.2528\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   12/13125 [..............................] - ETA: 16:22 - loss: 6.1696 - accuracy: 0.2565\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   13/13125 [..............................] - ETA: 16:45 - loss: 6.1286 - accuracy: 0.2680\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   14/13125 [..............................] - ETA: 16:42 - loss: 6.0921 - accuracy: 0.2690\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   15/13125 [..............................] - ETA: 16:38 - loss: 6.0288 - accuracy: 0.2750\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   16/13125 [..............................] - ETA: 16:34 - loss: 5.9517 - accuracy: 0.2783\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   17/13125 [..............................] - ETA: 16:39 - loss: 5.8754 - accuracy: 0.2785\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   18/13125 [..............................] - ETA: 16:55 - loss: 5.8624 - accuracy: 0.2734\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   19/13125 [..............................] - ETA: 16:49 - loss: 5.7842 - accuracy: 0.2796\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   20/13125 [..............................] - ETA: 16:42 - loss: 5.7296 - accuracy: 0.2797\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   21/13125 [..............................] - ETA: 16:39 - loss: 5.6784 - accuracy: 0.2805","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Epoch 1/50\n\r    1/13125 [..............................] - ETA: 14:21:00 - loss: 6.2597 - accuracy: 0.0000e+00\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    2/13125 [..............................] - ETA: 21:59 - loss: 6.2580 - accuracy: 0.1172       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    3/13125 [..............................] - ETA: 21:04 - loss: 6.2562 - accuracy: 0.1615\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    4/13125 [..............................] - ETA: 19:16 - loss: 6.2539 - accuracy: 0.1797\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    5/13125 [..............................] - ETA: 18:10 - loss: 6.2507 - accuracy: 0.1969\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    6/13125 [..............................] - ETA: 17:48 - loss: 6.2467 - accuracy: 0.2083\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    7/13125 [..............................] - ETA: 17:16 - loss: 6.2407 - accuracy: 0.2210\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    8/13125 [..............................] - ETA: 16:55 - loss: 6.2316 - accuracy: 0.2383\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    9/13125 [..............................] - ETA: 17:03 - loss: 6.2258 - accuracy: 0.2326\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   10/13125 [..............................] - ETA: 16:44 - loss: 6.2090 - accuracy: 0.2469\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   11/13125 [..............................] - ETA: 16:30 - loss: 6.1921 - accuracy: 0.2528\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   12/13125 [..............................] - ETA: 16:22 - loss: 6.1696 - accuracy: 0.2565\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   13/13125 [..............................] - ETA: 16:45 - loss: 6.1286 - accuracy: 0.2680\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   14/13125 [..............................] - ETA: 16:42 - loss: 6.0921 - accuracy: 0.2690\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   15/13125 [..............................] - ETA: 16:38 - loss: 6.0288 - accuracy: 0.2750\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   16/13125 [..............................] - ETA: 16:34 - loss: 5.9517 - accuracy: 0.2783\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   17/13125 [..............................] - ETA: 16:39 - loss: 5.8754 - accuracy: 0.2785\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   18/13125 [..............................] - ETA: 16:55 - loss: 5.8624 - accuracy: 0.2734\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   19/13125 [..............................] - ETA: 16:49 - loss: 5.7842 - accuracy: 0.2796\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   20/13125 [..............................] - ETA: 16:42 - loss: 5.7296 - accuracy: 0.2797\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   21/13125 [..............................] - ETA: 16:39 - loss: 5.6784 - accuracy: 0.2805"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Cancelled","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Loading the second trained model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b2f0eafb-20dc-44cd-9872-47313de39890"}}},{"cell_type":"code","source":["%sh wget https://www.dropbox.com/s/zzr0xjf5yi78fqz/model_OnlyInitial_NoFreq_256Units.h5?dl=1 -O /tmp/Kaggle_NoFreq_256Units_onlyInitialCell.csv"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eacd4c9e-0589-489a-bf69-d5da52eb62d7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"--2022-06-04 11:55:56--  https://www.dropbox.com/s/zzr0xjf5yi78fqz/model_OnlyInitial_NoFreq_256Units.h5?dl=1\nResolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6016:18::a27d:112\nConnecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: /s/dl/zzr0xjf5yi78fqz/model_OnlyInitial_NoFreq_256Units.h5 [following]\n--2022-06-04 11:55:56--  https://www.dropbox.com/s/dl/zzr0xjf5yi78fqz/model_OnlyInitial_NoFreq_256Units.h5\nReusing existing connection to www.dropbox.com:443.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://uc9cc8b0ed55d1f314a73bdb543f.dl.dropboxusercontent.com/cd/0/get/Bmgue4yJjXvNFb7o8_2EEFxYeslypHZBw9GzHjPJbJMa7GKYZG7MMrg-Uwb8WCQeKGygdyZYcR56GetHxYeS-0xK1uZcG50FmU0dI1hI0PgGBlSQi0s8VafjxTvM6bTZ_JE0cYr3lGpQN3O8QPyTCyxjajvUCZqkReooHZaI0xmO4BMmdM1U2j9x_qHGnAkjJ_E/file?dl=1# [following]\n--2022-06-04 11:55:56--  https://uc9cc8b0ed55d1f314a73bdb543f.dl.dropboxusercontent.com/cd/0/get/Bmgue4yJjXvNFb7o8_2EEFxYeslypHZBw9GzHjPJbJMa7GKYZG7MMrg-Uwb8WCQeKGygdyZYcR56GetHxYeS-0xK1uZcG50FmU0dI1hI0PgGBlSQi0s8VafjxTvM6bTZ_JE0cYr3lGpQN3O8QPyTCyxjajvUCZqkReooHZaI0xmO4BMmdM1U2j9x_qHGnAkjJ_E/file?dl=1\nResolving uc9cc8b0ed55d1f314a73bdb543f.dl.dropboxusercontent.com (uc9cc8b0ed55d1f314a73bdb543f.dl.dropboxusercontent.com)... 162.125.8.15, 2620:100:6016:15::a27d:10f\nConnecting to uc9cc8b0ed55d1f314a73bdb543f.dl.dropboxusercontent.com (uc9cc8b0ed55d1f314a73bdb543f.dl.dropboxusercontent.com)|162.125.8.15|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 12015296 (11M) [application/binary]\nSaving to: /tmp/Kaggle_NoFreq_256Units_onlyInitialCell.csv\n\n     0K .......... .......... .......... .......... ..........  0%  490K 24s\n    50K .......... .......... .......... .......... ..........  0%  984K 18s\n   100K .......... .......... .......... .......... ..........  1% 1.19M 15s\n   150K .......... .......... .......... .......... ..........  1% 1.49M 13s\n   200K .......... .......... .......... .......... ..........  2% 2.88M 11s\n   250K .......... .......... .......... .......... ..........  2% 2.62M 10s\n   300K .......... .......... .......... .......... ..........  2% 3.01M 9s\n   350K .......... .......... .......... .......... ..........  3% 3.11M 8s\n   400K .......... .......... .......... .......... ..........  3% 5.18M 8s\n   450K .......... .......... .......... .......... ..........  4% 5.93M 7s\n   500K .......... .......... .......... .......... ..........  4% 5.89M 7s\n   550K .......... .......... .......... .......... ..........  5% 7.95M 6s\n   600K .......... .......... .......... .......... ..........  5% 7.76M 6s\n   650K .......... .......... .......... .......... ..........  5% 7.94M 5s\n   700K .......... .......... .......... .......... ..........  6% 9.52M 5s\n   750K .......... .......... .......... .......... ..........  6% 6.24M 5s\n   800K .......... .......... .......... .......... ..........  7% 8.96M 5s\n   850K .......... .......... .......... .......... ..........  7% 10.5M 4s\n   900K .......... .......... .......... .......... ..........  8% 10.6M 4s\n   950K .......... .......... .......... .......... ..........  8% 11.2M 4s\n  1000K .......... .......... .......... .......... ..........  8% 12.0M 4s\n  1050K .......... .......... .......... .......... ..........  9% 12.0M 4s\n  1100K .......... .......... .......... .......... ..........  9% 13.5M 4s\n  1150K .......... .......... .......... .......... .......... 10% 12.8M 3s\n  1200K .......... .......... .......... .......... .......... 10% 13.8M 3s\n  1250K .......... .......... .......... .......... .......... 11% 21.5M 3s\n  1300K .......... .......... .......... .......... .......... 11% 16.7M 3s\n  1350K .......... .......... .......... .......... .......... 11% 17.5M 3s\n  1400K .......... .......... .......... .......... .......... 12% 17.6M 3s\n  1450K .......... .......... .......... .......... .......... 12% 18.9M 3s\n  1500K .......... .......... .......... .......... .......... 13% 20.5M 3s\n  1550K .......... .......... .......... .......... .......... 13% 15.4M 3s\n  1600K .......... .......... .......... .......... .......... 14% 15.6M 3s\n  1650K .......... .......... .......... .......... .......... 14% 25.6M 2s\n  1700K .......... .......... .......... .......... .......... 14% 22.5M 2s\n  1750K .......... .......... .......... .......... .......... 15% 23.1M 2s\n  1800K .......... .......... .......... .......... .......... 15% 15.7M 2s\n  1850K .......... .......... .......... .......... .......... 16% 26.9M 2s\n  1900K .......... .......... .......... .......... .......... 16% 25.9M 2s\n  1950K .......... .......... .......... .......... .......... 17% 21.5M 2s\n  2000K .......... .......... .......... .......... .......... 17% 28.8M 2s\n  2050K .......... .......... .......... .......... .......... 17% 21.6M 2s\n  2100K .......... .......... .......... .......... .......... 18% 26.2M 2s\n  2150K .......... .......... .......... .......... .......... 18% 28.4M 2s\n  2200K .......... .......... .......... .......... .......... 19% 32.6M 2s\n  2250K .......... .......... .......... .......... .......... 19% 19.8M 2s\n  2300K .......... .......... .......... .......... .......... 20% 45.1M 2s\n  2350K .......... .......... .......... .......... .......... 20% 25.4M 2s\n  2400K .......... .......... .......... .......... .......... 20% 22.6M 2s\n  2450K .......... .......... .......... .......... .......... 21% 51.5M 2s\n  2500K .......... .......... .......... .......... .......... 21% 23.8M 2s\n  2550K .......... .......... .......... .......... .......... 22% 17.6M 2s\n  2600K .......... .......... .......... .......... .......... 22%  122M 2s\n  2650K .......... .......... .......... .......... .......... 23% 56.4M 2s\n  2700K .......... .......... .......... .......... .......... 23% 26.5M 1s\n  2750K .......... .......... .......... .......... .......... 23% 27.9M 1s\n  2800K .......... .......... .......... .......... .......... 24% 45.4M 1s\n  2850K .......... .......... .......... .......... .......... 24% 28.8M 1s\n  2900K .......... .......... .......... .......... .......... 25% 45.6M 1s\n  2950K .......... .......... .......... .......... .......... 25% 31.8M 1s\n  3000K .......... .......... .......... .......... .......... 25% 34.1M 1s\n  3050K .......... .......... .......... .......... .......... 26% 35.2M 1s\n  3100K .......... .......... .......... .......... .......... 26% 40.3M 1s\n  3150K .......... .......... .......... .......... .......... 27% 13.4M 1s\n  3200K .......... .......... .......... .......... .......... 27% 22.5M 1s\n  3250K .......... .......... .......... .......... .......... 28% 40.5M 1s\n  3300K .......... .......... .......... .......... .......... 28% 16.0M 1s\n  3350K .......... .......... .......... .......... .......... 28% 24.0M 1s\n  3400K .......... .......... .......... .......... .......... 29% 20.3M 1s\n  3450K .......... .......... .......... .......... .......... 29% 53.1M 1s\n  3500K .......... .......... .......... .......... .......... 30% 16.8M 1s\n  3550K .......... .......... .......... .......... .......... 30% 27.2M 1s\n  3600K .......... .......... .......... .......... .......... 31% 23.5M 1s\n  3650K .......... .......... .......... .......... .......... 31% 20.3M 1s\n  3700K .......... .......... .......... .......... .......... 31% 45.1M 1s\n  3750K .......... .......... .......... .......... .......... 32% 22.0M 1s\n  3800K .......... .......... .......... .......... .......... 32% 25.7M 1s\n  3850K .......... .......... .......... .......... .......... 33% 15.8M 1s\n  3900K .......... .......... .......... .......... .......... 33% 41.0M 1s\n  3950K .......... .......... .......... .......... .......... 34% 19.0M 1s\n  4000K .......... .......... .......... .......... .......... 34% 95.3M 1s\n  4050K .......... .......... .......... .......... .......... 34% 24.7M 1s\n  4100K .......... .......... .......... .......... .......... 35% 22.4M 1s\n  4150K .......... .......... .......... .......... .......... 35% 32.8M 1s\n  4200K .......... .......... .......... .......... .......... 36%  136M 1s\n  4250K .......... .......... .......... .......... .......... 36% 27.7M 1s\n  4300K .......... .......... .......... .......... .......... 37% 27.0M 1s\n  4350K .......... .......... .......... .......... .......... 37% 29.3M 1s\n  4400K .......... .......... .......... .......... .......... 37% 31.6M 1s\n  4450K .......... .......... .......... .......... .......... 38%  131M 1s\n  4500K .......... .......... .......... .......... .......... 38% 26.9M 1s\n  4550K .......... .......... .......... .......... .......... 39% 35.9M 1s\n  4600K .......... .......... .......... .......... .......... 39% 14.8M 1s\n  4650K .......... .......... .......... .......... .......... 40%  111M 1s\n  4700K .......... .......... .......... .......... .......... 40% 17.8M 1s\n  4750K .......... .......... .......... .......... .......... 40% 44.6M 1s\n  4800K .......... .......... .......... .......... .......... 41% 19.4M 1s\n  4850K .......... .......... .......... .......... .......... 41% 16.3M 1s\n  4900K .......... .......... .......... .......... .......... 42%  119M 1s\n  4950K .......... .......... .......... .......... .......... 42% 17.4M 1s\n  5000K .......... .......... .......... .......... .......... 43% 22.1M 1s\n  5050K .......... .......... .......... .......... .......... 43% 18.1M 1s\n  5100K .......... .......... .......... .......... .......... 43% 54.3M 1s\n  5150K .......... .......... .......... .......... .......... 44% 19.6M 1s\n  5200K .......... .......... .......... .......... .......... 44% 23.5M 1s\n  5250K .......... .......... .......... .......... .......... 45% 29.3M 1s\n  5300K .......... .......... .......... .......... .......... 45% 18.5M 1s\n  5350K .......... .......... .......... .......... .......... 46% 88.1M 1s\n  5400K .......... .......... .......... .......... .......... 46% 16.6M 1s\n  5450K .......... .......... .......... .......... .......... 46% 32.1M 1s\n  5500K .......... .......... .......... .......... .......... 47% 22.1M 1s\n  5550K .......... .......... .......... .......... .......... 47% 22.0M 1s\n  5600K .......... .......... .......... .......... .......... 48%  104M 1s\n  5650K .......... .......... .......... .......... .......... 48% 33.3M 1s\n  5700K .......... .......... .......... .......... .......... 49% 24.7M 1s\n  5750K .......... .......... .......... .......... .......... 49% 29.3M 1s\n  5800K .......... .......... .......... .......... .......... 49% 81.8M 1s\n  5850K .......... .......... .......... .......... .......... 50% 29.0M 1s\n  5900K .......... .......... .......... .......... .......... 50% 30.4M 1s\n  5950K .......... .......... .......... .......... .......... 51% 25.0M 1s\n  6000K .......... .......... .......... .......... .......... 51% 36.2M 1s\n  6050K .......... .......... .......... .......... .......... 51%  129M 1s\n  6100K .......... .......... .......... .......... .......... 52% 34.8M 1s\n  6150K .......... .......... .......... .......... .......... 52% 13.5M 1s\n  6200K .......... .......... .......... .......... .......... 53% 17.8M 1s\n  6250K .......... .......... .......... .......... .......... 53% 46.3M 0s\n  6300K .......... .......... .......... .......... .......... 54% 89.9M 0s\n  6350K .......... .......... .......... .......... .......... 54% 20.6M 0s\n  6400K .......... .......... .......... .......... .......... 54% 16.2M 0s\n  6450K .......... .......... .......... .......... .......... 55% 16.2M 0s\n  6500K .......... .......... .......... .......... .......... 55% 22.5M 0s\n  6550K .......... .......... .......... .......... .......... 56%  103M 0s\n  6600K .......... .......... .......... .......... .......... 56% 20.4M 0s\n  6650K .......... .......... .......... .......... .......... 57% 19.7M 0s\n  6700K .......... .......... .......... .......... .......... 57% 26.0M 0s\n  6750K .......... .......... .......... .......... .......... 57% 21.3M 0s\n  6800K .......... .......... .......... .......... .......... 58% 60.6M 0s\n  6850K .......... .......... .......... .......... .......... 58% 22.4M 0s\n  6900K .......... .......... .......... .......... .......... 59% 15.5M 0s\n  6950K .......... .......... .......... .......... .......... 59% 43.9M 0s\n  7000K .......... .......... .......... .......... .......... 60%  131M 0s\n  7050K .......... .......... .......... .......... .......... 60% 17.9M 0s\n  7100K .......... .......... .......... .......... .......... 60% 26.2M 0s\n  7150K .......... .......... .......... .......... .......... 61% 22.9M 0s\n  7200K .......... .......... .......... .......... .......... 61% 34.6M 0s\n  7250K .......... .......... .......... .......... .......... 62%  134M 0s\n  7300K .......... .......... .......... .......... .......... 62% 25.1M 0s\n  7350K .......... .......... .......... .......... .......... 63% 35.8M 0s\n  7400K .......... .......... .......... .......... .......... 63% 23.3M 0s\n  7450K .......... .......... .......... .......... .......... 63% 49.7M 0s\n  7500K .......... .......... .......... .......... .......... 64% 37.2M 0s\n  7550K .......... .......... .......... .......... .......... 64% 27.4M 0s\n  7600K .......... .......... .......... .......... .......... 65% 45.3M 0s\n  7650K .......... .......... .......... .......... .......... 65% 13.2M 0s\n  7700K .......... .......... .......... .......... .......... 66% 94.4M 0s\n  7750K .......... .......... .......... .......... .......... 66% 17.5M 0s\n  7800K .......... .......... .......... .......... .......... 66% 60.5M 0s\n  7850K .......... .......... .......... .......... .......... 67% 20.9M 0s\n  7900K .......... .......... .......... .......... .......... 67% 89.9M 0s\n  7950K .......... .......... .......... .......... .......... 68% 17.2M 0s\n  8000K .......... .......... .......... .......... .......... 68% 14.8M 0s\n  8050K .......... .......... .......... .......... .......... 69% 24.6M 0s\n  8100K .......... .......... .......... .......... .......... 69% 18.9M 0s\n  8150K .......... .......... .......... .......... .......... 69%  122M 0s\n  8200K .......... .......... .......... .......... .......... 70% 24.6M 0s\n  8250K .......... .......... .......... .......... .......... 70% 19.9M 0s\n  8300K .......... .......... .......... .......... .......... 71% 23.4M 0s\n  8350K .......... .......... .......... .......... .......... 71% 19.3M 0s\n  8400K .......... .......... .......... .......... .......... 72% 13.6M 0s\n  8450K .......... .......... .......... .......... .......... 72%  118M 0s\n  8500K .......... .......... .......... .......... .......... 72% 62.7M 0s\n  8550K .......... .......... .......... .......... .......... 73% 17.6M 0s\n  8600K .......... .......... .......... .......... .......... 73% 32.6M 0s\n  8650K .......... .......... .......... .......... .......... 74%  137M 0s\n  8700K .......... .......... .......... .......... .......... 74% 19.2M 0s\n  8750K .......... .......... .......... .......... .......... 74% 29.5M 0s\n  8800K .......... .......... .......... .......... .......... 75% 29.8M 0s\n  8850K .......... .......... .......... .......... .......... 75% 34.1M 0s\n  8900K .......... .......... .......... .......... .......... 76%  127M 0s\n  8950K .......... .......... .......... .......... .......... 76% 20.5M 0s\n  9000K .......... .......... .......... .......... .......... 77% 22.1M 0s\n  9050K .......... .......... .......... .......... .......... 77% 42.8M 0s\n  9100K .......... .......... .......... .......... .......... 77%  103M 0s\n  9150K .......... .......... .......... .......... .......... 78% 44.9M 0s\n  9200K .......... .......... .......... .......... .......... 78% 14.3M 0s\n  9250K .......... .......... .......... .......... .......... 79% 15.2M 0s\n  9300K .......... .......... .......... .......... .......... 79% 49.0M 0s\n  9350K .......... .......... .......... .......... .......... 80% 16.6M 0s\n  9400K .......... .......... .......... .......... .......... 80%  121M 0s\n  9450K .......... .......... .......... .......... .......... 80% 25.4M 0s\n  9500K .......... .......... .......... .......... .......... 81% 14.4M 0s\n  9550K .......... .......... .......... .......... .......... 81% 24.7M 0s\n  9600K .......... .......... .......... .......... .......... 82% 61.0M 0s\n  9650K .......... .......... .......... .......... .......... 82% 21.2M 0s\n  9700K .......... .......... .......... .......... .......... 83% 24.2M 0s\n  9750K .......... .......... .......... .......... .......... 83% 22.5M 0s\n  9800K .......... .......... .......... .......... .......... 83% 35.8M 0s\n  9850K .......... .......... .......... .......... .......... 84% 36.0M 0s\n  9900K .......... .......... .......... .......... .......... 84% 18.7M 0s\n  9950K .......... .......... .......... .......... .......... 85% 12.0M 0s\n 10000K .......... .......... .......... .......... .......... 85% 28.6M 0s\n 10050K .......... .......... .......... .......... .......... 86% 30.1M 0s\n 10100K .......... .......... .......... .......... .......... 86% 34.5M 0s\n 10150K .......... .......... .......... .......... .......... 86% 23.9M 0s\n 10200K .......... .......... .......... .......... .......... 87% 35.9M 0s\n 10250K .......... .......... .......... .......... .......... 87% 25.7M 0s\n 10300K .......... .......... .......... .......... .......... 88% 25.6M 0s\n 10350K .......... .......... .......... .......... .......... 88% 31.3M 0s\n 10400K .......... .......... .......... .......... .......... 89% 23.2M 0s\n 10450K .......... .......... .......... .......... .......... 89% 38.0M 0s\n 10500K .......... .......... .......... .......... .......... 89% 26.8M 0s\n 10550K .......... .......... .......... .......... .......... 90% 32.2M 0s\n 10600K .......... .......... .......... .......... .......... 90% 23.6M 0s\n 10650K .......... .......... .......... .......... .......... 91% 35.0M 0s\n 10700K .......... .......... .......... .......... .......... 91% 27.5M 0s\n 10750K .......... .......... .......... .......... .......... 92% 23.8M 0s\n 10800K .......... .......... .......... .......... .......... 92% 24.1M 0s\n 10850K .......... .......... .......... .......... .......... 92% 33.4M 0s\n 10900K .......... .......... .......... .......... .......... 93% 24.3M 0s\n 10950K .......... .......... .......... .......... .......... 93% 40.6M 0s\n 11000K .......... .......... .......... .......... .......... 94% 22.6M 0s\n 11050K .......... .......... .......... .......... .......... 94% 34.4M 0s\n 11100K .......... .......... .......... .......... .......... 95% 24.9M 0s\n 11150K .......... .......... .......... .......... .......... 95% 22.4M 0s\n 11200K .......... .......... .......... .......... .......... 95% 46.5M 0s\n 11250K .......... .......... .......... .......... .......... 96% 21.1M 0s\n 11300K .......... .......... .......... .......... .......... 96% 37.1M 0s\n 11350K .......... .......... .......... .......... .......... 97% 25.7M 0s\n 11400K .......... .......... .......... .......... .......... 97% 33.0M 0s\n 11450K .......... .......... .......... .......... .......... 98% 23.4M 0s\n 11500K .......... .......... .......... .......... .......... 98% 22.0M 0s\n 11550K .......... .......... .......... .......... .......... 98% 21.3M 0s\n 11600K .......... .......... .......... .......... .......... 99% 30.3M 0s\n 11650K .......... .......... .......... .......... .......... 99% 33.5M 0s\n 11700K .......... .......... .......... ...                  100% 28.8M=0.8s\n\n2022-06-04 11:55:58 (14.7 MB/s) - /tmp/Kaggle_NoFreq_256Units_onlyInitialCell.csv saved [12015296/12015296]\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["--2022-06-04 11:55:56--  https://www.dropbox.com/s/zzr0xjf5yi78fqz/model_OnlyInitial_NoFreq_256Units.h5?dl=1\nResolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6016:18::a27d:112\nConnecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: /s/dl/zzr0xjf5yi78fqz/model_OnlyInitial_NoFreq_256Units.h5 [following]\n--2022-06-04 11:55:56--  https://www.dropbox.com/s/dl/zzr0xjf5yi78fqz/model_OnlyInitial_NoFreq_256Units.h5\nReusing existing connection to www.dropbox.com:443.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://uc9cc8b0ed55d1f314a73bdb543f.dl.dropboxusercontent.com/cd/0/get/Bmgue4yJjXvNFb7o8_2EEFxYeslypHZBw9GzHjPJbJMa7GKYZG7MMrg-Uwb8WCQeKGygdyZYcR56GetHxYeS-0xK1uZcG50FmU0dI1hI0PgGBlSQi0s8VafjxTvM6bTZ_JE0cYr3lGpQN3O8QPyTCyxjajvUCZqkReooHZaI0xmO4BMmdM1U2j9x_qHGnAkjJ_E/file?dl=1# [following]\n--2022-06-04 11:55:56--  https://uc9cc8b0ed55d1f314a73bdb543f.dl.dropboxusercontent.com/cd/0/get/Bmgue4yJjXvNFb7o8_2EEFxYeslypHZBw9GzHjPJbJMa7GKYZG7MMrg-Uwb8WCQeKGygdyZYcR56GetHxYeS-0xK1uZcG50FmU0dI1hI0PgGBlSQi0s8VafjxTvM6bTZ_JE0cYr3lGpQN3O8QPyTCyxjajvUCZqkReooHZaI0xmO4BMmdM1U2j9x_qHGnAkjJ_E/file?dl=1\nResolving uc9cc8b0ed55d1f314a73bdb543f.dl.dropboxusercontent.com (uc9cc8b0ed55d1f314a73bdb543f.dl.dropboxusercontent.com)... 162.125.8.15, 2620:100:6016:15::a27d:10f\nConnecting to uc9cc8b0ed55d1f314a73bdb543f.dl.dropboxusercontent.com (uc9cc8b0ed55d1f314a73bdb543f.dl.dropboxusercontent.com)|162.125.8.15|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 12015296 (11M) [application/binary]\nSaving to: /tmp/Kaggle_NoFreq_256Units_onlyInitialCell.csv\n\n     0K .......... .......... .......... .......... ..........  0%  490K 24s\n    50K .......... .......... .......... .......... ..........  0%  984K 18s\n   100K .......... .......... .......... .......... ..........  1% 1.19M 15s\n   150K .......... .......... .......... .......... ..........  1% 1.49M 13s\n   200K .......... .......... .......... .......... ..........  2% 2.88M 11s\n   250K .......... .......... .......... .......... ..........  2% 2.62M 10s\n   300K .......... .......... .......... .......... ..........  2% 3.01M 9s\n   350K .......... .......... .......... .......... ..........  3% 3.11M 8s\n   400K .......... .......... .......... .......... ..........  3% 5.18M 8s\n   450K .......... .......... .......... .......... ..........  4% 5.93M 7s\n   500K .......... .......... .......... .......... ..........  4% 5.89M 7s\n   550K .......... .......... .......... .......... ..........  5% 7.95M 6s\n   600K .......... .......... .......... .......... ..........  5% 7.76M 6s\n   650K .......... .......... .......... .......... ..........  5% 7.94M 5s\n   700K .......... .......... .......... .......... ..........  6% 9.52M 5s\n   750K .......... .......... .......... .......... ..........  6% 6.24M 5s\n   800K .......... .......... .......... .......... ..........  7% 8.96M 5s\n   850K .......... .......... .......... .......... ..........  7% 10.5M 4s\n   900K .......... .......... .......... .......... ..........  8% 10.6M 4s\n   950K .......... .......... .......... .......... ..........  8% 11.2M 4s\n  1000K .......... .......... .......... .......... ..........  8% 12.0M 4s\n  1050K .......... .......... .......... .......... ..........  9% 12.0M 4s\n  1100K .......... .......... .......... .......... ..........  9% 13.5M 4s\n  1150K .......... .......... .......... .......... .......... 10% 12.8M 3s\n  1200K .......... .......... .......... .......... .......... 10% 13.8M 3s\n  1250K .......... .......... .......... .......... .......... 11% 21.5M 3s\n  1300K .......... .......... .......... .......... .......... 11% 16.7M 3s\n  1350K .......... .......... .......... .......... .......... 11% 17.5M 3s\n  1400K .......... .......... .......... .......... .......... 12% 17.6M 3s\n  1450K .......... .......... .......... .......... .......... 12% 18.9M 3s\n  1500K .......... .......... .......... .......... .......... 13% 20.5M 3s\n  1550K .......... .......... .......... .......... .......... 13% 15.4M 3s\n  1600K .......... .......... .......... .......... .......... 14% 15.6M 3s\n  1650K .......... .......... .......... .......... .......... 14% 25.6M 2s\n  1700K .......... .......... .......... .......... .......... 14% 22.5M 2s\n  1750K .......... .......... .......... .......... .......... 15% 23.1M 2s\n  1800K .......... .......... .......... .......... .......... 15% 15.7M 2s\n  1850K .......... .......... .......... .......... .......... 16% 26.9M 2s\n  1900K .......... .......... .......... .......... .......... 16% 25.9M 2s\n  1950K .......... .......... .......... .......... .......... 17% 21.5M 2s\n  2000K .......... .......... .......... .......... .......... 17% 28.8M 2s\n  2050K .......... .......... .......... .......... .......... 17% 21.6M 2s\n  2100K .......... .......... .......... .......... .......... 18% 26.2M 2s\n  2150K .......... .......... .......... .......... .......... 18% 28.4M 2s\n  2200K .......... .......... .......... .......... .......... 19% 32.6M 2s\n  2250K .......... .......... .......... .......... .......... 19% 19.8M 2s\n  2300K .......... .......... .......... .......... .......... 20% 45.1M 2s\n  2350K .......... .......... .......... .......... .......... 20% 25.4M 2s\n  2400K .......... .......... .......... .......... .......... 20% 22.6M 2s\n  2450K .......... .......... .......... .......... .......... 21% 51.5M 2s\n  2500K .......... .......... .......... .......... .......... 21% 23.8M 2s\n  2550K .......... .......... .......... .......... .......... 22% 17.6M 2s\n  2600K .......... .......... .......... .......... .......... 22%  122M 2s\n  2650K .......... .......... .......... .......... .......... 23% 56.4M 2s\n  2700K .......... .......... .......... .......... .......... 23% 26.5M 1s\n  2750K .......... .......... .......... .......... .......... 23% 27.9M 1s\n  2800K .......... .......... .......... .......... .......... 24% 45.4M 1s\n  2850K .......... .......... .......... .......... .......... 24% 28.8M 1s\n  2900K .......... .......... .......... .......... .......... 25% 45.6M 1s\n  2950K .......... .......... .......... .......... .......... 25% 31.8M 1s\n  3000K .......... .......... .......... .......... .......... 25% 34.1M 1s\n  3050K .......... .......... .......... .......... .......... 26% 35.2M 1s\n  3100K .......... .......... .......... .......... .......... 26% 40.3M 1s\n  3150K .......... .......... .......... .......... .......... 27% 13.4M 1s\n  3200K .......... .......... .......... .......... .......... 27% 22.5M 1s\n  3250K .......... .......... .......... .......... .......... 28% 40.5M 1s\n  3300K .......... .......... .......... .......... .......... 28% 16.0M 1s\n  3350K .......... .......... .......... .......... .......... 28% 24.0M 1s\n  3400K .......... .......... .......... .......... .......... 29% 20.3M 1s\n  3450K .......... .......... .......... .......... .......... 29% 53.1M 1s\n  3500K .......... .......... .......... .......... .......... 30% 16.8M 1s\n  3550K .......... .......... .......... .......... .......... 30% 27.2M 1s\n  3600K .......... .......... .......... .......... .......... 31% 23.5M 1s\n  3650K .......... .......... .......... .......... .......... 31% 20.3M 1s\n  3700K .......... .......... .......... .......... .......... 31% 45.1M 1s\n  3750K .......... .......... .......... .......... .......... 32% 22.0M 1s\n  3800K .......... .......... .......... .......... .......... 32% 25.7M 1s\n  3850K .......... .......... .......... .......... .......... 33% 15.8M 1s\n  3900K .......... .......... .......... .......... .......... 33% 41.0M 1s\n  3950K .......... .......... .......... .......... .......... 34% 19.0M 1s\n  4000K .......... .......... .......... .......... .......... 34% 95.3M 1s\n  4050K .......... .......... .......... .......... .......... 34% 24.7M 1s\n  4100K .......... .......... .......... .......... .......... 35% 22.4M 1s\n  4150K .......... .......... .......... .......... .......... 35% 32.8M 1s\n  4200K .......... .......... .......... .......... .......... 36%  136M 1s\n  4250K .......... .......... .......... .......... .......... 36% 27.7M 1s\n  4300K .......... .......... .......... .......... .......... 37% 27.0M 1s\n  4350K .......... .......... .......... .......... .......... 37% 29.3M 1s\n  4400K .......... .......... .......... .......... .......... 37% 31.6M 1s\n  4450K .......... .......... .......... .......... .......... 38%  131M 1s\n  4500K .......... .......... .......... .......... .......... 38% 26.9M 1s\n  4550K .......... .......... .......... .......... .......... 39% 35.9M 1s\n  4600K .......... .......... .......... .......... .......... 39% 14.8M 1s\n  4650K .......... .......... .......... .......... .......... 40%  111M 1s\n  4700K .......... .......... .......... .......... .......... 40% 17.8M 1s\n  4750K .......... .......... .......... .......... .......... 40% 44.6M 1s\n  4800K .......... .......... .......... .......... .......... 41% 19.4M 1s\n  4850K .......... .......... .......... .......... .......... 41% 16.3M 1s\n  4900K .......... .......... .......... .......... .......... 42%  119M 1s\n  4950K .......... .......... .......... .......... .......... 42% 17.4M 1s\n  5000K .......... .......... .......... .......... .......... 43% 22.1M 1s\n  5050K .......... .......... .......... .......... .......... 43% 18.1M 1s\n  5100K .......... .......... .......... .......... .......... 43% 54.3M 1s\n  5150K .......... .......... .......... .......... .......... 44% 19.6M 1s\n  5200K .......... .......... .......... .......... .......... 44% 23.5M 1s\n  5250K .......... .......... .......... .......... .......... 45% 29.3M 1s\n  5300K .......... .......... .......... .......... .......... 45% 18.5M 1s\n  5350K .......... .......... .......... .......... .......... 46% 88.1M 1s\n  5400K .......... .......... .......... .......... .......... 46% 16.6M 1s\n  5450K .......... .......... .......... .......... .......... 46% 32.1M 1s\n  5500K .......... .......... .......... .......... .......... 47% 22.1M 1s\n  5550K .......... .......... .......... .......... .......... 47% 22.0M 1s\n  5600K .......... .......... .......... .......... .......... 48%  104M 1s\n  5650K .......... .......... .......... .......... .......... 48% 33.3M 1s\n  5700K .......... .......... .......... .......... .......... 49% 24.7M 1s\n  5750K .......... .......... .......... .......... .......... 49% 29.3M 1s\n  5800K .......... .......... .......... .......... .......... 49% 81.8M 1s\n  5850K .......... .......... .......... .......... .......... 50% 29.0M 1s\n  5900K .......... .......... .......... .......... .......... 50% 30.4M 1s\n  5950K .......... .......... .......... .......... .......... 51% 25.0M 1s\n  6000K .......... .......... .......... .......... .......... 51% 36.2M 1s\n  6050K .......... .......... .......... .......... .......... 51%  129M 1s\n  6100K .......... .......... .......... .......... .......... 52% 34.8M 1s\n  6150K .......... .......... .......... .......... .......... 52% 13.5M 1s\n  6200K .......... .......... .......... .......... .......... 53% 17.8M 1s\n  6250K .......... .......... .......... .......... .......... 53% 46.3M 0s\n  6300K .......... .......... .......... .......... .......... 54% 89.9M 0s\n  6350K .......... .......... .......... .......... .......... 54% 20.6M 0s\n  6400K .......... .......... .......... .......... .......... 54% 16.2M 0s\n  6450K .......... .......... .......... .......... .......... 55% 16.2M 0s\n  6500K .......... .......... .......... .......... .......... 55% 22.5M 0s\n  6550K .......... .......... .......... .......... .......... 56%  103M 0s\n  6600K .......... .......... .......... .......... .......... 56% 20.4M 0s\n  6650K .......... .......... .......... .......... .......... 57% 19.7M 0s\n  6700K .......... .......... .......... .......... .......... 57% 26.0M 0s\n  6750K .......... .......... .......... .......... .......... 57% 21.3M 0s\n  6800K .......... .......... .......... .......... .......... 58% 60.6M 0s\n  6850K .......... .......... .......... .......... .......... 58% 22.4M 0s\n  6900K .......... .......... .......... .......... .......... 59% 15.5M 0s\n  6950K .......... .......... .......... .......... .......... 59% 43.9M 0s\n  7000K .......... .......... .......... .......... .......... 60%  131M 0s\n  7050K .......... .......... .......... .......... .......... 60% 17.9M 0s\n  7100K .......... .......... .......... .......... .......... 60% 26.2M 0s\n  7150K .......... .......... .......... .......... .......... 61% 22.9M 0s\n  7200K .......... .......... .......... .......... .......... 61% 34.6M 0s\n  7250K .......... .......... .......... .......... .......... 62%  134M 0s\n  7300K .......... .......... .......... .......... .......... 62% 25.1M 0s\n  7350K .......... .......... .......... .......... .......... 63% 35.8M 0s\n  7400K .......... .......... .......... .......... .......... 63% 23.3M 0s\n  7450K .......... .......... .......... .......... .......... 63% 49.7M 0s\n  7500K .......... .......... .......... .......... .......... 64% 37.2M 0s\n  7550K .......... .......... .......... .......... .......... 64% 27.4M 0s\n  7600K .......... .......... .......... .......... .......... 65% 45.3M 0s\n  7650K .......... .......... .......... .......... .......... 65% 13.2M 0s\n  7700K .......... .......... .......... .......... .......... 66% 94.4M 0s\n  7750K .......... .......... .......... .......... .......... 66% 17.5M 0s\n  7800K .......... .......... .......... .......... .......... 66% 60.5M 0s\n  7850K .......... .......... .......... .......... .......... 67% 20.9M 0s\n  7900K .......... .......... .......... .......... .......... 67% 89.9M 0s\n  7950K .......... .......... .......... .......... .......... 68% 17.2M 0s\n  8000K .......... .......... .......... .......... .......... 68% 14.8M 0s\n  8050K .......... .......... .......... .......... .......... 69% 24.6M 0s\n  8100K .......... .......... .......... .......... .......... 69% 18.9M 0s\n  8150K .......... .......... .......... .......... .......... 69%  122M 0s\n  8200K .......... .......... .......... .......... .......... 70% 24.6M 0s\n  8250K .......... .......... .......... .......... .......... 70% 19.9M 0s\n  8300K .......... .......... .......... .......... .......... 71% 23.4M 0s\n  8350K .......... .......... .......... .......... .......... 71% 19.3M 0s\n  8400K .......... .......... .......... .......... .......... 72% 13.6M 0s\n  8450K .......... .......... .......... .......... .......... 72%  118M 0s\n  8500K .......... .......... .......... .......... .......... 72% 62.7M 0s\n  8550K .......... .......... .......... .......... .......... 73% 17.6M 0s\n  8600K .......... .......... .......... .......... .......... 73% 32.6M 0s\n  8650K .......... .......... .......... .......... .......... 74%  137M 0s\n  8700K .......... .......... .......... .......... .......... 74% 19.2M 0s\n  8750K .......... .......... .......... .......... .......... 74% 29.5M 0s\n  8800K .......... .......... .......... .......... .......... 75% 29.8M 0s\n  8850K .......... .......... .......... .......... .......... 75% 34.1M 0s\n  8900K .......... .......... .......... .......... .......... 76%  127M 0s\n  8950K .......... .......... .......... .......... .......... 76% 20.5M 0s\n  9000K .......... .......... .......... .......... .......... 77% 22.1M 0s\n  9050K .......... .......... .......... .......... .......... 77% 42.8M 0s\n  9100K .......... .......... .......... .......... .......... 77%  103M 0s\n  9150K .......... .......... .......... .......... .......... 78% 44.9M 0s\n  9200K .......... .......... .......... .......... .......... 78% 14.3M 0s\n  9250K .......... .......... .......... .......... .......... 79% 15.2M 0s\n  9300K .......... .......... .......... .......... .......... 79% 49.0M 0s\n  9350K .......... .......... .......... .......... .......... 80% 16.6M 0s\n  9400K .......... .......... .......... .......... .......... 80%  121M 0s\n  9450K .......... .......... .......... .......... .......... 80% 25.4M 0s\n  9500K .......... .......... .......... .......... .......... 81% 14.4M 0s\n  9550K .......... .......... .......... .......... .......... 81% 24.7M 0s\n  9600K .......... .......... .......... .......... .......... 82% 61.0M 0s\n  9650K .......... .......... .......... .......... .......... 82% 21.2M 0s\n  9700K .......... .......... .......... .......... .......... 83% 24.2M 0s\n  9750K .......... .......... .......... .......... .......... 83% 22.5M 0s\n  9800K .......... .......... .......... .......... .......... 83% 35.8M 0s\n  9850K .......... .......... .......... .......... .......... 84% 36.0M 0s\n  9900K .......... .......... .......... .......... .......... 84% 18.7M 0s\n  9950K .......... .......... .......... .......... .......... 85% 12.0M 0s\n 10000K .......... .......... .......... .......... .......... 85% 28.6M 0s\n 10050K .......... .......... .......... .......... .......... 86% 30.1M 0s\n 10100K .......... .......... .......... .......... .......... 86% 34.5M 0s\n 10150K .......... .......... .......... .......... .......... 86% 23.9M 0s\n 10200K .......... .......... .......... .......... .......... 87% 35.9M 0s\n 10250K .......... .......... .......... .......... .......... 87% 25.7M 0s\n 10300K .......... .......... .......... .......... .......... 88% 25.6M 0s\n 10350K .......... .......... .......... .......... .......... 88% 31.3M 0s\n 10400K .......... .......... .......... .......... .......... 89% 23.2M 0s\n 10450K .......... .......... .......... .......... .......... 89% 38.0M 0s\n 10500K .......... .......... .......... .......... .......... 89% 26.8M 0s\n 10550K .......... .......... .......... .......... .......... 90% 32.2M 0s\n 10600K .......... .......... .......... .......... .......... 90% 23.6M 0s\n 10650K .......... .......... .......... .......... .......... 91% 35.0M 0s\n 10700K .......... .......... .......... .......... .......... 91% 27.5M 0s\n 10750K .......... .......... .......... .......... .......... 92% 23.8M 0s\n 10800K .......... .......... .......... .......... .......... 92% 24.1M 0s\n 10850K .......... .......... .......... .......... .......... 92% 33.4M 0s\n 10900K .......... .......... .......... .......... .......... 93% 24.3M 0s\n 10950K .......... .......... .......... .......... .......... 93% 40.6M 0s\n 11000K .......... .......... .......... .......... .......... 94% 22.6M 0s\n 11050K .......... .......... .......... .......... .......... 94% 34.4M 0s\n 11100K .......... .......... .......... .......... .......... 95% 24.9M 0s\n 11150K .......... .......... .......... .......... .......... 95% 22.4M 0s\n 11200K .......... .......... .......... .......... .......... 95% 46.5M 0s\n 11250K .......... .......... .......... .......... .......... 96% 21.1M 0s\n 11300K .......... .......... .......... .......... .......... 96% 37.1M 0s\n 11350K .......... .......... .......... .......... .......... 97% 25.7M 0s\n 11400K .......... .......... .......... .......... .......... 97% 33.0M 0s\n 11450K .......... .......... .......... .......... .......... 98% 23.4M 0s\n 11500K .......... .......... .......... .......... .......... 98% 22.0M 0s\n 11550K .......... .......... .......... .......... .......... 98% 21.3M 0s\n 11600K .......... .......... .......... .......... .......... 99% 30.3M 0s\n 11650K .......... .......... .......... .......... .......... 99% 33.5M 0s\n 11700K .......... .......... .......... ...                  100% 28.8M=0.8s\n\n2022-06-04 11:55:58 (14.7 MB/s) - /tmp/Kaggle_NoFreq_256Units_onlyInitialCell.csv saved [12015296/12015296]\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["model2_Initialmoves = keras.models.load_model(\"/tmp/Kaggle_NoFreq_256Units_onlyInitialCell.csv\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"11e09aef-55de-4b25-94e1-95bc374ddc66"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Plotting of the loss and accuracy values during the train"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"87ad900f-8102-4b6b-8734-9ab6e96059bb"}}},{"cell_type":"code","source":["plt.plot(history.history['loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train'], loc='upper right')\nplt.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"64c8d9e8-85cf-4b31-afe2-82ea8f817744"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["<img src=\"https://www.dropbox.com/s/cb4n7d459nith21/Model4_loss.png?dl=1\">"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3c1c9434-91f7-4946-9123-e57f58f9322d"}}},{"cell_type":"code","source":["plt.plot(history.history['accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train'], loc='upper left')\nplt.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e49cdc82-19e9-4c5f-b108-7ad01a322046"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["<img src=\"https://www.dropbox.com/s/nosg96kwndnstgi/Model4_accuracy.png?dl=1\">"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"323bba73-8aab-4273-9a91-75b71d78c908"}}},{"cell_type":"markdown","source":["##### Third Model trained with the same Neural Netwrok of the previus model but for 100 epochs\nGiven the results obtained with the previous model, since it was better than the first one, I trained a third model with the same network as the second one but making it train for 100 epochs."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"71421164-9fc7-4691-bb38-f014907d5728"}}},{"cell_type":"code","source":["model = Sequential()\nmodel.add(Embedding(vocab_size, n_steps, input_length=seq_len))\nmodel.add(LSTM(256, return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(256))\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dense(vocab_size, activation='softmax'))\nprint(model.summary())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f6637b36-61d3-403b-88cf-394293d804ca"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Model: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n embedding (Embedding)       (None, 5, 5)              2615      \n                                                                 \n lstm (LSTM)                 (None, 5, 256)            268288    \n                                                                 \n dropout (Dropout)           (None, 5, 256)            0         \n                                                                 \n lstm_1 (LSTM)               (None, 256)               525312    \n                                                                 \n dense (Dense)               (None, 256)               65792     \n                                                                 \n dense_1 (Dense)             (None, 523)               134411    \n                                                                 \n=================================================================\nTotal params: 996,418\nTrainable params: 996,418\nNon-trainable params: 0\n_________________________________________________________________\nNone\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Model: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n embedding (Embedding)       (None, 5, 5)              2615      \n                                                                 \n lstm (LSTM)                 (None, 5, 256)            268288    \n                                                                 \n dropout (Dropout)           (None, 5, 256)            0         \n                                                                 \n lstm_1 (LSTM)               (None, 256)               525312    \n                                                                 \n dense (Dense)               (None, 256)               65792     \n                                                                 \n dense_1 (Dense)             (None, 523)               134411    \n                                                                 \n=================================================================\nTotal params: 996,418\nTrainable params: 996,418\nNon-trainable params: 0\n_________________________________________________________________\nNone\n"]}}],"execution_count":0},{"cell_type":"code","source":["# compile model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n# fit model\nhistory=model.fit(train_x, train_y, batch_size=32, epochs=100, callbacks=[callback])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8889ce65-7d6c-4d4d-88f4-67a87ac7048b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Epoch 1/100\n\r    1/26250 [..............................] - ETA: 26:55:11 - loss: 6.2595 - accuracy: 0.0000e+00\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    2/26250 [..............................] - ETA: 23:25 - loss: 6.2571 - accuracy: 0.2344       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    3/26250 [..............................] - ETA: 24:34 - loss: 6.2552 - accuracy: 0.2396\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    4/26250 [..............................] - ETA: 24:11 - loss: 6.2532 - accuracy: 0.2500\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    5/26250 [..............................] - ETA: 23:38 - loss: 6.2497 - accuracy: 0.2688\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    7/26250 [..............................] - ETA: 22:42 - loss: 6.2408 - accuracy: 0.2857\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    9/26250 [..............................] - ETA: 22:14 - loss: 6.2262 - accuracy: 0.2847\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   10/26250 [..............................] - ETA: 22:24 - loss: 6.2169 - accuracy: 0.2812\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   12/26250 [..............................] - ETA: 22:08 - loss: 6.1806 - accuracy: 0.2839\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   13/26250 [..............................] - ETA: 22:26 - loss: 6.1628 - accuracy: 0.2764\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   14/26250 [..............................] - ETA: 22:38 - loss: 6.1481 - accuracy: 0.2679\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   15/26250 [..............................] - ETA: 22:39 - loss: 6.1075 - accuracy: 0.2688\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   17/26250 [..............................] - ETA: 22:28 - loss: 6.0130 - accuracy: 0.2647\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   19/26250 [..............................] - ETA: 22:21 - loss: 5.8700 - accuracy: 0.2681\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   20/26250 [..............................] - ETA: 22:19 - loss: 5.8256 - accuracy: 0.2703\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   22/26250 [..............................] - ETA: 22:13 - loss: 5.7439 - accuracy: 0.2685","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Epoch 1/100\n\r    1/26250 [..............................] - ETA: 26:55:11 - loss: 6.2595 - accuracy: 0.0000e+00\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    2/26250 [..............................] - ETA: 23:25 - loss: 6.2571 - accuracy: 0.2344       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    3/26250 [..............................] - ETA: 24:34 - loss: 6.2552 - accuracy: 0.2396\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    4/26250 [..............................] - ETA: 24:11 - loss: 6.2532 - accuracy: 0.2500\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    5/26250 [..............................] - ETA: 23:38 - loss: 6.2497 - accuracy: 0.2688\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    7/26250 [..............................] - ETA: 22:42 - loss: 6.2408 - accuracy: 0.2857\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    9/26250 [..............................] - ETA: 22:14 - loss: 6.2262 - accuracy: 0.2847\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   10/26250 [..............................] - ETA: 22:24 - loss: 6.2169 - accuracy: 0.2812\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   12/26250 [..............................] - ETA: 22:08 - loss: 6.1806 - accuracy: 0.2839\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   13/26250 [..............................] - ETA: 22:26 - loss: 6.1628 - accuracy: 0.2764\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   14/26250 [..............................] - ETA: 22:38 - loss: 6.1481 - accuracy: 0.2679\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   15/26250 [..............................] - ETA: 22:39 - loss: 6.1075 - accuracy: 0.2688\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   17/26250 [..............................] - ETA: 22:28 - loss: 6.0130 - accuracy: 0.2647\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   19/26250 [..............................] - ETA: 22:21 - loss: 5.8700 - accuracy: 0.2681\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   20/26250 [..............................] - ETA: 22:19 - loss: 5.8256 - accuracy: 0.2703\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   22/26250 [..............................] - ETA: 22:13 - loss: 5.7439 - accuracy: 0.2685"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Cancelled","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Loading the third trained model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ac992e43-33c3-47e1-abcf-7f750f760196"}}},{"cell_type":"code","source":["%sh wget https://www.dropbox.com/s/r5fphpb09dreepq/model_OnlyInitial_NoFreq_256Units_100epoch.h5?dl=1 -O /tmp/Kaggle_NoFreq_256Units_100epoch_onlyInitialCell.csv"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dd3f0f0f-35ac-44a4-ac3f-9806569072bc"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"--2022-06-04 11:56:11--  https://www.dropbox.com/s/r5fphpb09dreepq/model_OnlyInitial_NoFreq_256Units_100epoch.h5?dl=1\nResolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6016:18::a27d:112\nConnecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: /s/dl/r5fphpb09dreepq/model_OnlyInitial_NoFreq_256Units_100epoch.h5 [following]\n--2022-06-04 11:56:12--  https://www.dropbox.com/s/dl/r5fphpb09dreepq/model_OnlyInitial_NoFreq_256Units_100epoch.h5\nReusing existing connection to www.dropbox.com:443.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://ucac65acb331719a1be6248a4c7e.dl.dropboxusercontent.com/cd/0/get/BmjHtuR3VYmzUiYvNtliKkVPXimCpNkSr3-uu_LrpuJDxKpmjd5KK0HaMCciPQ8NZ7iAokAbpnCtRKAMMvHDWJ2ivoW4nh97Prx8xmS9Jl948MyevUCdSG2TopB5kmfICwzusgmJTicnjFgdVQADIVodjEkx3VZEMzTcATJrToC54ewk_B2qOnBRflJ7x71CyjY/file?dl=1# [following]\n--2022-06-04 11:56:12--  https://ucac65acb331719a1be6248a4c7e.dl.dropboxusercontent.com/cd/0/get/BmjHtuR3VYmzUiYvNtliKkVPXimCpNkSr3-uu_LrpuJDxKpmjd5KK0HaMCciPQ8NZ7iAokAbpnCtRKAMMvHDWJ2ivoW4nh97Prx8xmS9Jl948MyevUCdSG2TopB5kmfICwzusgmJTicnjFgdVQADIVodjEkx3VZEMzTcATJrToC54ewk_B2qOnBRflJ7x71CyjY/file?dl=1\nResolving ucac65acb331719a1be6248a4c7e.dl.dropboxusercontent.com (ucac65acb331719a1be6248a4c7e.dl.dropboxusercontent.com)... 162.125.8.15, 2620:100:6016:15::a27d:10f\nConnecting to ucac65acb331719a1be6248a4c7e.dl.dropboxusercontent.com (ucac65acb331719a1be6248a4c7e.dl.dropboxusercontent.com)|162.125.8.15|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 12015296 (11M) [application/binary]\nSaving to: /tmp/Kaggle_NoFreq_256Units_100epoch_onlyInitialCell.csv\n\n     0K .......... .......... .......... .......... ..........  0%  490K 24s\n    50K .......... .......... .......... .......... ..........  0%  988K 18s\n   100K .......... .......... .......... .......... ..........  1% 1.18M 15s\n   150K .......... .......... .......... .......... ..........  1% 1.49M 13s\n   200K .......... .......... .......... .......... ..........  2% 2.90M 11s\n   250K .......... .......... .......... .......... ..........  2% 2.65M 10s\n   300K .......... .......... .......... .......... ..........  2% 3.02M 9s\n   350K .......... .......... .......... .......... ..........  3% 3.08M 8s\n   400K .......... .......... .......... .......... ..........  3% 5.18M 8s\n   450K .......... .......... .......... .......... ..........  4% 5.96M 7s\n   500K .......... .......... .......... .......... ..........  4% 5.80M 7s\n   550K .......... .......... .......... .......... ..........  5% 8.08M 6s\n   600K .......... .......... .......... .......... ..........  5% 7.81M 6s\n   650K .......... .......... .......... .......... ..........  5% 7.98M 5s\n   700K .......... .......... .......... .......... ..........  6% 9.59M 5s\n   750K .......... .......... .......... .......... ..........  6% 6.22M 5s\n   800K .......... .......... .......... .......... ..........  7% 8.94M 5s\n   850K .......... .......... .......... .......... ..........  7% 10.6M 4s\n   900K .......... .......... .......... .......... ..........  8% 10.5M 4s\n   950K .......... .......... .......... .......... ..........  8% 11.4M 4s\n  1000K .......... .......... .......... .......... ..........  8% 11.9M 4s\n  1050K .......... .......... .......... .......... ..........  9% 12.1M 4s\n  1100K .......... .......... .......... .......... ..........  9% 13.4M 4s\n  1150K .......... .......... .......... .......... .......... 10% 12.8M 3s\n  1200K .......... .......... .......... .......... .......... 10% 13.8M 3s\n  1250K .......... .......... .......... .......... .......... 11% 21.2M 3s\n  1300K .......... .......... .......... .......... .......... 11% 17.2M 3s\n  1350K .......... .......... .......... .......... .......... 11% 17.6M 3s\n  1400K .......... .......... .......... .......... .......... 12% 16.6M 3s\n  1450K .......... .......... .......... .......... .......... 12% 18.3M 3s\n  1500K .......... .......... .......... .......... .......... 13% 12.7M 3s\n  1550K .......... .......... .......... .......... .......... 13% 26.0M 3s\n  1600K .......... .......... .......... .......... .......... 14% 17.4M 3s\n  1650K .......... .......... .......... .......... .......... 14% 20.8M 2s\n  1700K .......... .......... .......... .......... .......... 14% 30.7M 2s\n  1750K .......... .......... .......... .......... .......... 15% 22.7M 2s\n  1800K .......... .......... .......... .......... .......... 15% 18.2M 2s\n  1850K .......... .......... .......... .......... .......... 16% 24.3M 2s\n  1900K .......... .......... .......... .......... .......... 16% 28.1M 2s\n  1950K .......... .......... .......... .......... .......... 17% 18.2M 2s\n  2000K .......... .......... .......... .......... .......... 17% 29.0M 2s\n  2050K .......... .......... .......... .......... .......... 17% 21.2M 2s\n  2100K .......... .......... .......... .......... .......... 18% 22.4M 2s\n  2150K .......... .......... .......... .......... .......... 18% 31.3M 2s\n  2200K .......... .......... .......... .......... .......... 19% 36.0M 2s\n  2250K .......... .......... .......... .......... .......... 19% 20.8M 2s\n  2300K .......... .......... .......... .......... .......... 20% 47.7M 2s\n  2350K .......... .......... .......... .......... .......... 20% 22.4M 2s\n  2400K .......... .......... .......... .......... .......... 20% 25.5M 2s\n  2450K .......... .......... .......... .......... .......... 21% 34.9M 2s\n  2500K .......... .......... .......... .......... .......... 21% 26.1M 2s\n  2550K .......... .......... .......... .......... .......... 22% 36.1M 2s\n  2600K .......... .......... .......... .......... .......... 22% 29.5M 2s\n  2650K .......... .......... .......... .......... .......... 23% 32.0M 2s\n  2700K .......... .......... .......... .......... .......... 23% 32.6M 1s\n  2750K .......... .......... .......... .......... .......... 23% 29.0M 1s\n  2800K .......... .......... .......... .......... .......... 24% 44.0M 1s\n  2850K .......... .......... .......... .......... .......... 24% 29.4M 1s\n  2900K .......... .......... .......... .......... .......... 25% 46.7M 1s\n  2950K .......... .......... .......... .......... .......... 25% 30.8M 1s\n  3000K .......... .......... .......... .......... .......... 25% 39.3M 1s\n  3050K .......... .......... .......... .......... .......... 26% 34.7M 1s\n  3100K .......... .......... .......... .......... .......... 26% 54.3M 1s\n  3150K .......... .......... .......... .......... .......... 27% 15.3M 1s\n  3200K .......... .......... .......... .......... .......... 27% 17.1M 1s\n  3250K .......... .......... .......... .......... .......... 28% 44.6M 1s\n  3300K .......... .......... .......... .......... .......... 28% 15.1M 1s\n  3350K .......... .......... .......... .......... .......... 28% 22.8M 1s\n  3400K .......... .......... .......... .......... .......... 29% 25.1M 1s\n  3450K .......... .......... .......... .......... .......... 29% 42.4M 1s\n  3500K .......... .......... .......... .......... .......... 30% 16.0M 1s\n  3550K .......... .......... .......... .......... .......... 30% 27.1M 1s\n  3600K .......... .......... .......... .......... .......... 31% 23.9M 1s\n  3650K .......... .......... .......... .......... .......... 31% 13.2M 1s\n  3700K .......... .......... .......... .......... .......... 31% 80.9M 1s\n  3750K .......... .......... .......... .......... .......... 32% 32.1M 1s\n  3800K .......... .......... .......... .......... .......... 32% 30.2M 1s\n  3850K .......... .......... .......... .......... .......... 33% 23.7M 1s\n  3900K .......... .......... .......... .......... .......... 33% 25.3M 1s\n  3950K .......... .......... .......... .......... .......... 34% 23.2M 1s\n  4000K .......... .......... .......... .......... .......... 34% 64.0M 1s\n  4050K .......... .......... .......... .......... .......... 34% 18.8M 1s\n  4100K .......... .......... .......... .......... .......... 35% 23.4M 1s\n  4150K .......... .......... .......... .......... .......... 35% 32.5M 1s\n  4200K .......... .......... .......... .......... .......... 36% 64.8M 1s\n  4250K .......... .......... .......... .......... .......... 36% 30.9M 1s\n  4300K .......... .......... .......... .......... .......... 37% 24.3M 1s\n  4350K .......... .......... .......... .......... .......... 37% 41.4M 1s\n  4400K .......... .......... .......... .......... .......... 37% 24.2M 1s\n  4450K .......... .......... .......... .......... .......... 38%  107M 1s\n  4500K .......... .......... .......... .......... .......... 38% 41.3M 1s\n  4550K .......... .......... .......... .......... .......... 39% 23.6M 1s\n  4600K .......... .......... .......... .......... .......... 39% 20.1M 1s\n  4650K .......... .......... .......... .......... .......... 40% 56.0M 1s\n  4700K .......... .......... .......... .......... .......... 40% 13.4M 1s\n  4750K .......... .......... .......... .......... .......... 40% 19.0M 1s\n  4800K .......... .......... .......... .......... .......... 41% 22.2M 1s\n  4850K .......... .......... .......... .......... .......... 41% 14.8M 1s\n  4900K .......... .......... .......... .......... .......... 42% 21.3M 1s\n  4950K .......... .......... .......... .......... .......... 42% 82.0M 1s\n  5000K .......... .......... .......... .......... .......... 43% 66.7M 1s\n  5050K .......... .......... .......... .......... .......... 43% 22.6M 1s\n  5100K .......... .......... .......... .......... .......... 43% 58.0M 1s\n  5150K .......... .......... .......... .......... .......... 44% 29.9M 1s\n  5200K .......... .......... .......... .......... .......... 44% 3.41M 1s\n  5250K .......... .......... .......... .......... .......... 45% 34.3M 1s\n  5300K .......... .......... .......... .......... .......... 45% 68.1M 1s\n  5350K .......... .......... .......... .......... .......... 46% 82.1M 1s\n  5400K .......... .......... .......... .......... .......... 46% 35.6M 1s\n  5450K .......... .......... .......... .......... .......... 46% 23.1M 1s\n  5500K .......... .......... .......... .......... .......... 47% 59.6M 1s\n  5550K .......... .......... .......... .......... .......... 47% 45.7M 1s\n  5600K .......... .......... .......... .......... .......... 48% 26.1M 1s\n  5650K .......... .......... .......... .......... .......... 48% 67.4M 1s\n  5700K .......... .......... .......... .......... .......... 49% 73.7M 1s\n  5750K .......... .......... .......... .......... .......... 49% 77.2M 1s\n  5800K .......... .......... .......... .......... .......... 49% 45.9M 1s\n  5850K .......... .......... .......... .......... .......... 50% 95.9M 1s\n  5900K .......... .......... .......... .......... .......... 50% 9.92M 1s\n  5950K .......... .......... .......... .......... .......... 51%  101M 1s\n  6000K .......... .......... .......... .......... .......... 51%  143M 1s\n  6050K .......... .......... .......... .......... .......... 51%  132M 1s\n  6100K .......... .......... .......... .......... .......... 52%  138M 1s\n  6150K .......... .......... .......... .......... .......... 52%  133M 1s\n  6200K .......... .......... .......... .......... .......... 53% 18.9M 1s\n  6250K .......... .......... .......... .......... .......... 53% 17.6M 1s\n  6300K .......... .......... .......... .......... .......... 54% 10.5M 0s\n  6350K .......... .......... .......... .......... .......... 54% 10.7M 0s\n  6400K .......... .......... .......... .......... .......... 54% 43.6M 0s\n  6450K .......... .......... .......... .......... .......... 55% 21.9M 0s\n  6500K .......... .......... .......... .......... .......... 55% 37.0M 0s\n  6550K .......... .......... .......... .......... .......... 56% 34.6M 0s\n  6600K .......... .......... .......... .......... .......... 56% 37.4M 0s\n  6650K .......... .......... .......... .......... .......... 57% 36.7M 0s\n  6700K .......... .......... .......... .......... .......... 57% 32.0M 0s\n  6750K .......... .......... .......... .......... .......... 57% 23.9M 0s\n  6800K .......... .......... .......... .......... .......... 58%  102M 0s\n  6850K .......... .......... .......... .......... .......... 58% 38.7M 0s\n  6900K .......... .......... .......... .......... .......... 59% 12.9M 0s\n  6950K .......... .......... .......... .......... .......... 59% 22.4M 0s\n  7000K .......... .......... .......... .......... .......... 60% 60.1M 0s\n  7050K .......... .......... .......... .......... .......... 60% 47.2M 0s\n  7100K .......... .......... .......... .......... .......... 60% 37.2M 0s\n  7150K .......... .......... .......... .......... .......... 61% 34.9M 0s\n  7200K .......... .......... .......... .......... .......... 61% 37.6M 0s\n  7250K .......... .......... .......... .......... .......... 62%  113M 0s\n  7300K .......... .......... .......... .......... .......... 62% 16.9M 0s\n  7350K .......... .......... .......... .......... .......... 63% 46.7M 0s\n  7400K .......... .......... .......... .......... .......... 63% 37.6M 0s\n  7450K .......... .......... .......... .......... .......... 63% 34.3M 0s\n  7500K .......... .......... .......... .......... .......... 64% 31.4M 0s\n  7550K .......... .......... .......... .......... .......... 64% 34.9M 0s\n  7600K .......... .......... .......... .......... .......... 65% 39.3M 0s\n  7650K .......... .......... .......... .......... .......... 65% 10.3M 0s\n  7700K .......... .......... .......... .......... .......... 66% 75.8M 0s\n  7750K .......... .......... .......... .......... .......... 66% 91.7M 0s\n  7800K .......... .......... .......... .......... .......... 66% 59.1M 0s\n  7850K .......... .......... .......... .......... .......... 67% 20.8M 0s\n  7900K .......... .......... .......... .......... .......... 67% 76.4M 0s\n  7950K .......... .......... .......... .......... .......... 68% 13.4M 0s\n  8000K .......... .......... .......... .......... .......... 68% 19.7M 0s\n  8050K .......... .......... .......... .......... .......... 69% 17.9M 0s\n  8100K .......... .......... .......... .......... .......... 69% 20.2M 0s\n  8150K .......... .......... .......... .......... .......... 69% 51.6M 0s\n  8200K .......... .......... .......... .......... .......... 70% 17.5M 0s\n  8250K .......... .......... .......... .......... .......... 70% 22.9M 0s\n  8300K .......... .......... .......... .......... .......... 71% 53.5M 0s\n  8350K .......... .......... .......... .......... .......... 71% 17.4M 0s\n  8400K .......... .......... .......... .......... .......... 72% 23.2M 0s\n  8450K .......... .......... .......... .......... .......... 72% 3.07M 0s\n  8500K .......... .......... .......... .......... .......... 72% 23.6M 0s\n  8550K .......... .......... .......... .......... .......... 73% 29.6M 0s\n  8600K .......... .......... .......... .......... .......... 73% 38.1M 0s\n  8650K .......... .......... .......... .......... .......... 74% 14.7M 0s\n  8700K .......... .......... .......... .......... .......... 74% 22.3M 0s\n  8750K .......... .......... .......... .......... .......... 74% 22.5M 0s\n  8800K .......... .......... .......... .......... .......... 75% 17.1M 0s\n  8850K .......... .......... .......... .......... .......... 75%  122M 0s\n  8900K .......... .......... .......... .......... .......... 76% 8.16M 0s\n  8950K .......... .......... .......... .......... .......... 76% 14.6M 0s\n  9000K .......... .......... .......... .......... .......... 77%  105M 0s\n  9050K .......... .......... .......... .......... .......... 77% 30.1M 0s\n  9100K .......... .......... .......... .......... .......... 77% 47.5M 0s\n  9150K .......... .......... .......... .......... .......... 78% 38.0M 0s\n  9200K .......... .......... .......... .......... .......... 78% 26.7M 0s\n  9250K .......... .......... .......... .......... .......... 79%  117M 0s\n  9300K .......... .......... .......... .......... .......... 79% 34.0M 0s\n  9350K .......... .......... .......... .......... .......... 80%  108M 0s\n  9400K .......... .......... .......... .......... .......... 80%  141M 0s\n  9450K .......... .......... .......... .......... .......... 80% 56.6M 0s\n  9500K .......... .......... .......... .......... .......... 81% 34.5M 0s\n  9550K .......... .......... .......... .......... .......... 81% 27.2M 0s\n  9600K .......... .......... .......... .......... .......... 82% 13.7M 0s\n  9650K .......... .......... .......... .......... .......... 82%  112M 0s\n  9700K .......... .......... .......... .......... .......... 83%  110M 0s\n  9750K .......... .......... .......... .......... .......... 83% 28.9M 0s\n  9800K .......... .......... .......... .......... .......... 83%  111M 0s\n  9850K .......... .......... .......... .......... .......... 84% 26.7M 0s\n  9900K .......... .......... .......... .......... .......... 84%  141M 0s\n  9950K .......... .......... .......... .......... .......... 85% 18.0M 0s\n 10000K .......... .......... .......... .......... .......... 85%  103M 0s\n 10050K .......... .......... .......... .......... .......... 86% 28.6M 0s\n 10100K .......... .......... .......... .......... .......... 86% 95.2M 0s\n 10150K .......... .......... .......... .......... .......... 86% 27.4M 0s\n 10200K .......... .......... .......... .......... .......... 87%  110M 0s\n 10250K .......... .......... .......... .......... .......... 87% 22.8M 0s\n 10300K .......... .......... .......... .......... .......... 88% 57.3M 0s\n 10350K .......... .......... .......... .......... .......... 88% 30.5M 0s\n 10400K .......... .......... .......... .......... .......... 89% 39.8M 0s\n 10450K .......... .......... .......... .......... .......... 89% 74.0M 0s\n 10500K .......... .......... .......... .......... .......... 89% 33.1M 0s\n 10550K .......... .......... .......... .......... .......... 90% 71.3M 0s\n 10600K .......... .......... .......... .......... .......... 90% 41.7M 0s\n 10650K .......... .......... .......... .......... .......... 91% 30.1M 0s\n 10700K .......... .......... .......... .......... .......... 91% 78.5M 0s\n 10750K .......... .......... .......... .......... .......... 92% 24.8M 0s\n 10800K .......... .......... .......... .......... .......... 92% 46.7M 0s\n 10850K .......... .......... .......... .......... .......... 92% 38.7M 0s\n 10900K .......... .......... .......... .......... .......... 93% 31.7M 0s\n 10950K .......... .......... .......... .......... .......... 93% 23.5M 0s\n 11000K .......... .......... .......... .......... .......... 94% 44.8M 0s\n 11050K .......... .......... .......... .......... .......... 94% 21.9M 0s\n 11100K .......... .......... .......... .......... .......... 95% 21.7M 0s\n 11150K .......... .......... .......... .......... .......... 95% 28.4M 0s\n 11200K .......... .......... .......... .......... .......... 95% 21.2M 0s\n 11250K .......... .......... .......... .......... .......... 96% 70.2M 0s\n 11300K .......... .......... .......... .......... .......... 96% 22.9M 0s\n 11350K .......... .......... .......... .......... .......... 97% 34.8M 0s\n 11400K .......... .......... .......... .......... .......... 97% 15.8M 0s\n 11450K .......... .......... .......... .......... .......... 98% 58.4M 0s\n 11500K .......... .......... .......... .......... .......... 98% 22.3M 0s\n 11550K .......... .......... .......... .......... .......... 98% 24.6M 0s\n 11600K .......... .......... .......... .......... .......... 99% 18.8M 0s\n 11650K .......... .......... .......... .......... .......... 99% 30.5M 0s\n 11700K .......... .......... .......... ...                  100% 38.6M=0.8s\n\n2022-06-04 11:56:13 (14.7 MB/s) - /tmp/Kaggle_NoFreq_256Units_100epoch_onlyInitialCell.csv saved [12015296/12015296]\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["--2022-06-04 11:56:11--  https://www.dropbox.com/s/r5fphpb09dreepq/model_OnlyInitial_NoFreq_256Units_100epoch.h5?dl=1\nResolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6016:18::a27d:112\nConnecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: /s/dl/r5fphpb09dreepq/model_OnlyInitial_NoFreq_256Units_100epoch.h5 [following]\n--2022-06-04 11:56:12--  https://www.dropbox.com/s/dl/r5fphpb09dreepq/model_OnlyInitial_NoFreq_256Units_100epoch.h5\nReusing existing connection to www.dropbox.com:443.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://ucac65acb331719a1be6248a4c7e.dl.dropboxusercontent.com/cd/0/get/BmjHtuR3VYmzUiYvNtliKkVPXimCpNkSr3-uu_LrpuJDxKpmjd5KK0HaMCciPQ8NZ7iAokAbpnCtRKAMMvHDWJ2ivoW4nh97Prx8xmS9Jl948MyevUCdSG2TopB5kmfICwzusgmJTicnjFgdVQADIVodjEkx3VZEMzTcATJrToC54ewk_B2qOnBRflJ7x71CyjY/file?dl=1# [following]\n--2022-06-04 11:56:12--  https://ucac65acb331719a1be6248a4c7e.dl.dropboxusercontent.com/cd/0/get/BmjHtuR3VYmzUiYvNtliKkVPXimCpNkSr3-uu_LrpuJDxKpmjd5KK0HaMCciPQ8NZ7iAokAbpnCtRKAMMvHDWJ2ivoW4nh97Prx8xmS9Jl948MyevUCdSG2TopB5kmfICwzusgmJTicnjFgdVQADIVodjEkx3VZEMzTcATJrToC54ewk_B2qOnBRflJ7x71CyjY/file?dl=1\nResolving ucac65acb331719a1be6248a4c7e.dl.dropboxusercontent.com (ucac65acb331719a1be6248a4c7e.dl.dropboxusercontent.com)... 162.125.8.15, 2620:100:6016:15::a27d:10f\nConnecting to ucac65acb331719a1be6248a4c7e.dl.dropboxusercontent.com (ucac65acb331719a1be6248a4c7e.dl.dropboxusercontent.com)|162.125.8.15|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 12015296 (11M) [application/binary]\nSaving to: /tmp/Kaggle_NoFreq_256Units_100epoch_onlyInitialCell.csv\n\n     0K .......... .......... .......... .......... ..........  0%  490K 24s\n    50K .......... .......... .......... .......... ..........  0%  988K 18s\n   100K .......... .......... .......... .......... ..........  1% 1.18M 15s\n   150K .......... .......... .......... .......... ..........  1% 1.49M 13s\n   200K .......... .......... .......... .......... ..........  2% 2.90M 11s\n   250K .......... .......... .......... .......... ..........  2% 2.65M 10s\n   300K .......... .......... .......... .......... ..........  2% 3.02M 9s\n   350K .......... .......... .......... .......... ..........  3% 3.08M 8s\n   400K .......... .......... .......... .......... ..........  3% 5.18M 8s\n   450K .......... .......... .......... .......... ..........  4% 5.96M 7s\n   500K .......... .......... .......... .......... ..........  4% 5.80M 7s\n   550K .......... .......... .......... .......... ..........  5% 8.08M 6s\n   600K .......... .......... .......... .......... ..........  5% 7.81M 6s\n   650K .......... .......... .......... .......... ..........  5% 7.98M 5s\n   700K .......... .......... .......... .......... ..........  6% 9.59M 5s\n   750K .......... .......... .......... .......... ..........  6% 6.22M 5s\n   800K .......... .......... .......... .......... ..........  7% 8.94M 5s\n   850K .......... .......... .......... .......... ..........  7% 10.6M 4s\n   900K .......... .......... .......... .......... ..........  8% 10.5M 4s\n   950K .......... .......... .......... .......... ..........  8% 11.4M 4s\n  1000K .......... .......... .......... .......... ..........  8% 11.9M 4s\n  1050K .......... .......... .......... .......... ..........  9% 12.1M 4s\n  1100K .......... .......... .......... .......... ..........  9% 13.4M 4s\n  1150K .......... .......... .......... .......... .......... 10% 12.8M 3s\n  1200K .......... .......... .......... .......... .......... 10% 13.8M 3s\n  1250K .......... .......... .......... .......... .......... 11% 21.2M 3s\n  1300K .......... .......... .......... .......... .......... 11% 17.2M 3s\n  1350K .......... .......... .......... .......... .......... 11% 17.6M 3s\n  1400K .......... .......... .......... .......... .......... 12% 16.6M 3s\n  1450K .......... .......... .......... .......... .......... 12% 18.3M 3s\n  1500K .......... .......... .......... .......... .......... 13% 12.7M 3s\n  1550K .......... .......... .......... .......... .......... 13% 26.0M 3s\n  1600K .......... .......... .......... .......... .......... 14% 17.4M 3s\n  1650K .......... .......... .......... .......... .......... 14% 20.8M 2s\n  1700K .......... .......... .......... .......... .......... 14% 30.7M 2s\n  1750K .......... .......... .......... .......... .......... 15% 22.7M 2s\n  1800K .......... .......... .......... .......... .......... 15% 18.2M 2s\n  1850K .......... .......... .......... .......... .......... 16% 24.3M 2s\n  1900K .......... .......... .......... .......... .......... 16% 28.1M 2s\n  1950K .......... .......... .......... .......... .......... 17% 18.2M 2s\n  2000K .......... .......... .......... .......... .......... 17% 29.0M 2s\n  2050K .......... .......... .......... .......... .......... 17% 21.2M 2s\n  2100K .......... .......... .......... .......... .......... 18% 22.4M 2s\n  2150K .......... .......... .......... .......... .......... 18% 31.3M 2s\n  2200K .......... .......... .......... .......... .......... 19% 36.0M 2s\n  2250K .......... .......... .......... .......... .......... 19% 20.8M 2s\n  2300K .......... .......... .......... .......... .......... 20% 47.7M 2s\n  2350K .......... .......... .......... .......... .......... 20% 22.4M 2s\n  2400K .......... .......... .......... .......... .......... 20% 25.5M 2s\n  2450K .......... .......... .......... .......... .......... 21% 34.9M 2s\n  2500K .......... .......... .......... .......... .......... 21% 26.1M 2s\n  2550K .......... .......... .......... .......... .......... 22% 36.1M 2s\n  2600K .......... .......... .......... .......... .......... 22% 29.5M 2s\n  2650K .......... .......... .......... .......... .......... 23% 32.0M 2s\n  2700K .......... .......... .......... .......... .......... 23% 32.6M 1s\n  2750K .......... .......... .......... .......... .......... 23% 29.0M 1s\n  2800K .......... .......... .......... .......... .......... 24% 44.0M 1s\n  2850K .......... .......... .......... .......... .......... 24% 29.4M 1s\n  2900K .......... .......... .......... .......... .......... 25% 46.7M 1s\n  2950K .......... .......... .......... .......... .......... 25% 30.8M 1s\n  3000K .......... .......... .......... .......... .......... 25% 39.3M 1s\n  3050K .......... .......... .......... .......... .......... 26% 34.7M 1s\n  3100K .......... .......... .......... .......... .......... 26% 54.3M 1s\n  3150K .......... .......... .......... .......... .......... 27% 15.3M 1s\n  3200K .......... .......... .......... .......... .......... 27% 17.1M 1s\n  3250K .......... .......... .......... .......... .......... 28% 44.6M 1s\n  3300K .......... .......... .......... .......... .......... 28% 15.1M 1s\n  3350K .......... .......... .......... .......... .......... 28% 22.8M 1s\n  3400K .......... .......... .......... .......... .......... 29% 25.1M 1s\n  3450K .......... .......... .......... .......... .......... 29% 42.4M 1s\n  3500K .......... .......... .......... .......... .......... 30% 16.0M 1s\n  3550K .......... .......... .......... .......... .......... 30% 27.1M 1s\n  3600K .......... .......... .......... .......... .......... 31% 23.9M 1s\n  3650K .......... .......... .......... .......... .......... 31% 13.2M 1s\n  3700K .......... .......... .......... .......... .......... 31% 80.9M 1s\n  3750K .......... .......... .......... .......... .......... 32% 32.1M 1s\n  3800K .......... .......... .......... .......... .......... 32% 30.2M 1s\n  3850K .......... .......... .......... .......... .......... 33% 23.7M 1s\n  3900K .......... .......... .......... .......... .......... 33% 25.3M 1s\n  3950K .......... .......... .......... .......... .......... 34% 23.2M 1s\n  4000K .......... .......... .......... .......... .......... 34% 64.0M 1s\n  4050K .......... .......... .......... .......... .......... 34% 18.8M 1s\n  4100K .......... .......... .......... .......... .......... 35% 23.4M 1s\n  4150K .......... .......... .......... .......... .......... 35% 32.5M 1s\n  4200K .......... .......... .......... .......... .......... 36% 64.8M 1s\n  4250K .......... .......... .......... .......... .......... 36% 30.9M 1s\n  4300K .......... .......... .......... .......... .......... 37% 24.3M 1s\n  4350K .......... .......... .......... .......... .......... 37% 41.4M 1s\n  4400K .......... .......... .......... .......... .......... 37% 24.2M 1s\n  4450K .......... .......... .......... .......... .......... 38%  107M 1s\n  4500K .......... .......... .......... .......... .......... 38% 41.3M 1s\n  4550K .......... .......... .......... .......... .......... 39% 23.6M 1s\n  4600K .......... .......... .......... .......... .......... 39% 20.1M 1s\n  4650K .......... .......... .......... .......... .......... 40% 56.0M 1s\n  4700K .......... .......... .......... .......... .......... 40% 13.4M 1s\n  4750K .......... .......... .......... .......... .......... 40% 19.0M 1s\n  4800K .......... .......... .......... .......... .......... 41% 22.2M 1s\n  4850K .......... .......... .......... .......... .......... 41% 14.8M 1s\n  4900K .......... .......... .......... .......... .......... 42% 21.3M 1s\n  4950K .......... .......... .......... .......... .......... 42% 82.0M 1s\n  5000K .......... .......... .......... .......... .......... 43% 66.7M 1s\n  5050K .......... .......... .......... .......... .......... 43% 22.6M 1s\n  5100K .......... .......... .......... .......... .......... 43% 58.0M 1s\n  5150K .......... .......... .......... .......... .......... 44% 29.9M 1s\n  5200K .......... .......... .......... .......... .......... 44% 3.41M 1s\n  5250K .......... .......... .......... .......... .......... 45% 34.3M 1s\n  5300K .......... .......... .......... .......... .......... 45% 68.1M 1s\n  5350K .......... .......... .......... .......... .......... 46% 82.1M 1s\n  5400K .......... .......... .......... .......... .......... 46% 35.6M 1s\n  5450K .......... .......... .......... .......... .......... 46% 23.1M 1s\n  5500K .......... .......... .......... .......... .......... 47% 59.6M 1s\n  5550K .......... .......... .......... .......... .......... 47% 45.7M 1s\n  5600K .......... .......... .......... .......... .......... 48% 26.1M 1s\n  5650K .......... .......... .......... .......... .......... 48% 67.4M 1s\n  5700K .......... .......... .......... .......... .......... 49% 73.7M 1s\n  5750K .......... .......... .......... .......... .......... 49% 77.2M 1s\n  5800K .......... .......... .......... .......... .......... 49% 45.9M 1s\n  5850K .......... .......... .......... .......... .......... 50% 95.9M 1s\n  5900K .......... .......... .......... .......... .......... 50% 9.92M 1s\n  5950K .......... .......... .......... .......... .......... 51%  101M 1s\n  6000K .......... .......... .......... .......... .......... 51%  143M 1s\n  6050K .......... .......... .......... .......... .......... 51%  132M 1s\n  6100K .......... .......... .......... .......... .......... 52%  138M 1s\n  6150K .......... .......... .......... .......... .......... 52%  133M 1s\n  6200K .......... .......... .......... .......... .......... 53% 18.9M 1s\n  6250K .......... .......... .......... .......... .......... 53% 17.6M 1s\n  6300K .......... .......... .......... .......... .......... 54% 10.5M 0s\n  6350K .......... .......... .......... .......... .......... 54% 10.7M 0s\n  6400K .......... .......... .......... .......... .......... 54% 43.6M 0s\n  6450K .......... .......... .......... .......... .......... 55% 21.9M 0s\n  6500K .......... .......... .......... .......... .......... 55% 37.0M 0s\n  6550K .......... .......... .......... .......... .......... 56% 34.6M 0s\n  6600K .......... .......... .......... .......... .......... 56% 37.4M 0s\n  6650K .......... .......... .......... .......... .......... 57% 36.7M 0s\n  6700K .......... .......... .......... .......... .......... 57% 32.0M 0s\n  6750K .......... .......... .......... .......... .......... 57% 23.9M 0s\n  6800K .......... .......... .......... .......... .......... 58%  102M 0s\n  6850K .......... .......... .......... .......... .......... 58% 38.7M 0s\n  6900K .......... .......... .......... .......... .......... 59% 12.9M 0s\n  6950K .......... .......... .......... .......... .......... 59% 22.4M 0s\n  7000K .......... .......... .......... .......... .......... 60% 60.1M 0s\n  7050K .......... .......... .......... .......... .......... 60% 47.2M 0s\n  7100K .......... .......... .......... .......... .......... 60% 37.2M 0s\n  7150K .......... .......... .......... .......... .......... 61% 34.9M 0s\n  7200K .......... .......... .......... .......... .......... 61% 37.6M 0s\n  7250K .......... .......... .......... .......... .......... 62%  113M 0s\n  7300K .......... .......... .......... .......... .......... 62% 16.9M 0s\n  7350K .......... .......... .......... .......... .......... 63% 46.7M 0s\n  7400K .......... .......... .......... .......... .......... 63% 37.6M 0s\n  7450K .......... .......... .......... .......... .......... 63% 34.3M 0s\n  7500K .......... .......... .......... .......... .......... 64% 31.4M 0s\n  7550K .......... .......... .......... .......... .......... 64% 34.9M 0s\n  7600K .......... .......... .......... .......... .......... 65% 39.3M 0s\n  7650K .......... .......... .......... .......... .......... 65% 10.3M 0s\n  7700K .......... .......... .......... .......... .......... 66% 75.8M 0s\n  7750K .......... .......... .......... .......... .......... 66% 91.7M 0s\n  7800K .......... .......... .......... .......... .......... 66% 59.1M 0s\n  7850K .......... .......... .......... .......... .......... 67% 20.8M 0s\n  7900K .......... .......... .......... .......... .......... 67% 76.4M 0s\n  7950K .......... .......... .......... .......... .......... 68% 13.4M 0s\n  8000K .......... .......... .......... .......... .......... 68% 19.7M 0s\n  8050K .......... .......... .......... .......... .......... 69% 17.9M 0s\n  8100K .......... .......... .......... .......... .......... 69% 20.2M 0s\n  8150K .......... .......... .......... .......... .......... 69% 51.6M 0s\n  8200K .......... .......... .......... .......... .......... 70% 17.5M 0s\n  8250K .......... .......... .......... .......... .......... 70% 22.9M 0s\n  8300K .......... .......... .......... .......... .......... 71% 53.5M 0s\n  8350K .......... .......... .......... .......... .......... 71% 17.4M 0s\n  8400K .......... .......... .......... .......... .......... 72% 23.2M 0s\n  8450K .......... .......... .......... .......... .......... 72% 3.07M 0s\n  8500K .......... .......... .......... .......... .......... 72% 23.6M 0s\n  8550K .......... .......... .......... .......... .......... 73% 29.6M 0s\n  8600K .......... .......... .......... .......... .......... 73% 38.1M 0s\n  8650K .......... .......... .......... .......... .......... 74% 14.7M 0s\n  8700K .......... .......... .......... .......... .......... 74% 22.3M 0s\n  8750K .......... .......... .......... .......... .......... 74% 22.5M 0s\n  8800K .......... .......... .......... .......... .......... 75% 17.1M 0s\n  8850K .......... .......... .......... .......... .......... 75%  122M 0s\n  8900K .......... .......... .......... .......... .......... 76% 8.16M 0s\n  8950K .......... .......... .......... .......... .......... 76% 14.6M 0s\n  9000K .......... .......... .......... .......... .......... 77%  105M 0s\n  9050K .......... .......... .......... .......... .......... 77% 30.1M 0s\n  9100K .......... .......... .......... .......... .......... 77% 47.5M 0s\n  9150K .......... .......... .......... .......... .......... 78% 38.0M 0s\n  9200K .......... .......... .......... .......... .......... 78% 26.7M 0s\n  9250K .......... .......... .......... .......... .......... 79%  117M 0s\n  9300K .......... .......... .......... .......... .......... 79% 34.0M 0s\n  9350K .......... .......... .......... .......... .......... 80%  108M 0s\n  9400K .......... .......... .......... .......... .......... 80%  141M 0s\n  9450K .......... .......... .......... .......... .......... 80% 56.6M 0s\n  9500K .......... .......... .......... .......... .......... 81% 34.5M 0s\n  9550K .......... .......... .......... .......... .......... 81% 27.2M 0s\n  9600K .......... .......... .......... .......... .......... 82% 13.7M 0s\n  9650K .......... .......... .......... .......... .......... 82%  112M 0s\n  9700K .......... .......... .......... .......... .......... 83%  110M 0s\n  9750K .......... .......... .......... .......... .......... 83% 28.9M 0s\n  9800K .......... .......... .......... .......... .......... 83%  111M 0s\n  9850K .......... .......... .......... .......... .......... 84% 26.7M 0s\n  9900K .......... .......... .......... .......... .......... 84%  141M 0s\n  9950K .......... .......... .......... .......... .......... 85% 18.0M 0s\n 10000K .......... .......... .......... .......... .......... 85%  103M 0s\n 10050K .......... .......... .......... .......... .......... 86% 28.6M 0s\n 10100K .......... .......... .......... .......... .......... 86% 95.2M 0s\n 10150K .......... .......... .......... .......... .......... 86% 27.4M 0s\n 10200K .......... .......... .......... .......... .......... 87%  110M 0s\n 10250K .......... .......... .......... .......... .......... 87% 22.8M 0s\n 10300K .......... .......... .......... .......... .......... 88% 57.3M 0s\n 10350K .......... .......... .......... .......... .......... 88% 30.5M 0s\n 10400K .......... .......... .......... .......... .......... 89% 39.8M 0s\n 10450K .......... .......... .......... .......... .......... 89% 74.0M 0s\n 10500K .......... .......... .......... .......... .......... 89% 33.1M 0s\n 10550K .......... .......... .......... .......... .......... 90% 71.3M 0s\n 10600K .......... .......... .......... .......... .......... 90% 41.7M 0s\n 10650K .......... .......... .......... .......... .......... 91% 30.1M 0s\n 10700K .......... .......... .......... .......... .......... 91% 78.5M 0s\n 10750K .......... .......... .......... .......... .......... 92% 24.8M 0s\n 10800K .......... .......... .......... .......... .......... 92% 46.7M 0s\n 10850K .......... .......... .......... .......... .......... 92% 38.7M 0s\n 10900K .......... .......... .......... .......... .......... 93% 31.7M 0s\n 10950K .......... .......... .......... .......... .......... 93% 23.5M 0s\n 11000K .......... .......... .......... .......... .......... 94% 44.8M 0s\n 11050K .......... .......... .......... .......... .......... 94% 21.9M 0s\n 11100K .......... .......... .......... .......... .......... 95% 21.7M 0s\n 11150K .......... .......... .......... .......... .......... 95% 28.4M 0s\n 11200K .......... .......... .......... .......... .......... 95% 21.2M 0s\n 11250K .......... .......... .......... .......... .......... 96% 70.2M 0s\n 11300K .......... .......... .......... .......... .......... 96% 22.9M 0s\n 11350K .......... .......... .......... .......... .......... 97% 34.8M 0s\n 11400K .......... .......... .......... .......... .......... 97% 15.8M 0s\n 11450K .......... .......... .......... .......... .......... 98% 58.4M 0s\n 11500K .......... .......... .......... .......... .......... 98% 22.3M 0s\n 11550K .......... .......... .......... .......... .......... 98% 24.6M 0s\n 11600K .......... .......... .......... .......... .......... 99% 18.8M 0s\n 11650K .......... .......... .......... .......... .......... 99% 30.5M 0s\n 11700K .......... .......... .......... ...                  100% 38.6M=0.8s\n\n2022-06-04 11:56:13 (14.7 MB/s) - /tmp/Kaggle_NoFreq_256Units_100epoch_onlyInitialCell.csv saved [12015296/12015296]\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["model3_Initialmoves = keras.models.load_model(\"/tmp/Kaggle_NoFreq_256Units_100epoch_onlyInitialCell.csv\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ca506868-b9c5-4c58-a58f-d2efc9346a88"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Plotting of the loss and accuracy values during the train"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"90c2b286-990b-44ff-84c7-6bde5ce82f52"}}},{"cell_type":"code","source":["plt.plot(history.history['loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train'], loc='upper right')\nplt.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b9ccf0c2-6ba6-4e48-a56c-89072fce2d0b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["<img src=\"https://www.dropbox.com/s/2cwu18r732s8515/model5_loss.png?dl=1\">"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f7ac874e-b519-4342-ba06-d38ea8e41ec7"}}},{"cell_type":"code","source":["plt.plot(history.history['accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train'], loc='upper left')\nplt.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b89329b3-baba-450c-b358-502015d0e233"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["<img src=\"https://www.dropbox.com/s/hprvvxvw4l6gjz6/Model5_accuracy.png?dl=1\">"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7a25adfe-1bf7-4cb4-a5e0-6afb600e203c"}}},{"cell_type":"markdown","source":["##### Fourth Model created using the most frequent moves\nFor the fourth model in this approach, a new dictionary was calculated by taking only the most frequent moves and giving all others an additional move. With this method I tried to reduce the dictionary to increase performance."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"66a63522-bf10-4c5b-8d46-92c8f0d0a237"}}},{"cell_type":"markdown","source":["In the following cell I calculate the frequence of the moves on the games contained in x."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"30792fbf-531d-470d-9b75-9d720d3b9984"}}},{"cell_type":"code","source":["frequency={}\nfor el in x:\n  for e in el:\n    e=int(e)\n    if e in frequency.keys():\n      frequency[e]=frequency[e]+1\n    else:\n      frequency[e]=1"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2cb1a22f-4329-47df-9e9f-167f9510122b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["In the next cell is the declaration of two functions used on the frequency dictionary just computed. <br>\nThe **averageFreq** function calculates what is the average occurrence of the moves within the dictionary.<br>\nThe **freq_dic** funcion takes as input the average value of occurrences and generates a dictionary only with the moves that occur more times than the average."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6007c818-6c79-4137-9cc1-dde20c0b0d9f"}}},{"cell_type":"code","source":["def averageFreq():\n  s=0\n  for el in frequency:\n    s=s+frequency[el]\n  return s/len(frequency)\n\ndef freq_dic(avg):\n  freq_dic={}\n  for el in frequency:\n    if frequency[el]>=avg:\n      freq_dic[el]=frequency[el]\n  return freq_dic"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f3bc2f5e-9ba9-4dfa-b114-fbd757828c18"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["In the next cell I call the function averageFreq and use the value it returns to compute the new dictionary with the function freq_dic."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"39feae8a-f462-49ba-b233-2bcfb28287aa"}}},{"cell_type":"code","source":["avg_freq=int(averageFreq())\nprint(\"The average frequency of moves is\",avg_freq)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2c6fd869-bb3a-4de0-83ac-32c79ad78b15"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"The average frequency of moves is 10396\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["The average frequency of moves is 10396\n"]}}],"execution_count":0},{"cell_type":"code","source":["freq_dic=freq_dic(avg_freq)\nprint(\"The number of moves that occur more times than the average is\",len(freq_dic))\nprint(freq_dic)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d33d1bfe-2227-4915-9e85-ba9e0db42cfe"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"The number of moves that occur more times than the average is 115\n{244: 44558, 518: 53971, 32: 40670, 247: 41667, 261: 71635, 219: 74381, 507: 24775, 139: 21873, 159: 30045, 350: 40650, 216: 21393, 166: 38690, 329: 14191, 390: 15589, 10: 23593, 378: 13690, 382: 15434, 190: 114600, 361: 22403, 335: 18416, 99: 34063, 246: 25407, 337: 35177, 292: 13805, 98: 39876, 387: 15967, 515: 15090, 412: 18258, 182: 26142, 91: 33061, 64: 16297, 322: 33013, 467: 39676, 161: 15110, 50: 15312, 270: 15291, 225: 22816, 522: 1304258, 447: 11454, 355: 14081, 438: 64378, 285: 23927, 49: 38850, 408: 17223, 368: 14357, 402: 21898, 144: 47954, 256: 31615, 286: 13280, 314: 14087, 238: 24104, 370: 18001, 463: 29806, 218: 46079, 72: 17354, 422: 28841, 138: 44880, 167: 53653, 491: 11121, 191: 25406, 242: 20740, 116: 14492, 317: 35584, 324: 16493, 343: 13888, 404: 21331, 336: 10923, 21: 22188, 330: 11119, 464: 30860, 437: 19870, 420: 13626, 201: 30411, 15: 33145, 149: 32238, 51: 11979, 411: 11887, 325: 24094, 239: 18133, 120: 16467, 265: 22859, 96: 41444, 386: 19436, 195: 15356, 93: 17875, 60: 14195, 81: 24677, 469: 23319, 478: 25585, 392: 20300, 441: 17420, 440: 21138, 485: 21735, 340: 33919, 372: 10553, 263: 10404, 377: 15120, 9: 10937, 44: 11475, 457: 11934, 277: 22662, 17: 19613, 473: 13185, 267: 10552, 186: 24922, 349: 11410, 176: 23240, 77: 13914, 184: 12686, 12: 18488, 115: 12509, 121: 20706, 123: 10996, 280: 12426, 46: 13860}\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["The number of moves that occur more times than the average is 115\n{244: 44558, 518: 53971, 32: 40670, 247: 41667, 261: 71635, 219: 74381, 507: 24775, 139: 21873, 159: 30045, 350: 40650, 216: 21393, 166: 38690, 329: 14191, 390: 15589, 10: 23593, 378: 13690, 382: 15434, 190: 114600, 361: 22403, 335: 18416, 99: 34063, 246: 25407, 337: 35177, 292: 13805, 98: 39876, 387: 15967, 515: 15090, 412: 18258, 182: 26142, 91: 33061, 64: 16297, 322: 33013, 467: 39676, 161: 15110, 50: 15312, 270: 15291, 225: 22816, 522: 1304258, 447: 11454, 355: 14081, 438: 64378, 285: 23927, 49: 38850, 408: 17223, 368: 14357, 402: 21898, 144: 47954, 256: 31615, 286: 13280, 314: 14087, 238: 24104, 370: 18001, 463: 29806, 218: 46079, 72: 17354, 422: 28841, 138: 44880, 167: 53653, 491: 11121, 191: 25406, 242: 20740, 116: 14492, 317: 35584, 324: 16493, 343: 13888, 404: 21331, 336: 10923, 21: 22188, 330: 11119, 464: 30860, 437: 19870, 420: 13626, 201: 30411, 15: 33145, 149: 32238, 51: 11979, 411: 11887, 325: 24094, 239: 18133, 120: 16467, 265: 22859, 96: 41444, 386: 19436, 195: 15356, 93: 17875, 60: 14195, 81: 24677, 469: 23319, 478: 25585, 392: 20300, 441: 17420, 440: 21138, 485: 21735, 340: 33919, 372: 10553, 263: 10404, 377: 15120, 9: 10937, 44: 11475, 457: 11934, 277: 22662, 17: 19613, 473: 13185, 267: 10552, 186: 24922, 349: 11410, 176: 23240, 77: 13914, 184: 12686, 12: 18488, 115: 12509, 121: 20706, 123: 10996, 280: 12426, 46: 13860}\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["So far freq_dic is composed of the encoding of the move as the key and the frequency of that move as the value. Now we assign a new encoding to the old encoding, since the numbers present from the old encoding are not in order and the one hot encoder would return vectors too large compared to what we need. <br>\nAlso at the end of the dictionary an extra move to encode all the moves that occur less times than average is added."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aeca1cf8-9ba7-40da-9bd2-047e927c33e8"}}},{"cell_type":"code","source":["dic_f=dict((c, i) for i, c in enumerate(freq_dic.keys()))\n#Element for uncommon moves\ndic_f[len(int_moves)]=len(dic_f)\nprint(dic_f)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ef669c31-39cb-4823-bee0-c9a564e33cd8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"{244: 0, 518: 1, 32: 2, 247: 3, 261: 4, 219: 5, 507: 6, 139: 7, 159: 8, 350: 9, 216: 10, 166: 11, 329: 12, 390: 13, 10: 14, 378: 15, 382: 16, 190: 17, 361: 18, 335: 19, 99: 20, 246: 21, 337: 22, 292: 23, 98: 24, 387: 25, 515: 26, 412: 27, 182: 28, 91: 29, 64: 30, 322: 31, 467: 32, 161: 33, 50: 34, 270: 35, 225: 36, 522: 37, 447: 38, 355: 39, 438: 40, 285: 41, 49: 42, 408: 43, 368: 44, 402: 45, 144: 46, 256: 47, 286: 48, 314: 49, 238: 50, 370: 51, 463: 52, 218: 53, 72: 54, 422: 55, 138: 56, 167: 57, 491: 58, 191: 59, 242: 60, 116: 61, 317: 62, 324: 63, 343: 64, 404: 65, 336: 66, 21: 67, 330: 68, 464: 69, 437: 70, 420: 71, 201: 72, 15: 73, 149: 74, 51: 75, 411: 76, 325: 77, 239: 78, 120: 79, 265: 80, 96: 81, 386: 82, 195: 83, 93: 84, 60: 85, 81: 86, 469: 87, 478: 88, 392: 89, 441: 90, 440: 91, 485: 92, 340: 93, 372: 94, 263: 95, 377: 96, 9: 97, 44: 98, 457: 99, 277: 100, 17: 101, 473: 102, 267: 103, 186: 104, 349: 105, 176: 106, 77: 107, 184: 108, 12: 109, 115: 110, 121: 111, 123: 112, 280: 113, 46: 114, 523: 115}\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["{244: 0, 518: 1, 32: 2, 247: 3, 261: 4, 219: 5, 507: 6, 139: 7, 159: 8, 350: 9, 216: 10, 166: 11, 329: 12, 390: 13, 10: 14, 378: 15, 382: 16, 190: 17, 361: 18, 335: 19, 99: 20, 246: 21, 337: 22, 292: 23, 98: 24, 387: 25, 515: 26, 412: 27, 182: 28, 91: 29, 64: 30, 322: 31, 467: 32, 161: 33, 50: 34, 270: 35, 225: 36, 522: 37, 447: 38, 355: 39, 438: 40, 285: 41, 49: 42, 408: 43, 368: 44, 402: 45, 144: 46, 256: 47, 286: 48, 314: 49, 238: 50, 370: 51, 463: 52, 218: 53, 72: 54, 422: 55, 138: 56, 167: 57, 491: 58, 191: 59, 242: 60, 116: 61, 317: 62, 324: 63, 343: 64, 404: 65, 336: 66, 21: 67, 330: 68, 464: 69, 437: 70, 420: 71, 201: 72, 15: 73, 149: 74, 51: 75, 411: 76, 325: 77, 239: 78, 120: 79, 265: 80, 96: 81, 386: 82, 195: 83, 93: 84, 60: 85, 81: 86, 469: 87, 478: 88, 392: 89, 441: 90, 440: 91, 485: 92, 340: 93, 372: 94, 263: 95, 377: 96, 9: 97, 44: 98, 457: 99, 277: 100, 17: 101, 473: 102, 267: 103, 186: 104, 349: 105, 176: 106, 77: 107, 184: 108, 12: 109, 115: 110, 121: 111, 123: 112, 280: 113, 46: 114, 523: 115}\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["Save the dictionary in the DBFS."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"050c34d2-32b0-4b2e-a517-898ab8b4e815"}}},{"cell_type":"code","source":["import json\nwith open(\"/tmp/dic_freq_moves_onlyInitial.txt\", \"w+\") as fp:\n    json.dump(dic_f, fp)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f082c224-0e80-453a-b095-45a99c2f6f40"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.cp(\"file:/tmp/dic_freq_moves_onlyInitial.txt\", \"dbfs:/tmp/dic_freq_moves_onlyInitial.txt\")\ndisplay(dbutils.fs.ls(\"/tmp/dic_freq_moves_onlyInitial.txt\"))\ndbutils.fs.cp(\"dbfs:/tmp/dic_freq_moves_onlyInitial.txt\", \"/FileStore/dic_freq_moves_onlyInitial.txt\")\n#community.cloud.databricks.com/files/dic_freq_moves_onlyInitial.txt?o=5604700270830977"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bbd6d03b-9b31-4fd7-9eb4-f3b39a2f2450"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.cp(\"dbfs:/tmp/dic_freq_moves_onlyInitial.txt\", \"file:/tmp/dic_freq_moves_onlyInitial.txt\")\nwith open('/tmp/dic_freq_moves_onlyInitial.txt') as json_file:\n    app = json.load(json_file)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d95520ee-51a2-49e9-a191-25cbcc5cf381"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["dic_f={}\nfor key, value in app.items():\n  dic_f[int(key)]=int(value)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c9770c7d-805a-45b4-bd81-fb8a3189079a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Encoding x and y with the new encoding"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e39e81c5-7085-41fb-8349-23f42ecdfb92"}}},{"cell_type":"code","source":["out=len(int_moves)\nfor i in range(0,len(x)):\n  s=[]\n  for e in x[i]:\n    if int(e) in dic_f:\n      s.append(dic_f[e])\n    else:\n      s.append(dic_f[out])\n  x[i]=s"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a1ddf969-0810-44fc-bb87-6bf0ddd98763"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.cp(\"dbfs:/tmp/Y_slide_windows_10-75_5steps_games.txt\", \"file:/tmp/Y_slide_windows_10-75_5steps_games.txt\")\nout=len(int_moves)\ny=[]\nc=0\nfor line in open(\"/tmp/Y_slide_windows_10-75_5steps_games.txt\", \"r\"):\n  stripped_line = line.strip()\n  s_l=int(stripped_line)\n  if s_l in dic_f:\n    y.append(dic_f[s_l])\n  else:\n    y.append(dic_f[out])\n  if c==(matches*steps_per_match)-1:\n    break\n  c+=1\ny = np.array(y, dtype=int)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"675fedfd-897b-4bd4-8e16-fe7bc010e112"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["In the following cells, the train and test set are created. 20% of the matches are used for the test set."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"25dbc90a-2430-4c58-929a-aff466d02cbb"}}},{"cell_type":"code","source":["print(\"TRAIN X\")\ntrain_freq_x= x[:int(len(x)*0.8)]\nprint(\"The number of elements in train_x is\",len(train_freq_x), \"(80% of the dataset)\")\n\nprint(\"TRAIN Y\")\ntrain_freq_y = y[:int(len(y)*0.8)]\nprint(\"The number of elements in train_y is\",len(train_freq_y), \"(80% of the dataset)\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8bcdc082-e9d1-413b-89c2-3be8f13d4b80"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"TRAIN X\nThe number of elements in train_x is 840000 (80% of the dataset)\nTRAIN Y\nThe number of elements in train_y is 840000 (80% of the dataset)\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["TRAIN X\nThe number of elements in train_x is 840000 (80% of the dataset)\nTRAIN Y\nThe number of elements in train_y is 840000 (80% of the dataset)\n"]}}],"execution_count":0},{"cell_type":"code","source":["print(\"TEST X\")\ntest_freq_x = x[-int(len(x)*0.2):] \nprint(\"The number of elements in test_x is\",len(test_freq_x), \"(20% of the dataset)\")\n\nprint(\"TEST Y\")\ntest_freq_y= y[-int(len(y)*0.2):] \nprint(\"The number of elements in test_y is\",len(test_freq_y), \"(20% of the dataset)\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"462bea03-fade-494c-9c6d-c1fe69822237"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"TEST X\nThe number of elements in test_x is 210000 (20% of the dataset)\nTEST Y\nThe number of elements in test_y is 210000 (20% of the dataset)\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["TEST X\nThe number of elements in test_x is 210000 (20% of the dataset)\nTEST Y\nThe number of elements in test_y is 210000 (20% of the dataset)\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["In this cell we see how the data are structured"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"584a48a9-1cfa-4294-9747-fbf9f35cf68d"}}},{"cell_type":"code","source":["for i in range(0,len(test_freq_x[:70])):\n  print(test_freq_x[i], \"  \" , test_freq_y[i])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ee582cbf-868c-48d5-8036-47e75552c3dd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"[337 218 438 322 244]    464\n[218 438 322 244 464]    420\n[438 322 244 464 420]    219\n[322 244 464 420 219]    159\n[244 464 420 219 159]    167\n[464 420 219 159 167]    17\n[420 219 159 167  17]    225\n[219 159 167  17 225]    270\n[159 167  17 225 270]    33\n[167  17 225 270  33]    261\n[ 17 225 270  33 261]    377\n[225 270  33 261 377]    485\n[270  33 261 377 485]    182\n[ 33 261 377 485 182]    21\n[261 377 485 182  21]    303\n[377 485 182  21 303]    441\n[485 182  21 303 441]    265\n[182  21 303 441 265]    138\n[ 21 303 441 265 138]    478\n[303 441 265 138 478]    190\n[441 265 138 478 190]    286\n[265 138 478 190 286]    98\n[138 478 190 286  98]    325\n[478 190 286  98 325]    110\n[190 286  98 325 110]    337\n[286  98 325 110 337]    96\n[ 98 325 110 337  96]    218\n[325 110 337  96 218]    280\n[110 337  96 218 280]    408\n[337  96 218 280 408]    447\n[ 96 218 280 408 447]    247\n[218 280 408 447 247]    301\n[280 408 447 247 301]    338\n[408 447 247 301 338]    522\n[447 247 301 338 522]    522\n[247 301 338 522 522]    522\n[301 338 522 522 522]    522\n[338 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["[337 218 438 322 244]    464\n[218 438 322 244 464]    420\n[438 322 244 464 420]    219\n[322 244 464 420 219]    159\n[244 464 420 219 159]    167\n[464 420 219 159 167]    17\n[420 219 159 167  17]    225\n[219 159 167  17 225]    270\n[159 167  17 225 270]    33\n[167  17 225 270  33]    261\n[ 17 225 270  33 261]    377\n[225 270  33 261 377]    485\n[270  33 261 377 485]    182\n[ 33 261 377 485 182]    21\n[261 377 485 182  21]    303\n[377 485 182  21 303]    441\n[485 182  21 303 441]    265\n[182  21 303 441 265]    138\n[ 21 303 441 265 138]    478\n[303 441 265 138 478]    190\n[441 265 138 478 190]    286\n[265 138 478 190 286]    98\n[138 478 190 286  98]    325\n[478 190 286  98 325]    110\n[190 286  98 325 110]    337\n[286  98 325 110 337]    96\n[ 98 325 110 337  96]    218\n[325 110 337  96 218]    280\n[110 337  96 218 280]    408\n[337  96 218 280 408]    447\n[ 96 218 280 408 447]    247\n[218 280 408 447 247]    301\n[280 408 447 247 301]    338\n[408 447 247 301 338]    522\n[447 247 301 338 522]    522\n[247 301 338 522 522]    522\n[301 338 522 522 522]    522\n[338 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n[522 522 522 522 522]    522\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["In the following cell, One Hot Encoding is applied on the labels of the train set."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fb46f1d0-96a8-4716-810c-38e9f7900440"}}},{"cell_type":"code","source":["train_freq_y = to_categorical(train_freq_y)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aa16f2df-85bc-4e65-8145-c7a1b09078f7"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["In the following cell we define values that will be used for embedding layer of the model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7223c560-57b6-4553-91bc-bef7b4e2f9f4"}}},{"cell_type":"code","source":["seq_len=train_freq_x.shape[1]\nvocab_size=len(dic_f)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"37e348e4-7cd0-4f17-b329-dae2698e5d91"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Defining a callback to prevent overfitting"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c7c52b0b-64a2-4cb8-a219-b39ed4d95ce2"}}},{"cell_type":"code","source":["from tensorflow.keras.callbacks import EarlyStopping\ncallback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a5371fb0-44b2-4162-8578-9130b4f47768"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["In the next cell we have the model used for the train. The train of this model was performed on kaggle. For this model we use the same Neural Network of the first model."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b8332b5f-70da-41ae-b5c5-63066affb839"}}},{"cell_type":"code","source":["model = Sequential()\nmodel.add(Embedding(vocab_size, n_steps, input_length=seq_len))\nmodel.add(LSTM(100, return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(100))\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dense(vocab_size, activation='softmax'))\nprint(model.summary())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4e58356d-c2cb-403f-baa9-33c746fb7c50"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Model: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n embedding_1 (Embedding)     (None, 5, 5)              580       \n                                                                 \n lstm_2 (LSTM)               (None, 5, 100)            42400     \n                                                                 \n dropout_1 (Dropout)         (None, 5, 100)            0         \n                                                                 \n lstm_3 (LSTM)               (None, 100)               80400     \n                                                                 \n dense_2 (Dense)             (None, 100)               10100     \n                                                                 \n dense_3 (Dense)             (None, 116)               11716     \n                                                                 \n=================================================================\nTotal params: 145,196\nTrainable params: 145,196\nNon-trainable params: 0\n_________________________________________________________________\nNone\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Model: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n embedding_1 (Embedding)     (None, 5, 5)              580       \n                                                                 \n lstm_2 (LSTM)               (None, 5, 100)            42400     \n                                                                 \n dropout_1 (Dropout)         (None, 5, 100)            0         \n                                                                 \n lstm_3 (LSTM)               (None, 100)               80400     \n                                                                 \n dense_2 (Dense)             (None, 100)               10100     \n                                                                 \n dense_3 (Dense)             (None, 116)               11716     \n                                                                 \n=================================================================\nTotal params: 145,196\nTrainable params: 145,196\nNon-trainable params: 0\n_________________________________________________________________\nNone\n"]}}],"execution_count":0},{"cell_type":"code","source":["# compile model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n# fit model\nhistory=model.fit(train_freq_x, train_freq_y, batch_size=64, epochs=50, callbacks=[callback])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6796f621-9cf3-4b68-b3ec-c7e11ff529d3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Epoch 1/50\n\r    1/13125 [..............................] - ETA: 19:14:01 - loss: 4.7534 - accuracy: 0.0000e+00\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    2/13125 [..............................] - ETA: 15:10 - loss: 4.7512 - accuracy: 0.2422       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    5/13125 [..............................] - ETA: 7:26 - loss: 4.7443 - accuracy: 0.2688 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    8/13125 [..............................] - ETA: 6:04 - loss: 4.7355 - accuracy: 0.2734\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   11/13125 [..............................] - ETA: 5:34 - loss: 4.7231 - accuracy: 0.2699\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   14/13125 [..............................] - ETA: 5:15 - loss: 4.7044 - accuracy: 0.2746\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   17/13125 [..............................] - ETA: 5:04 - loss: 4.6811 - accuracy: 0.2711\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   20/13125 [..............................] - ETA: 4:58 - loss: 4.6381 - accuracy: 0.2695\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   23/13125 [..............................] - ETA: 4:53 - loss: 4.5635 - accuracy: 0.2738\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   25/13125 [..............................] - ETA: 5:04 - loss: 4.4974 - accuracy: 0.2756\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   28/13125 [..............................] - ETA: 5:02 - loss: 4.3678 - accuracy: 0.2773\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   31/13125 [..............................] - ETA: 4:57 - loss: 4.2861 - accuracy: 0.2792\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   34/13125 [..............................] - ETA: 4:54 - loss: 4.2126 - accuracy: 0.2794\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   37/13125 [..............................] - ETA: 4:52 - loss: 4.1395 - accuracy: 0.2796\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   40/13125 [..............................] - ETA: 4:50 - loss: 4.0846 - accuracy: 0.2914","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Epoch 1/50\n\r    1/13125 [..............................] - ETA: 19:14:01 - loss: 4.7534 - accuracy: 0.0000e+00\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    2/13125 [..............................] - ETA: 15:10 - loss: 4.7512 - accuracy: 0.2422       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    5/13125 [..............................] - ETA: 7:26 - loss: 4.7443 - accuracy: 0.2688 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    8/13125 [..............................] - ETA: 6:04 - loss: 4.7355 - accuracy: 0.2734\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   11/13125 [..............................] - ETA: 5:34 - loss: 4.7231 - accuracy: 0.2699\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   14/13125 [..............................] - ETA: 5:15 - loss: 4.7044 - accuracy: 0.2746\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   17/13125 [..............................] - ETA: 5:04 - loss: 4.6811 - accuracy: 0.2711\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   20/13125 [..............................] - ETA: 4:58 - loss: 4.6381 - accuracy: 0.2695\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   23/13125 [..............................] - ETA: 4:53 - loss: 4.5635 - accuracy: 0.2738\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   25/13125 [..............................] - ETA: 5:04 - loss: 4.4974 - accuracy: 0.2756\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   28/13125 [..............................] - ETA: 5:02 - loss: 4.3678 - accuracy: 0.2773\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   31/13125 [..............................] - ETA: 4:57 - loss: 4.2861 - accuracy: 0.2792\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   34/13125 [..............................] - ETA: 4:54 - loss: 4.2126 - accuracy: 0.2794\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   37/13125 [..............................] - ETA: 4:52 - loss: 4.1395 - accuracy: 0.2796\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   40/13125 [..............................] - ETA: 4:50 - loss: 4.0846 - accuracy: 0.2914"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Cancelled","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Loading the trained model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d3949c86-8533-4e3c-8a73-fcb844909c0c"}}},{"cell_type":"code","source":["%sh wget https://www.dropbox.com/s/8zz9fkbcb3saazc/OnlyInitialCell_FREQ_Kaggle_model.h5?dl=1 -O /tmp/Kaggle_Freq_100Units_onlyInitialCell.csv"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a043b0fb-3fba-49ea-8140-1ab6d9a04fd6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"--2022-06-04 11:56:43--  https://www.dropbox.com/s/8zz9fkbcb3saazc/OnlyInitialCell_FREQ_Kaggle_model.h5?dl=1\nResolving www.dropbox.com (www.dropbox.com)... 162.125.8.18, 2620:100:6016:18::a27d:112\nConnecting to www.dropbox.com (www.dropbox.com)|162.125.8.18|:443... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: /s/dl/8zz9fkbcb3saazc/OnlyInitialCell_FREQ_Kaggle_model.h5 [following]\n--2022-06-04 11:56:43--  https://www.dropbox.com/s/dl/8zz9fkbcb3saazc/OnlyInitialCell_FREQ_Kaggle_model.h5\nReusing existing connection to www.dropbox.com:443.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://uc29fc4b2713ef37ee0e8311fb5d.dl.dropboxusercontent.com/cd/0/get/Bmh8iOFB1mBTdV1O0Ul7mcgGMc4VuAThpcxisTbIZBvJwUn5_z5us_LGTGwCwk9KrpCe98ZjdpwsagbzLiNofMVfwDbtLiCfh3pFYc3hETIPZ8S2iAs0yy2CEEuv-xox6tREZooLyhk2xRDpFuSiDPDtY_p74lYMSnyNNQ-KyjPJzyJm31xjTdPniaM6fZ4SKAA/file?dl=1# [following]\n--2022-06-04 11:56:44--  https://uc29fc4b2713ef37ee0e8311fb5d.dl.dropboxusercontent.com/cd/0/get/Bmh8iOFB1mBTdV1O0Ul7mcgGMc4VuAThpcxisTbIZBvJwUn5_z5us_LGTGwCwk9KrpCe98ZjdpwsagbzLiNofMVfwDbtLiCfh3pFYc3hETIPZ8S2iAs0yy2CEEuv-xox6tREZooLyhk2xRDpFuSiDPDtY_p74lYMSnyNNQ-KyjPJzyJm31xjTdPniaM6fZ4SKAA/file?dl=1\nResolving uc29fc4b2713ef37ee0e8311fb5d.dl.dropboxusercontent.com (uc29fc4b2713ef37ee0e8311fb5d.dl.dropboxusercontent.com)... 162.125.2.15, 2620:100:6016:15::a27d:10f\nConnecting to uc29fc4b2713ef37ee0e8311fb5d.dl.dropboxusercontent.com (uc29fc4b2713ef37ee0e8311fb5d.dl.dropboxusercontent.com)|162.125.2.15|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1801136 (1.7M) [application/binary]\nSaving to: /tmp/Kaggle_Freq_100Units_onlyInitialCell.csv\n\n     0K .......... .......... .......... .......... ..........  2%  914K 2s\n    50K .......... .......... .......... .......... ..........  5% 1.79M 1s\n   100K .......... .......... .......... .......... ..........  8% 2.20M 1s\n   150K .......... .......... .......... .......... .......... 11% 2.80M 1s\n   200K .......... .......... .......... .......... .......... 14% 5.55M 1s\n   250K .......... .......... .......... .......... .......... 17% 5.59M 1s\n   300K .......... .......... .......... .......... .......... 19% 4.93M 1s\n   350K .......... .......... .......... .......... .......... 22% 7.22M 1s\n   400K .......... .......... .......... .......... .......... 25% 6.33M 0s\n   450K .......... .......... .......... .......... .......... 28% 12.5M 0s\n   500K .......... .......... .......... .......... .......... 31% 11.8M 0s\n   550K .......... .......... .......... .......... .......... 34% 12.5M 0s\n   600K .......... .......... .......... .......... .......... 36% 14.5M 0s\n   650K .......... .......... .......... .......... .......... 39% 17.5M 0s\n   700K .......... .......... .......... .......... .......... 42% 15.2M 0s\n   750K .......... .......... .......... .......... .......... 45% 15.2M 0s\n   800K .......... .......... .......... .......... .......... 48% 13.1M 0s\n   850K .......... .......... .......... .......... .......... 51% 18.2M 0s\n   900K .......... .......... .......... .......... .......... 54% 24.9M 0s\n   950K .......... .......... .......... .......... .......... 56% 20.0M 0s\n  1000K .......... .......... .......... .......... .......... 59% 21.9M 0s\n  1050K .......... .......... .......... .......... .......... 62% 18.1M 0s\n  1100K .......... .......... .......... .......... .......... 65% 25.9M 0s\n  1150K .......... .......... .......... .......... .......... 68% 29.3M 0s\n  1200K .......... .......... .......... .......... .......... 71% 20.4M 0s\n  1250K .......... .......... .......... .......... .......... 73% 36.0M 0s\n  1300K .......... .......... .......... .......... .......... 76% 25.1M 0s\n  1350K .......... .......... .......... .......... .......... 79% 39.4M 0s\n  1400K .......... .......... .......... .......... .......... 82% 28.5M 0s\n  1450K .......... .......... .......... .......... .......... 85% 44.0M 0s\n  1500K .......... .......... .......... .......... .......... 88% 24.4M 0s\n  1550K .......... .......... .......... .......... .......... 90% 49.7M 0s\n  1600K .......... .......... .......... .......... .......... 93% 29.5M 0s\n  1650K .......... .......... .......... .......... .......... 96% 35.1M 0s\n  1700K .......... .......... .......... .......... .......... 99% 61.6M 0s\n  1750K ........                                              100%  159M=0.2s\n\n2022-06-04 11:56:44 (7.67 MB/s) - /tmp/Kaggle_Freq_100Units_onlyInitialCell.csv saved [1801136/1801136]\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["--2022-06-04 11:56:43--  https://www.dropbox.com/s/8zz9fkbcb3saazc/OnlyInitialCell_FREQ_Kaggle_model.h5?dl=1\nResolving www.dropbox.com (www.dropbox.com)... 162.125.8.18, 2620:100:6016:18::a27d:112\nConnecting to www.dropbox.com (www.dropbox.com)|162.125.8.18|:443... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: /s/dl/8zz9fkbcb3saazc/OnlyInitialCell_FREQ_Kaggle_model.h5 [following]\n--2022-06-04 11:56:43--  https://www.dropbox.com/s/dl/8zz9fkbcb3saazc/OnlyInitialCell_FREQ_Kaggle_model.h5\nReusing existing connection to www.dropbox.com:443.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://uc29fc4b2713ef37ee0e8311fb5d.dl.dropboxusercontent.com/cd/0/get/Bmh8iOFB1mBTdV1O0Ul7mcgGMc4VuAThpcxisTbIZBvJwUn5_z5us_LGTGwCwk9KrpCe98ZjdpwsagbzLiNofMVfwDbtLiCfh3pFYc3hETIPZ8S2iAs0yy2CEEuv-xox6tREZooLyhk2xRDpFuSiDPDtY_p74lYMSnyNNQ-KyjPJzyJm31xjTdPniaM6fZ4SKAA/file?dl=1# [following]\n--2022-06-04 11:56:44--  https://uc29fc4b2713ef37ee0e8311fb5d.dl.dropboxusercontent.com/cd/0/get/Bmh8iOFB1mBTdV1O0Ul7mcgGMc4VuAThpcxisTbIZBvJwUn5_z5us_LGTGwCwk9KrpCe98ZjdpwsagbzLiNofMVfwDbtLiCfh3pFYc3hETIPZ8S2iAs0yy2CEEuv-xox6tREZooLyhk2xRDpFuSiDPDtY_p74lYMSnyNNQ-KyjPJzyJm31xjTdPniaM6fZ4SKAA/file?dl=1\nResolving uc29fc4b2713ef37ee0e8311fb5d.dl.dropboxusercontent.com (uc29fc4b2713ef37ee0e8311fb5d.dl.dropboxusercontent.com)... 162.125.2.15, 2620:100:6016:15::a27d:10f\nConnecting to uc29fc4b2713ef37ee0e8311fb5d.dl.dropboxusercontent.com (uc29fc4b2713ef37ee0e8311fb5d.dl.dropboxusercontent.com)|162.125.2.15|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1801136 (1.7M) [application/binary]\nSaving to: /tmp/Kaggle_Freq_100Units_onlyInitialCell.csv\n\n     0K .......... .......... .......... .......... ..........  2%  914K 2s\n    50K .......... .......... .......... .......... ..........  5% 1.79M 1s\n   100K .......... .......... .......... .......... ..........  8% 2.20M 1s\n   150K .......... .......... .......... .......... .......... 11% 2.80M 1s\n   200K .......... .......... .......... .......... .......... 14% 5.55M 1s\n   250K .......... .......... .......... .......... .......... 17% 5.59M 1s\n   300K .......... .......... .......... .......... .......... 19% 4.93M 1s\n   350K .......... .......... .......... .......... .......... 22% 7.22M 1s\n   400K .......... .......... .......... .......... .......... 25% 6.33M 0s\n   450K .......... .......... .......... .......... .......... 28% 12.5M 0s\n   500K .......... .......... .......... .......... .......... 31% 11.8M 0s\n   550K .......... .......... .......... .......... .......... 34% 12.5M 0s\n   600K .......... .......... .......... .......... .......... 36% 14.5M 0s\n   650K .......... .......... .......... .......... .......... 39% 17.5M 0s\n   700K .......... .......... .......... .......... .......... 42% 15.2M 0s\n   750K .......... .......... .......... .......... .......... 45% 15.2M 0s\n   800K .......... .......... .......... .......... .......... 48% 13.1M 0s\n   850K .......... .......... .......... .......... .......... 51% 18.2M 0s\n   900K .......... .......... .......... .......... .......... 54% 24.9M 0s\n   950K .......... .......... .......... .......... .......... 56% 20.0M 0s\n  1000K .......... .......... .......... .......... .......... 59% 21.9M 0s\n  1050K .......... .......... .......... .......... .......... 62% 18.1M 0s\n  1100K .......... .......... .......... .......... .......... 65% 25.9M 0s\n  1150K .......... .......... .......... .......... .......... 68% 29.3M 0s\n  1200K .......... .......... .......... .......... .......... 71% 20.4M 0s\n  1250K .......... .......... .......... .......... .......... 73% 36.0M 0s\n  1300K .......... .......... .......... .......... .......... 76% 25.1M 0s\n  1350K .......... .......... .......... .......... .......... 79% 39.4M 0s\n  1400K .......... .......... .......... .......... .......... 82% 28.5M 0s\n  1450K .......... .......... .......... .......... .......... 85% 44.0M 0s\n  1500K .......... .......... .......... .......... .......... 88% 24.4M 0s\n  1550K .......... .......... .......... .......... .......... 90% 49.7M 0s\n  1600K .......... .......... .......... .......... .......... 93% 29.5M 0s\n  1650K .......... .......... .......... .......... .......... 96% 35.1M 0s\n  1700K .......... .......... .......... .......... .......... 99% 61.6M 0s\n  1750K ........                                              100%  159M=0.2s\n\n2022-06-04 11:56:44 (7.67 MB/s) - /tmp/Kaggle_Freq_100Units_onlyInitialCell.csv saved [1801136/1801136]\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["model4_Initialmoves = keras.models.load_model(\"/tmp/Kaggle_Freq_100Units_onlyInitialCell.csv\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2b14fbea-3296-4499-a81d-a50e7fea240f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Plotting of the loss and accuracy values during the train"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2ae1201b-d68a-4eaa-9820-2919392b4e1a"}}},{"cell_type":"code","source":["plt.plot(history.history['loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train'], loc='upper right')\nplt.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"03f6fd0a-2528-436d-bec3-e3a57a3acbdc"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["<img src=\"https://www.dropbox.com/s/a74f9mko2es7utr/Model6_loss.png?dl=1\">"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9c39067f-7d1f-4d81-a3ea-734eaec46ec8"}}},{"cell_type":"code","source":["plt.plot(history.history['accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train'], loc='upper left')\nplt.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2069a48a-600d-4f47-a216-89e45a3464dd"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["<img src=\"https://www.dropbox.com/s/83f7rlc3fw2hf4k/Model6_accuracy.png?dl=1\">"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2d349db7-67c3-40b7-962d-65aaf9cf9368"}}},{"cell_type":"markdown","source":["### Evaluation of Second approach models\nIn this section the evaluation of models trained with this approach is performed. For more information on the evaluation conducted the reader is invited to read the section \"Evaluation Backgorund\" in the Introduction."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2f5ed6a7-896f-4b29-ae49-20d7fae6aa31"}}},{"cell_type":"markdown","source":["In the following cell, we check how many times the model exactly predicts the moves of the matches in the test set. We also calculate the perplexity of the models for each match."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"df558f53-28e0-4ca0-92af-009090139932"}}},{"cell_type":"code","source":["#How many times do the models predict correctly\nm1=0\nm2=0\nm3=0\nm4=0\n#Multiplication of probabilities for final calculation of perplexity\np_m1=1\np_m2=1\np_m3=1\np_m4=1\n#List for saving perplexities divided by match \np_model1=[]\np_model2=[]\np_model3=[]\np_model4=[]\n\ngam_count=0\nfor i in range(0,len(test_x[:35000])):\n  el = np.asarray(test_x[i])\n  el = np.reshape(el, (1, len(el), 1))\n  \n  el_freq = np.asarray(test_freq_x[i])\n  el_freq = np.reshape(el_freq, (1, len(el_freq), 1))\n  \n  #MODEL 1 \n  prediction = model1_Initialmoves.predict(el, verbose=0)\n  index = np.argmax(prediction)\n  p_m1=p_m1 * prediction[0][index]\n\n  #MODEL 2\n  prediction2 = model2_Initialmoves.predict(el, verbose=0)\n  index2 = np.argmax(prediction2)\n  p_m2=p_m2 * prediction2[0][index2]\n  \n  #MODEL 3\n  prediction3 = model3_Initialmoves.predict(el, verbose=0)\n  index3 = np.argmax(prediction3)\n  p_m3=p_m3 * prediction3[0][index3]\n  \n  #MODEL 4\n  prediction4 = model4_Initialmoves.predict(el_freq, verbose=0)\n  index4 = np.argmax(prediction4)\n  p_m4=p_m4 * prediction4[0][index4]\n  \n  if gam_count==69:\n    p_model1.append(p_m1**(-(1/70)))\n    p_model2.append(p_m2**(-(1/70)))\n    p_model3.append(p_m3**(-(1/70)))\n    p_model4.append(p_m4**(-(1/70)))\n    p_m1=1\n    p_m2=1\n    p_m3=1\n    p_m4=1\n    gam_count=0\n\n  if index==test_y[i]:\n    m1+=1\n  if index2==test_y[i]:\n    m2+=1\n  if index3==test_y[i]:\n    m3+=1\n  if index4==test_freq_y[i]:\n    m4+=1\n  if i%100==0:\n    print(i)\n    \n  gam_count+=1\n  \nprint(\"-----------------------\")\nprint(\"model1 -> \", m1)\nprint(\"model2 -> \", m2)\nprint(\"model3 -> \", m3)\nprint(\"model4 (frequencies) -> \", m4)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7f49e494-cf40-4db1-8a47-18a9ad4d2e8e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"0\n100\n200\n300\n400\n500\n600\n700\n800\n900\n1000\n1100\n1200\n1300\n1400\n1500\n1600\n1700\n1800\n1900\n2000\n2100\n2200\n2300\n2400\n2500\n2600\n2700\n2800\n2900\n3000\n3100\n3200\n3300\n3400\n3500\n3600\n3700\n3800\n3900\n4000\n4100\n4200\n4300\n4400\n4500\n4600\n4700\n4800\n4900\n5000\n5100\n5200\n5300\n5400\n5500\n5600\n5700\n5800\n5900\n6000\n6100\n6200\n6300\n6400\n6500\n6600\n6700\n6800\n6900\n7000\n7100\n7200\n7300\n7400\n7500\n7600\n7700\n7800\n7900\n8000\n8100\n8200\n8300\n8400\n8500\n8600\n8700\n8800\n8900\n9000\n9100\n9200\n9300\n9400\n9500\n9600\n9700\n9800\n9900\n10000\n10100\n10200\n10300\n10400\n10500\n10600\n10700\n10800\n10900\n11000\n11100\n11200\n11300\n11400\n11500\n11600\n11700\n11800\n11900\n12000\n12100\n12200\n12300\n12400\n12500\n12600\n12700\n12800\n12900\n13000\n13100\n13200\n13300\n13400\n13500\n13600\n13700\n13800\n13900\n14000\n14100\n14200\n14300\n14400\n14500\n14600\n14700\n14800\n14900\n15000\n15100\n15200\n15300\n15400\n15500\n15600\n15700\n15800\n15900\n16000\n16100\n16200\n16300\n16400\n16500\n16600\n16700\n16800\n16900\n17000\n17100\n17200\n17300\n17400\n17500\n17600\n17700\n17800\n17900\n18000\n18100\n18200\n18300\n18400\n18500\n18600\n18700\n18800\n18900\n19000\n19100\n19200\n19300\n19400\n19500\n19600\n19700\n19800\n19900\n20000\n20100\n20200\n20300\n20400\n20500\n20600\n20700\n20800\n20900\n21000\n21100\n21200\n21300\n21400\n21500\n21600\n21700\n21800\n21900\n22000\n22100\n22200\n22300\n22400\n22500\n22600\n22700\n22800\n22900\n23000\n23100\n23200\n23300\n23400\n23500\n23600\n23700\n23800\n23900\n24000\n24100\n24200\n24300\n24400\n24500\n24600\n24700\n24800\n24900\n25000\n25100\n25200\n25300\n25400\n25500\n25600\n25700\n25800\n25900\n26000\n26100\n26200\n26300\n26400\n26500\n26600\n26700\n26800\n26900\n27000\n27100\n27200\n27300\n27400\n27500\n27600\n27700\n27800\n27900\n28000\n28100\n28200\n28300\n28400\n28500\n28600\n28700\n28800\n28900\n29000\n29100\n29200\n29300\n29400\n29500\n29600\n29700\n29800\n29900\n30000\n30100\n30200\n30300\n30400\n30500\n30600\n30700\n30800\n30900\n31000\n31100\n31200\n31300\n31400\n31500\n31600\n31700\n31800\n31900\n32000\n32100\n32200\n32300\n32400\n32500\n32600\n32700\n32800\n32900\n33000\n33100\n33200\n33300\n33400\n33500\n33600\n33700\n33800\n33900\n34000\n34100\n34200\n34300\n34400\n34500\n34600\n34700\n34800\n34900\n-----------------------\nmodel1 ->  15676\nmodel2 ->  16033\nmodel3 ->  16000\nmodel4 (frequencies) ->  20974\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["0\n100\n200\n300\n400\n500\n600\n700\n800\n900\n1000\n1100\n1200\n1300\n1400\n1500\n1600\n1700\n1800\n1900\n2000\n2100\n2200\n2300\n2400\n2500\n2600\n2700\n2800\n2900\n3000\n3100\n3200\n3300\n3400\n3500\n3600\n3700\n3800\n3900\n4000\n4100\n4200\n4300\n4400\n4500\n4600\n4700\n4800\n4900\n5000\n5100\n5200\n5300\n5400\n5500\n5600\n5700\n5800\n5900\n6000\n6100\n6200\n6300\n6400\n6500\n6600\n6700\n6800\n6900\n7000\n7100\n7200\n7300\n7400\n7500\n7600\n7700\n7800\n7900\n8000\n8100\n8200\n8300\n8400\n8500\n8600\n8700\n8800\n8900\n9000\n9100\n9200\n9300\n9400\n9500\n9600\n9700\n9800\n9900\n10000\n10100\n10200\n10300\n10400\n10500\n10600\n10700\n10800\n10900\n11000\n11100\n11200\n11300\n11400\n11500\n11600\n11700\n11800\n11900\n12000\n12100\n12200\n12300\n12400\n12500\n12600\n12700\n12800\n12900\n13000\n13100\n13200\n13300\n13400\n13500\n13600\n13700\n13800\n13900\n14000\n14100\n14200\n14300\n14400\n14500\n14600\n14700\n14800\n14900\n15000\n15100\n15200\n15300\n15400\n15500\n15600\n15700\n15800\n15900\n16000\n16100\n16200\n16300\n16400\n16500\n16600\n16700\n16800\n16900\n17000\n17100\n17200\n17300\n17400\n17500\n17600\n17700\n17800\n17900\n18000\n18100\n18200\n18300\n18400\n18500\n18600\n18700\n18800\n18900\n19000\n19100\n19200\n19300\n19400\n19500\n19600\n19700\n19800\n19900\n20000\n20100\n20200\n20300\n20400\n20500\n20600\n20700\n20800\n20900\n21000\n21100\n21200\n21300\n21400\n21500\n21600\n21700\n21800\n21900\n22000\n22100\n22200\n22300\n22400\n22500\n22600\n22700\n22800\n22900\n23000\n23100\n23200\n23300\n23400\n23500\n23600\n23700\n23800\n23900\n24000\n24100\n24200\n24300\n24400\n24500\n24600\n24700\n24800\n24900\n25000\n25100\n25200\n25300\n25400\n25500\n25600\n25700\n25800\n25900\n26000\n26100\n26200\n26300\n26400\n26500\n26600\n26700\n26800\n26900\n27000\n27100\n27200\n27300\n27400\n27500\n27600\n27700\n27800\n27900\n28000\n28100\n28200\n28300\n28400\n28500\n28600\n28700\n28800\n28900\n29000\n29100\n29200\n29300\n29400\n29500\n29600\n29700\n29800\n29900\n30000\n30100\n30200\n30300\n30400\n30500\n30600\n30700\n30800\n30900\n31000\n31100\n31200\n31300\n31400\n31500\n31600\n31700\n31800\n31900\n32000\n32100\n32200\n32300\n32400\n32500\n32600\n32700\n32800\n32900\n33000\n33100\n33200\n33300\n33400\n33500\n33600\n33700\n33800\n33900\n34000\n34100\n34200\n34300\n34400\n34500\n34600\n34700\n34800\n34900\n-----------------------\nmodel1 ->  15676\nmodel2 ->  16033\nmodel3 ->  16000\nmodel4 (frequencies) ->  20974\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["Once we have calculated the model perplexity for each match we calculate an average of these perplexities to compare the models"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f091afc7-ed13-4297-be7a-09c778629cac"}}},{"cell_type":"code","source":["s_m1=0\ns_m2=0\ns_m3=0\ns_m4=0\nfor i in range(0,len(p_model1)):\n  s_m1+=p_model1[i]\n  s_m2+=p_model2[i]\n  s_m3+=p_model3[i]\n  s_m4+=p_model4[i]\n  \navg_m1=s_m1/len(p_model1)\navg_m2=s_m2/len(p_model2)\navg_m3=s_m3/len(p_model3)\navg_m4=s_m4/len(p_model4)\n\nprint(\"The average perplexity of model 1 is\", avg_m1)\nprint(\"The average perplexity of model 2 is\", avg_m2)\nprint(\"The average perplexity of model 3 is\", avg_m3)\nprint(\"The average perplexity of model 4 (frequencies) is\", avg_m4)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cd8182f3-7c1f-4de3-9a98-c1218d41c710"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"The average perplexity of model 1 is 5.480566942177195\nThe average perplexity of model 2 is 4.225299987699861\nThe average perplexity of model 3 is 3.971523752831022\nThe average perplexity of model 4 (frequencies) is 1.9508132250131074\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["The average perplexity of model 1 is 5.480566942177195\nThe average perplexity of model 2 is 4.225299987699861\nThe average perplexity of model 3 is 3.971523752831022\nThe average perplexity of model 4 (frequencies) is 1.9508132250131074\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Discussion of evaluation results\nThe created models were evaluated in the same way. In the extrinsic evaluation out of 35000 predictions the The trained models without frequencies gave very similar values predicting correctly from 15676 to 16033 times. Model 1 predicted the fewest times while model 2 had the highest accuracy. The model with the dictionary reduced by the frequencies of the moves predicted 20974 times correctly. The perplexity of these models follows a similar pattern. The model with the reduced dictionary has an average complexity of 1.9 which is lower than the others. The perplexity of the trained models without frequencies, on the other hand, follows a different trand than the correct predictions. The model with the worst perplexity is still model 1, with an average perplexity of 5.4. Then follows model 2 with an average complexity of 4.22 and finally the third model with an average complexity of 3.97."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f429ae43-0834-491c-8cb9-187b0bc6d770"}}},{"cell_type":"markdown","source":["# Section 4: Third Approaches\n\n<p>In the third approach, granularity is increased compared to the second approach. Disambiguities between pieces are also considered in this approach. So the only things that have not been considered are check, checkmate and capture of pieces.The process of arriving at sliding windows is the same the other two approaches but considering the moves differently.</p>\n\n\n<img src=\"https://www.dropbox.com/s/zw12jnad5dqh9we/ThirdApproach.jpg?dl=1\">\n\n\n<p>In this approach, as in the others, before creating the sliding windows the moves must be encoded and transformed to integer. For this example, the initial whole game is not a real game, in fact the written moves are not legal. I chose these moves so that I could show all the examples of how granularity of moves is managed in this approach.</p>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8c88235d-f57d-4868-a8b0-b827bedf8e0f"}}},{"cell_type":"markdown","source":["Function for parse the moves and codify the games. This function take in input a string that identify a move and return the moves without takes (\"x\"), check and checkmate."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ddedf998-b4cf-4c94-b0c7-aef1f3c096c2"}}},{"cell_type":"code","source":["def move_complete(m):\n  if m.startswith('O-O'):\n    return m\n  if m[-1]==\"#\" or m[-1]==\"+\":\n      m=m[:-1]\n  cell=m[-2:]\n  m=m[:-2]\n  if len(m)==2 and m[1]=='x':\n    m=m[:1]\n  if len(m)==3 and m[2]=='x':\n    m=m[:2]\n  return m + cell"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b907d947-633a-4fad-bde0-168f16173b61"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["In the next cell we read the filtered matches and for each move we go to clean the data by removing in move number related to the match and then pass the cleaned move to the move_complete function. <br>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0b977386-6d88-42ae-b0fe-6433c80d256c"}}},{"cell_type":"code","source":["game_man=[]\nconta=0\nfor el in game:\n  lista=[]\n  if el[0]!=None:\n    li=el[0].split(\" \")\n    for y in li:\n      if y != '':\n        z=y.split('.')[1]\n        if z!='':\n          m=move_complete(z)\n          lista.append(m)\n          \n  game_man.append(lista)\n  if conta%100000==0:\n      print(conta)\n  conta+=1"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ea8dca52-9a38-4f43-964f-582ca3ce9eab"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"0\n100000\n200000\n300000\n400000\n500000\n600000\n700000\n800000\n900000\n1000000\n1100000\n1200000\n1300000\n1400000\n1500000\n1600000\n1700000\n1800000\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["0\n100000\n200000\n300000\n400000\n500000\n600000\n700000\n800000\n900000\n1000000\n1100000\n1200000\n1300000\n1400000\n1500000\n1600000\n1700000\n1800000\n"]}}],"execution_count":0},{"cell_type":"code","source":["print(game_man[10])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"346f7b0d-4412-4004-b212-85a94130aa8b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"['d4', 'e6', 'c4', 'b6', 'a3', 'Bb7', 'Nc3', 'f5', 'd5', 'Nf6', 'g3', 'Na6', 'Bg2', 'Nc5', 'Nh3', 'Bd6', 'O-O', 'Be5', 'Qc2', 'O-O', 'Rd1', 'Qe7', 'Be3', 'Rab8', 'Rac1', 'Nce4', 'Ne4', 'Ne4', 'Nf4', 'c5', 'dc6', 'Bc6', 'Nd3', 'Bf6', 'f3', 'Nc5', 'b4', 'Nd3', 'Rd3', 'd5', 'f4', 'dc4', 'Qc4', 'Bg2', 'Kg2', 'Rf7', 'b5', 'Re8', 'Rcd1', 'e5', 'Rd7', 'Qe6', 'Qe6', 'Re6', 'Kf3', 'ef4', 'gf4', 'Rd7', 'Rd7', 'Re7', 'Re7', 'Be7', 'a4', 'Kf7', 'Bd4', 'Bd6', 'e4', 'g6', 'h3', 'Ke6', 'Bc3', 'Bc7', 'Bb4', 'Bd8', 'e5']\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["['d4', 'e6', 'c4', 'b6', 'a3', 'Bb7', 'Nc3', 'f5', 'd5', 'Nf6', 'g3', 'Na6', 'Bg2', 'Nc5', 'Nh3', 'Bd6', 'O-O', 'Be5', 'Qc2', 'O-O', 'Rd1', 'Qe7', 'Be3', 'Rab8', 'Rac1', 'Nce4', 'Ne4', 'Ne4', 'Nf4', 'c5', 'dc6', 'Bc6', 'Nd3', 'Bf6', 'f3', 'Nc5', 'b4', 'Nd3', 'Rd3', 'd5', 'f4', 'dc4', 'Qc4', 'Bg2', 'Kg2', 'Rf7', 'b5', 'Re8', 'Rcd1', 'e5', 'Rd7', 'Qe6', 'Qe6', 'Re6', 'Kf3', 'ef4', 'gf4', 'Rd7', 'Rd7', 'Re7', 'Re7', 'Be7', 'a4', 'Kf7', 'Bd4', 'Bd6', 'e4', 'g6', 'h3', 'Ke6', 'Bc3', 'Bc7', 'Bb4', 'Bd8', 'e5']\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["In the next cell we reduce the number of matches to 15000 and calculate a dictionary with all the moves within these matches, in this dictionary we also save the frequencies of the moves."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"de020b6b-b50c-43b4-9411-b77bb112eb4a"}}},{"cell_type":"code","source":["dic_moves={}\ngame_man=game_man[:15000]\nfor el in game_man:\n  for e in el:\n    if e in dic_moves.keys():\n      dic_moves[e]=dic_moves[e]+1\n    else:\n      dic_moves[e]=1"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a0a2a9e2-da14-4e7d-8d8b-7f38a80bc445"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["print(\"The length of the dic is\",len(dic_moves))\nprint(dic_moves)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"68b90b7d-a10c-4049-a284-7a5450d919d9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"The length of the dic is 1421\n{'d4': 14688, 'd5': 12567, 'c4': 11344, 'e6': 9787, 'Nc3': 14469, 'Nf6': 17290, 'cd5': 4960, 'ed5': 4411, 'Bg5': 6060, 'Be7': 8138, 'e3': 4285, 'Ne4': 7372, 'Bd8': 713, 'Nd1': 372, 'Bc7': 703, 'Nb2': 280, 'Rb1': 2127, 'Nc4': 3021, 'Bc4': 4743, 'dc4': 2788, 'Ne2': 2191, 'O-O': 22913, 'b6': 4537, 'Na6': 1344, 'Bd6': 3695, 'Rd8': 3738, 'Ba3': 967, 'Bb7': 5092, 'e4': 12286, 'f6': 2770, 'Ke2': 728, 'Nc7': 1167, 'Rhd1': 216, 'Ba6': 2059, 'Ke3': 322, 'Kf7': 1440, 'g4': 3205, 'g5': 3036, 'h4': 3668, 'h6': 5242, 'Rh1': 391, 'Re8': 4564, 'f3': 3267, 'hg5': 938, 'fg5': 308, 'd6': 7082, 'Nd5': 7478, 'Bd5': 3037, 'Rh6': 427, 'c3': 3090, 'd7': 179, 'Re6': 1109, 'Rh7': 384, 'Kg8': 1236, 'Rbh1': 7, 'Bc6': 3076, 'Rh8': 487, 'Ra8': 1337, 'Bd7': 4575, 'Qd5': 2321, 'Qa5': 2819, 'Nf3': 15607, 'c6': 5392, 'Ne5': 7403, 'Bf5': 3456, 'Be4': 2885, 'a3': 4386, 'Nbd7': 3776, 'Be3': 6332, 'de5': 2658, 'Ng4': 1817, 'Bd4': 2831, 'b4': 4836, 'Qd8': 1888, 'Ne3': 1587, 'dc6': 721, 'bc6': 1308, 'Rd1': 3857, 'Rab8': 500, 'Rc1': 3610, 'Rfd8': 1175, 'c5': 11165, 'Bc5': 3483, 'bc5': 944, 'g6': 6268, 'Rb2': 707, 'Rd2': 1477, 'e5': 10314, 'Nc6': 10970, 'd3': 1749, 'Bb3': 2230, 'Nbd2': 1736, 'Be6': 4161, 'Qd7': 2917, 'Re1': 4645, 'Rfe8': 1041, 'Nf1': 786, 'Ne7': 2775, 'Ng3': 1648, 'Bg4': 2793, 'h3': 4275, 'Kh8': 2197, 'Bf6': 4453, 'gf6': 1129, 'ed4': 2226, 'cd4': 6175, 'Bb4': 3991, 'Re3': 1046, 'Rg8': 1033, 'Bh3': 904, 'Qd4': 2745, 'Rg6': 475, 'Qb4': 1306, 'Qc3': 1832, 'Bc2': 1461, 'Nh2': 303, 'b5': 6096, 'Rc8': 4212, 'Bd3': 6461, 'Bh5': 822, 'Nh5': 1697, 'Rh5': 434, 'Qf6': 2418, 'Bd1': 334, 'Qf3': 2390, 'Rad1': 1947, 'Nf8': 948, 'Ng6': 1683, 'Qc7': 4838, 'Qe7': 3646, 'h5': 3315, 'gh6': 227, 'Qf4': 1435, 'Qh6': 942, 'f5': 4687, 'ef6': 915, 'Qf7': 979, 'Rg3': 539, 'Rg7': 563, 'Nd4': 7959, 'Bd2': 3892, 'Nc2': 762, 'Bc3': 3078, 'Qg5': 1538, 'Qe2': 3584, 'Qg2': 379, 'O-O-O': 2812, 'Qe4': 1811, 'Rhg1': 152, 'f4': 4966, 'Rgf1': 18, 'Qh4': 1540, 'Be1': 343, 'Qa4': 1755, 'Rf6': 908, 'Qg4': 1617, 'Qa2': 592, 'Bg6': 1439, 'hg6': 950, 'fg6': 503, 'Qg6': 1095, 'Qh5': 1623, 'g3': 4828, 'Bg7': 5172, 'Bg2': 4078, 'Nfd7': 736, 'b3': 3504, 'Qc2': 4235, 'Qd2': 4354, 'a6': 6796, 'Nde2': 127, 'Qg7': 552, 'Nef4': 19, 'Ne6': 2053, 'Rcd8': 155, 'Qf2': 971, 'Bc8': 973, 'ef5': 1185, 'cd6': 427, 'Qb6': 3031, 'Ng8': 266, 'Bb2': 2194, 'Rad8': 1270, 'Qd6': 2310, 'dc5': 2390, 'Qc5': 1782, 'Rfd1': 1428, 'a5': 4549, 'Qd1': 1137, 'Nd7': 4947, 'Kg2': 1489, 'Kf8': 1952, 'Kf3': 317, 'Ke7': 1316, 'Bb5': 3933, 'Kd6': 238, 'Na4': 1481, 'gh4': 164, 'Rh4': 431, 'de4': 1734, 'Re4': 1241, 'Rh2': 200, 'fe6': 1058, 'Qd3': 2645, 'Qf5': 1326, 'Rd5': 1381, 'Re7': 1408, 'Ndb5': 264, 'Na3': 735, 'a4': 4995, 'ab5': 1805, 'Qa8': 526, 'Nce3': 57, 'Qb7': 1410, 'Rc5': 1117, 'Qe5': 1773, 'Qg1': 111, 'Qg3': 1185, 'fg3': 233, 'bc3': 2282, 'Rc3': 997, 'Be2': 4655, 'ef3': 276, 'Bf3': 2789, 'Qb3': 2544, 'Nde7': 23, 'Na5': 1794, 'Nec4': 20, 'Rfe1': 1059, 'Qb2': 980, 'Bb1': 469, 'Kh1': 1657, 'e7': 108, 'Rfb8': 195, 'Nc5': 3581, 'Nh3': 443, 'Be5': 2513, 'Rac1': 1340, 'Nce4': 176, 'Nf4': 2056, 'Nd3': 1566, 'Rd3': 1160, 'Qc4': 1796, 'Rf7': 1180, 'Rcd1': 191, 'Rd7': 1885, 'Qe6': 1505, 'ef4': 820, 'gf4': 574, 'Ke6': 322, 'Bf4': 4152, 'Ba8': 384, 'Qc8': 1268, 'Na8': 157, 'Nd2': 3191, 'Ngf3': 240, 'Bf1': 1176, 'gh5': 312, 'Rg1': 827, 'Rd4': 1419, 'Qc6': 1609, 'Bh6': 1695, 'Red8': 192, 'Nb3': 2183, 'Nb7': 442, 'Qe8': 1185, 'Ng5': 1992, 'Bf8': 1730, 'Qa6': 770, 'Rdd7': 30, 'Ngf6': 422, 'Qa3': 829, 'Ne8': 1055, 'Ba4': 1490, 'Nh6': 644, 'Qc1': 786, 'Bb6': 1052, 'Ba2': 442, 'Nd8': 455, 'Nf5': 2413, 'ed3': 79, 'Nb6': 2751, 'cb6': 242, 'Rhe8': 165, 'Nb8': 597, 'cb4': 442, 'ba6': 226, 'Rfb1': 143, 'Rb7': 1054, 'gf5': 775, 'Kh2': 902, 'Qh7': 415, 'Nge2': 604, 'Kd1': 265, 'Kd2': 622, 'Kc3': 148, 'Rhf1': 172, 'Kb2': 205, 'Rae1': 681, 'Kb1': 1198, 'Rh3': 508, 'Qh2': 225, 'fe4': 988, 'Kh7': 1149, 'Rde1': 191, 'Bg8': 84, 'Kg7': 2042, 'Qe3': 1885, 'Kf6': 432, 'Kd8': 655, 'Qf8': 527, 'Kc7': 367, 'Nb4': 1771, 'N3h4': 3, 'Ra3': 639, 'Bh4': 1664, 'Rac8': 1268, 'fe3': 434, 'Rd6': 1448, 'Ra6': 769, 'Nef6': 58, 'Qb1': 522, 'Ngf8': 5, 'Be8': 569, 'Nc8': 484, 'Re2': 1026, 'f7': 55, 'ed6': 506, 'Kd7': 648, 'Rab1': 465, 'Rbc1': 98, 'Qa7': 682, 'Kc6': 151, 'Qb8': 974, 'Nd6': 2039, 'N5b6': 18, 'Kf1': 1058, 'Rb8': 2756, 'Ba5': 588, 'Rdf1': 77, 'Rhg8': 91, 'Nbd5': 187, 'Bh2': 261, 'Rg2': 305, 'Bf2': 829, 'Bg3': 1266, 'Rc6': 1124, 'Ra1': 968, 'Ra5': 666, 'Rb5': 677, 'Rf4': 715, 'Rc7': 1609, 'Rf8': 1426, 'Rfc1': 581, 'cb5': 751, 'Rbe8': 47, 'Rhe1': 413, 'Ng7': 434, 'cd3': 233, 'ab4': 1197, 'Rfc8': 714, 'bc4': 895, 'Rdb1': 19, 'Rbc8': 109, 'R3c7': 1, 'gh3': 104, 'Qh3': 826, 'Kg1': 601, 'Nh4': 1115, 'Nhf5': 26, 'Bc1': 697, 'ab3': 693, 'ab6': 516, 'Kf2': 950, 'N2f3': 30, 'fe5': 1137, 'Nde5': 124, 'Rf5': 745, 'Kf4': 78, 'Kf5': 91, 'Ke5': 89, 'Rf1': 1070, 'Kd4': 68, 'Nba6': 13, 'Rf2': 700, 'Kg6': 335, 'f8=Q': 6, 'Nf7': 679, 'Rhd8': 151, 'Kc2': 338, 'Ra4': 747, 'Ra7': 1151, 'Kd3': 215, 'Re5': 1176, 'Kc4': 68, 'Kd5': 85, 'd2': 58, 'Kb8': 431, 'Ba7': 385, 'Ra2': 723, 'Rha1': 4, 'Ba1': 180, 'Qa1': 329, 'Kg3': 242, 'Nbc3': 106, 'N7e6': 2, 'Rg4': 445, 'Qf1': 259, 'ba4': 398, 'Bh7': 509, 'hg7': 51, 'Rdh1': 22, 'Bg1': 112, 'O-O-O+': 27, 'Ke8': 566, 'Rb6': 772, 'Qh8': 218, 'N6f5': 2, 'Rae8': 584, 'dxc8=Q': 3, 'Nf2': 476, 'Nb5': 1800, 'Qb5': 1116, 'Bf7': 660, 'Rb3': 553, 'Na2': 237, 'hg4': 536, 'Nge7': 384, 'cb3': 256, 'Rb4': 698, 'Nce2': 169, 'Rc2': 1149, 'Nc1': 327, 'Rc4': 1084, 'dc3': 323, 'Rdd8': 58, 'Ke4': 84, 'Kc8': 227, 'Ne1': 511, 'Kb7': 214, 'N8d7': 65, 'de6': 462, 'Rcc8': 50, 'Nh7': 444, 'N7b6': 27, 'Ncd5': 77, 'Rfa1': 50, 'R1a3': 2, 'ba5': 287, 'Rcc3': 8, 'Na1': 70, 'Rcb1': 21, 'Ngf4': 3, 'Qg8': 170, 'Rdc8': 115, 'R1c7': 2, 'Red2': 30, 'R6c7': 8, 'Kh6': 215, 'Rf3': 801, 'Ndf3': 79, 'N7c6': 12, 'Nbc6': 179, 'ed7': 39, 'Rce6': 5, 'Rad7': 9, 'R2e7': 1, 'N3d4': 12, 'Ndf5': 19, 'Ndf6': 163, 'Ka1': 118, 'Rbe1': 44, 'N4a6': 1, 'N3g5': 1, 'dc7': 31, 'Qe1': 828, 'Nbd4': 137, 'Rg5': 408, 'Rae4': 3, 'fg4': 295, 'Rce2': 8, 'Na7': 250, 'Nge4': 72, 'Nbd6': 13, 'Rcc7': 27, 'Ngh6': 3, 'N4f3': 8, 'Rbd1': 83, 'gf3': 601, 'N1c3': 46, 'Bh8': 137, 'Ke1': 220, 'Rae2': 16, 'Red3': 5, 'Nec3': 42, 'N6e5': 5, 'Rhc8': 87, 'Ka7': 62, 'Rec8': 96, 'Ka8': 103, 'Bb8': 211, 'Nb1': 255, 'Ka6': 30, 'Nfe4': 57, 'Nfe5': 57, 'N4a5': 2, 'Kb6': 65, 'Nfe7': 14, 'Rhd3': 7, 'c7': 108, 'Ng2': 200, 'Rbd8': 91, 'fg7': 99, 'b7': 51, 'Ncb8': 6, 'Ng1': 92, 'Ka2': 109, 'Kb3': 89, 'Rdg8': 47, 'Ref1': 28, 'Ndc5': 39, 'R1f2': 9, 'Rcd2': 20, 'R1d2': 25, 'Nfd2': 113, 'Ned6': 14, 'Rec1': 93, 'Raf8': 127, 'Nfd5': 143, 'Rgf7': 12, 'Rff8': 27, 'Ndf8': 12, 'Kh5': 62, 'Kg4': 59, 'Ncb5': 37, 'ba3': 203, 'Kg5': 91, 'Red1': 208, 'Nfe3': 8, 'R6b7': 1, 'Nh1': 35, 'Raf1': 129, 'R6f4': 2, 'Ree6': 15, 'Nce6': 8, 'Rff1': 20, 'Rhc1': 94, 'Nef5': 25, 'Rdf6': 5, 'R5c3': 3, 'Ndc6': 14, 'hg3': 490, 'N1e2': 31, 'Ngh5': 7, 'Reb8': 43, 'Rea4': 4, 'Nce7': 111, 'Rcb5': 1, 'R5b6': 3, 'Rbb7': 27, 'cxd8=Q': 3, 'd8=Q': 18, 'Nfg6': 13, 'N5c3': 12, 'Rdc1': 118, 'N1f3': 33, 'Rge8': 12, 'Rff2': 16, 'Nbc5': 17, 'Ncd6': 18, 'Ndc4': 19, 'Nfg5': 35, 'Nce5': 109, 'Kb5': 27, 'Ka5': 19, 'Kb4': 43, 'Kc5': 66, 'Nhf8': 11, 'de7': 46, 'e8=R': 1, 'Rdg1': 92, 'Nac7': 14, 'ab7': 30, 'Nec6': 61, 'R5f6': 5, 'N3g4': 2, 'N3d5': 8, 'Rdf8': 54, 'Rhf8': 70, 'cb2': 44, 'fe7': 30, 'bxa1=Q': 6, 'Rag8': 36, 'Rgc8': 6, 'R1d6': 5, 'de2': 12, 'R1c5': 3, 'Rcc1': 41, 'ef7': 106, 'Rec7': 11, 'Nde8': 3, 'Nfd6': 12, 'N7g6': 11, 'ba7': 12, 'a8=R': 1, 'Rea1': 22, 'Rbb4': 7, 'Rgg8': 12, 'Rgc1': 3, 'R5d2': 9, 'R3d2': 16, 'Rff3': 3, 'Rca8': 11, 'Rcc6': 8, 'Qh1': 117, 'Nfd4': 103, 'Rce1': 59, 'Rce8': 64, 'R6e7': 6, 'Nge6': 11, 'Kh4': 42, 'R1e2': 18, 'Rce3': 13, 'R1c2': 22, 'Nde6': 10, 'Rce4': 7, 'Reb3': 4, 'Rda5': 3, 'R5a7': 1, 'Ned7': 77, 'Ned5': 75, 'Nab5': 17, 'Rdb8': 27, 'Raa7': 21, 'fg2': 27, 'Ncb6': 17, 'c2': 51, 'cxb1=Q': 1, 'gh7': 41, 'N3d2': 10, 'Bh1': 68, 'N2g3': 8, 'Nhf6': 96, 'gf7': 57, 'fxe8=Q': 4, 'Nec8': 6, 'R1d7': 1, 'Rcg1': 6, 'R1g7': 1, 'Nge5': 98, 'N3e2': 4, 'Nac4': 38, 'Nbc4': 28, 'Rde4': 6, 'ab2': 19, 'a2': 20, 'Kh3': 116, 'Nfg8': 9, 'Neg4': 29, 'Ndf4': 18, 'Ncd7': 71, 'Nef3': 9, 'Rfc7': 6, 'Rfc5': 1, 'N7f6': 53, 'Ree3': 13, 'Nhg6': 6, 'N5f6': 41, 'Ngh7': 1, 'R7e6': 1, 'Rfe7': 8, 'Ree7': 28, 'Rda1': 13, 'Ngf5': 16, 'Rdd3': 13, 'Rda3': 1, 'N2b3': 25, 'Rec2': 6, 'Rdd1': 50, 'N1h2': 15, 'Rgb8': 3, 'Rca1': 10, 'Rdg7': 1, 'Kc1': 172, 'Rdh8': 12, 'R4h6': 1, 'Ncb1': 2, 'dc2': 10, 'gf2': 13, 'Rde8': 98, 'Rgf3': 6, 'R5d4': 3, 'cb7': 48, 'ba2': 14, 'Ree8': 60, 'Rcd3': 12, 'Rbe7': 3, 'Rcb8': 36, 'Ree2': 21, 'Rdd6': 14, 'Rbb8': 37, 'a7': 60, 'c8=Q': 11, 'N5f3': 16, 'Reb1': 35, 'bc2': 16, 'N1d2': 30, 'R1e3': 12, 'Ned3': 17, 'Rfg3': 2, 'e2': 27, 'e1=Q': 4, 'Rdd2': 21, 'Raa8': 42, 'de3': 114, 'b8=Q': 11, 'Rcd4': 3, 'Rag1': 26, 'R5a2': 1, 'd8=R': 2, 'h7': 29, 'Raf7': 12, 'Nab4': 7, 'Nfg4': 28, 'Rah8': 10, 'Rhc7': 2, 'Rhg3': 4, 'R1g2': 6, 'Nh8': 48, 'Ned4': 54, 'N1a2': 9, 'R1e5': 2, 'O-O+': 7, 'Neg5': 45, 'Nbc7': 6, 'Rdh2': 4, 'R2h4': 1, 'R2b7': 3, 'Nde4': 87, 'Rbb1': 9, 'Nba5': 5, 'N8e6': 2, 'Nfe6': 10, 'Raa1': 28, 'N4c6': 3, 'Rcc5': 5, 'Rah1': 7, 'Ndf1': 12, 'N1g3': 2, 'N5e4': 6, 'Reg1': 4, 'hg2': 17, 'Rgd8': 10, 'Ree1': 30, 'Rcb3': 5, 'R3b4': 1, 'Rba8': 9, 'Rbe3': 6, 'Reg4': 4, 'Rhb8': 22, 'Nhg4': 10, 'N8c6': 13, 'Rcb7': 8, 'a8=Q': 10, 'R8c3': 3, 'Ngh2': 3, 'Nac6': 24, 'N7d5': 1, 'Rbf8': 18, 'R7g3': 1, 'Rhh1': 7, 'Rca4': 4, 'Rda7': 3, 'Rfa8': 43, 'Rag2': 2, 'N6d7': 35, 'R3d4': 1, 'R4d7': 3, 'g7': 28, 'dxc8=N': 1, 'Raf3': 2, 'Rcf1': 22, 'ef2': 26, 'Ncd2': 16, 'R2e5': 1, 'R8d4': 3, 'exd8=B': 2, 'R4d2': 11, 'Ned2': 29, 'Rfd2': 11, 'b2': 25, 'Rdc7': 23, 'Rae7': 10, 'bc7': 5, 'Rea2': 2, 'R5a3': 3, 'Rfd7': 7, 'R4c3': 2, 'N5d4': 10, 'Rbf7': 5, 'Rhg7': 9, 'Rcd7': 10, 'N5a4': 4, 'Rge1': 18, 'Rba1': 2, 'N7c5': 3, 'R6b5': 1, 'R1b2': 5, 'Raa6': 6, 'Rcb6': 3, 'R4e2': 6, 'Rcc4': 10, 'Rdf5': 2, 'R4g6': 1, 'Neg3': 7, 'Ncb4': 31, 'N8f6': 1, 'N4c3': 4, 'Rde7': 11, 'R5d3': 2, 'Nhf4': 23, 'Nhg2': 3, 'cd2': 10, 'N7b5': 1, 'Raa5': 1, 'Ref2': 7, 'Ncd4': 47, 'Nba3': 4, 'g2': 11, 'Rbg1': 8, 'Rbh8': 1, 'Nec5': 21, 'cd7': 16, 'Rea7': 2, 'Nac1': 4, 'Rhh5': 3, 'Rhf5': 2, 'Rae6': 5, 'Ndb4': 28, 'e8=Q': 11, 'e8=N': 1, 'Ree5': 6, 'R7f5': 2, 'Ka3': 32, 'Ngf2': 3, 'N2e4': 2, 'Rbd5': 3, 'Rcc2': 29, 'f2': 14, 'Nhf7': 7, 'exf8=Q': 8, 'Rcf8': 24, 'Nef2': 3, 'R6e2': 1, 'N6g5': 5, 'Rea8': 13, 'Rbc6': 2, 'R8c7': 17, 'R1g4': 2, 'Rah2': 2, 'R4e3': 2, 'Nge3': 7, 'R3c6': 2, 'Rgd1': 13, 'Nhg3': 5, 'R4f2': 3, 'Ndc3': 15, 'R8e6': 5, 'R5c4': 4, 'Ndb6': 9, 'Neg6': 13, 'R3b2': 2, 'R8d5': 5, 'Reh5': 1, 'Rhb1': 33, 'c1=Q': 4, 'Rhf6': 5, 'Rgg7': 6, 'N5c6': 9, 'R8c5': 3, 'R1b3': 3, 'fe2': 6, 'Reg7': 3, 'Nac8': 3, 'R4f5': 3, 'Rfe4': 7, 'Rcg8': 10, 'N5g6': 5, 'Rgg2': 3, 'Rgd2': 3, 'Rhd2': 4, 'Rac6': 3, 'gh2': 12, 'Rag3': 1, 'Nbc1': 6, 'R1f3': 3, 'Rdd5': 6, 'Rec3': 6, 'Rbd3': 2, 'Rad2': 10, 'Ncd3': 17, 'Nfe1': 7, 'N7a6': 3, 'Rfd6': 10, 'N6a5': 2, 'N6h5': 4, 'h2': 7, 'Ngf7': 4, 'Rba7': 3, 'Rac2': 6, 'R1d4': 8, 'Rde2': 13, 'R8f6': 1, 'Reg8': 7, 'Rfg2': 1, 'Rga7': 1, 'R1a7': 3, 'Reh1': 5, 'Rbb2': 11, 'Rdf2': 12, 'Red7': 12, 'R8f7': 5, 'Rgd6': 1, 'Rhf7': 1, 'Rhh7': 2, 'N2f4': 2, 'Rhh8': 8, 'R8e7': 6, 'R6d2': 6, 'Rae3': 6, 'Nfd3': 8, 'N8h7': 8, 'R4e6': 2, 'R1c6': 4, 'Ref8': 14, 'Rcd6': 8, 'R5e7': 7, 'Rac7': 10, 'Nfe8': 6, 'R4a2': 3, 'Nac5': 44, 'Rde3': 6, 'Rca5': 3, 'Rdh7': 1, 'N5e6': 1, 'Nac2': 7, 'Ncd1': 7, 'Rfd4': 3, 'R4d5': 2, 'R3d5': 3, 'R4c5': 4, 'Rff7': 19, 'Rdc2': 16, 'Neg1': 1, 'N2c4': 5, 'Nab1': 5, 'Rhe3': 3, 'Raa2': 10, 'Rbe6': 1, 'Rbb6': 3, 'Ndb3': 12, 'Rhc4': 1, 'g8=Q': 2, 'R2c7': 2, 'Rff6': 9, 'b1=Q': 4, 'R2f6': 1, 'Rab3': 9, 'Rcf6': 2, 'Nhf3': 13, 'fxe1=Q': 1, 'Nhf1': 3, 'N3h2': 15, 'R5a4': 1, 'Rbd7': 4, 'Rac4': 3, 'Rca7': 1, 'fxe8=N': 1, 'Ndc2': 1, 'Rdc3': 7, 'Rec4': 3, 'Rfe5': 2, 'Rdc5': 6, 'Rga8': 2, 'Rab7': 5, 'Nec7': 13, 'Rhd6': 4, 'Rff4': 2, 'Ref6': 3, 'Rgf6': 3, 'R6f7': 3, 'Red5': 5, 'Rba4': 2, 'Rfb2': 6, 'Rfh7': 1, 'Nce8': 16, 'Rad6': 6, 'N7e5': 4, 'Nbd3': 7, 'Nac3': 10, 'Rbf1': 14, 'R1c3': 6, 'g1=Q': 3, 'Rcf2': 5, 'Rdg5': 1, 'R6e4': 1, 'Nca2': 1, 'R5b3': 1, 'Nde3': 10, 'Nbc8': 5, 'Rcd5': 3, 'R5f4': 2, 'N3e4': 8, 'Rfh1': 2, 'cxd1=Q': 2, 'Rgg1': 5, 'Rda6': 1, 'Rbc7': 10, 'Nab8': 4, 'R1f6': 4, 'R7d6': 1, 'Rgd5': 3, 'Rbe5': 3, 'R5e2': 3, 'R1e4': 4, 'Rfd3': 6, 'R1f7': 1, 'R5d7': 4, 'Nef7': 2, 'R6d3': 3, 'Rhd4': 2, 'Rgg3': 6, 'Nfe2': 8, 'Reb2': 5, 'Ka4': 12, 'Rce5': 2, 'Rhf4': 1, 'R4f7': 1, 'Nde1': 1, 'hxg8=Q': 1, 'Rdd4': 7, 'Neg8': 5, 'Rda2': 2, 'N4e5': 4, 'Nfh5': 5, 'R7a6': 4, 'Rbc2': 7, 'Nce1': 4, 'Rad3': 1, 'Nfg3': 3, 'Rbd2': 7, 'Rgf8': 9, 'R3c2': 4, 'R4e5': 1, 'gxf8=Q': 3, 'N3e5': 1, 'Ndf7': 6, 'Rgh8': 2, 'R5h7': 1, 'R6g7': 2, 'Rdb2': 3, 'Ndc8': 1, 'R5c6': 1, 'Rfg8': 6, 'R2c5': 2, 'Rhe4': 1, 'Rdg4': 2, 'R8g7': 2, 'Rhc2': 1, 'N4b3': 4, 'Reh8': 2, 'R8b7': 6, 'Ngh3': 1, 'Rde6': 6, 'Ndf2': 2, 'Rhe6': 1, 'Nca5': 2, 'N8a6': 4, 'Rce7': 12, 'Rfh8': 2, 'R5f3': 2, 'N4d5': 16, 'R3f2': 8, 'Ngh4': 1, 'Ncb3': 4, 'R8a6': 2, 'R2a3': 1, 'R4a7': 1, 'Reg6': 2, 'R3e5': 3, 'Nba8': 1, 'Rba6': 1, 'N2c3': 13, 'Rbd4': 2, 'Rba5': 4, 'R7a3': 2, 'bxc8=Q': 3, 'Rda8': 6, 'Reb7': 5, 'R8a7': 7, 'R4b5': 3, 'Reh4': 1, 'N6h7': 4, 'Nba4': 8, 'R8e5': 3, 'Nab6': 5, 'Rgd3': 2, 'R2d5': 3, 'R4c6': 5, 'Rfg4': 3, 'Nbc2': 6, 'Neg2': 2, 'N8e7': 13, 'Rdh5': 1, 'R8e2': 2, 'Rgg6': 2, 'Rae5': 4, 'Rec6': 3, 'R5f7': 2, 'Ndb1': 3, 'Nba7': 2, 'dxe8=Q': 2, 'N4g5': 1, 'Rfe2': 4, 'Rdb7': 5, 'R1d3': 3, 'Rbe4': 1, 'Red4': 2, 'N3a2': 1, 'R2c3': 3, 'Ndc7': 4, 'R3d7': 1, 'R8d3': 3, 'R3e2': 7, 'Rdc4': 5, 'Rch1': 9, 'Rhh6': 1, 'Ref7': 4, 'Qcc1': 1, 'Rdf4': 6, 'N6b5': 3, 'Ree4': 2, 'Raa4': 3, 'Nab2': 2, 'Rdc6': 2, 'R8d7': 10, 'Nhg5': 5, 'Neg7': 2, 'Rgb1': 1, 'Rbe2': 2, 'Nhg7': 2, 'N6d5': 7, 'ed2': 8, 'R6d5': 4, 'N6e7': 5, 'N4g3': 2, 'Rgd4': 1, 'Rhh3': 2, 'Rbg8': 2, 'R1h3': 2, 'Rbc3': 5, 'Rdg3': 1, 'R2f5': 2, 'Nge8': 2, 'N6d4': 2, 'Rhc6': 1, 'Rhc5': 1, 'Rde5': 5, 'Rfg1': 9, 'N8f7': 2, 'Rgh5': 1, 'Ref3': 4, 'R6a7': 4, 'Rcg2': 2, 'R8g6': 2, 'Rdf3': 2, 'Rhh4': 2, 'Nfd8': 1, 'Rfc2': 3, 'Rad5': 1, 'Ndb8': 5, 'Reg2': 1, 'Ncb7': 3, 'R8d6': 5, 'R4g2': 1, 'Rfa7': 2, 'R5b2': 1, 'bxa8=Q': 1, 'N4f5': 6, 'R1e7': 1, 'N3c5': 3, 'R1e6': 1, 'Ncd8': 7, 'N4c5': 1, 'R1a2': 6, 'N6b4': 1, 'f8=N': 2, 'Ned1': 1, 'Rdb3': 1, 'R7b6': 3, 'R8b6': 3, 'Rab6': 2, 'R7d5': 1, 'R1h4': 2, 'Rgc7': 2, 'Rhg6': 1, 'R7f2': 3, 'R1a5': 2, 'Nca6': 3, 'h1=Q': 1, 'Nca3': 3, 'R7e3': 3, 'Rac3': 2, 'R6d7': 3, 'Rhf3': 2, 'N6g4': 1, 'N6c5': 1, 'Rbb5': 1, 'R4e7': 5, 'Rgg5': 2, 'Rge7': 2, 'Rba3': 4, 'R6c4': 1, 'Rba2': 2, 'Rbd6': 4, 'Rfh6': 1, 'Reg3': 3, 'R7f3': 1, 'Rcf3': 3, 'R5e6': 5, 'Rbc5': 2, 'R1c4': 2, 'Rfd5': 3, 'Nbd8': 1, 'Rfe6': 3, 'R3a5': 1, 'N8a7': 1, 'R8c6': 3, 'R8a4': 1, 'Rga1': 1, 'Ngf1': 3, 'R2d3': 1, 'Rfe3': 3, 'cxb8=Q': 1, 'Nfh3': 1, 'Rgh3': 4, 'Rgf2': 4, 'Rfh2': 1, 'R7b3': 1, 'R4c2': 11, 'N3c4': 1, 'Ned8': 1, 'R2f3': 1, 'Rgh1': 3, 'R1h2': 1, 'R2e4': 3, 'Rhh2': 2, 'Rhg2': 1, 'R6d4': 2, 'Rdb6': 1, 'Red6': 6, 'Rha5': 1, 'R5c7': 1, 'Rhg5': 1, 'Rhb2': 1, 'R8b2': 1, 'Nca4': 1, 'R4d6': 1, 'Rfb3': 1, 'Rea3': 1, 'N3a4': 1, 'f8=R': 1, 'Nab3': 5, 'R3e7': 1, 'R3c5': 1, 'R5g3': 1, 'h8=Q': 2, 'N3f5': 1, 'R2b3': 1, 'Rfc6': 1, 'Ref4': 3, 'Rgh4': 3, 'R1h6': 1, 'R4c7': 5, 'R1a6': 2, 'Rea5': 1, 'R8c2': 1, 'Rdg2': 2, 'R1d5': 2, 'Rcg7': 1, 'Rag7': 2, 'exd8=Q': 2, 'fxg8=Q': 1, 'Q1c2': 1, 'R8e3': 3, 'Nfg7': 1, 'Rbf3': 2, 'Rea6': 1, 'Rab2': 3, 'Rbb3': 1, 'Rhe5': 2, 'Rcf7': 1, 'Rab5': 3, 'R8h4': 2, 'Raf6': 1, 'R7e4': 1, 'R5e4': 2, 'Rbf2': 4, 'R6f5': 1, 'Nec2': 1, 'Rca2': 1, 'R6e5': 2, 'R8c4': 3, 'R5f2': 1, 'Rfh3': 1, 'Nhg8': 1, 'Rcb4': 2, 'R8f4': 1, 'Rac5': 1, 'R7f4': 1, 'N2e3': 1, 'R4a3': 1, 'Rad4': 1, 'R8a5': 1, 'Nfd1': 1, 'R4h3': 1, 'Rgf5': 3, 'Rhb3': 1, 'R1f5': 2, 'N8g7': 1, 'N3b4': 1, 'N8b7': 1, 'R2d6': 1, 'R8d2': 1, 'N1b2': 1, 'bxc1=Q': 1, 'R7c4': 1, 'R3g4': 1, 'R1a4': 1, 'Rgb5': 1, 'a1=Q': 1, 'Nfh6': 2, 'N5h6': 1, 'Rff5': 1, 'R7e5': 2, 'R8a3': 2, 'Rgg4': 1, 'd1=Q': 1, 'R1f4': 1, 'N3b5': 1, 'R5c2': 1, 'N4h5': 1, 'Rdb5': 1, 'R7g5': 1, 'Rhg4': 1, 'R6b2': 1, 'Rge3': 1, 'R7h5': 1, 'Nba2': 1, 'Ncb2': 1, 'N8c7': 1, 'Rge5': 1, 'Nca1': 1, 'Rfa4': 1, 'Nba1': 2, 'Rfg6': 1, 'N6c7': 1, 'Rfc3': 2, 'Rbh2': 1, 'R8f3': 1, 'Rfb7': 1, 'Rch3': 1, 'Rca6': 2, 'Rch4': 1, 'R7h4': 1, 'Nab7': 1, 'R7c3': 2, 'Rge2': 1, 'Rab4': 1, 'R8e4': 1, 'N4b5': 1, 'R5g7': 2, 'R4f3': 3, 'Rah3': 1, 'Rge4': 1, 'R2a5': 1, 'Reg5': 1, 'Rcg5': 1, 'Rbc4': 2, 'Raf5': 1, 'R2d4': 1, 'Rhd5': 1, 'Nhf2': 1, 'R3a2': 1, 'R5d6': 1, 'R2d7': 2, 'Nef8': 1, 'Ndb7': 1, 'Ref5': 2, 'N2d4': 1, 'R7a2': 1, 'Nfh2': 1, 'N4e6': 1, 'R4g3': 1, 'N6c4': 2, 'N4d6': 1, 'Rgb4': 1, 'R7b5': 1, 'Rhc3': 1, 'N5h4': 1, 'R1h7': 2, 'Rdf7': 1, 'Raf2': 2, 'exd1=Q': 1, 'Rch8': 1, 'Nef1': 1, 'N1c2': 1, 'N5e3': 1, 'Qeg5': 1, 'R8h6': 1, 'Rcf5': 1}\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["The length of the dic is 1421\n{'d4': 14688, 'd5': 12567, 'c4': 11344, 'e6': 9787, 'Nc3': 14469, 'Nf6': 17290, 'cd5': 4960, 'ed5': 4411, 'Bg5': 6060, 'Be7': 8138, 'e3': 4285, 'Ne4': 7372, 'Bd8': 713, 'Nd1': 372, 'Bc7': 703, 'Nb2': 280, 'Rb1': 2127, 'Nc4': 3021, 'Bc4': 4743, 'dc4': 2788, 'Ne2': 2191, 'O-O': 22913, 'b6': 4537, 'Na6': 1344, 'Bd6': 3695, 'Rd8': 3738, 'Ba3': 967, 'Bb7': 5092, 'e4': 12286, 'f6': 2770, 'Ke2': 728, 'Nc7': 1167, 'Rhd1': 216, 'Ba6': 2059, 'Ke3': 322, 'Kf7': 1440, 'g4': 3205, 'g5': 3036, 'h4': 3668, 'h6': 5242, 'Rh1': 391, 'Re8': 4564, 'f3': 3267, 'hg5': 938, 'fg5': 308, 'd6': 7082, 'Nd5': 7478, 'Bd5': 3037, 'Rh6': 427, 'c3': 3090, 'd7': 179, 'Re6': 1109, 'Rh7': 384, 'Kg8': 1236, 'Rbh1': 7, 'Bc6': 3076, 'Rh8': 487, 'Ra8': 1337, 'Bd7': 4575, 'Qd5': 2321, 'Qa5': 2819, 'Nf3': 15607, 'c6': 5392, 'Ne5': 7403, 'Bf5': 3456, 'Be4': 2885, 'a3': 4386, 'Nbd7': 3776, 'Be3': 6332, 'de5': 2658, 'Ng4': 1817, 'Bd4': 2831, 'b4': 4836, 'Qd8': 1888, 'Ne3': 1587, 'dc6': 721, 'bc6': 1308, 'Rd1': 3857, 'Rab8': 500, 'Rc1': 3610, 'Rfd8': 1175, 'c5': 11165, 'Bc5': 3483, 'bc5': 944, 'g6': 6268, 'Rb2': 707, 'Rd2': 1477, 'e5': 10314, 'Nc6': 10970, 'd3': 1749, 'Bb3': 2230, 'Nbd2': 1736, 'Be6': 4161, 'Qd7': 2917, 'Re1': 4645, 'Rfe8': 1041, 'Nf1': 786, 'Ne7': 2775, 'Ng3': 1648, 'Bg4': 2793, 'h3': 4275, 'Kh8': 2197, 'Bf6': 4453, 'gf6': 1129, 'ed4': 2226, 'cd4': 6175, 'Bb4': 3991, 'Re3': 1046, 'Rg8': 1033, 'Bh3': 904, 'Qd4': 2745, 'Rg6': 475, 'Qb4': 1306, 'Qc3': 1832, 'Bc2': 1461, 'Nh2': 303, 'b5': 6096, 'Rc8': 4212, 'Bd3': 6461, 'Bh5': 822, 'Nh5': 1697, 'Rh5': 434, 'Qf6': 2418, 'Bd1': 334, 'Qf3': 2390, 'Rad1': 1947, 'Nf8': 948, 'Ng6': 1683, 'Qc7': 4838, 'Qe7': 3646, 'h5': 3315, 'gh6': 227, 'Qf4': 1435, 'Qh6': 942, 'f5': 4687, 'ef6': 915, 'Qf7': 979, 'Rg3': 539, 'Rg7': 563, 'Nd4': 7959, 'Bd2': 3892, 'Nc2': 762, 'Bc3': 3078, 'Qg5': 1538, 'Qe2': 3584, 'Qg2': 379, 'O-O-O': 2812, 'Qe4': 1811, 'Rhg1': 152, 'f4': 4966, 'Rgf1': 18, 'Qh4': 1540, 'Be1': 343, 'Qa4': 1755, 'Rf6': 908, 'Qg4': 1617, 'Qa2': 592, 'Bg6': 1439, 'hg6': 950, 'fg6': 503, 'Qg6': 1095, 'Qh5': 1623, 'g3': 4828, 'Bg7': 5172, 'Bg2': 4078, 'Nfd7': 736, 'b3': 3504, 'Qc2': 4235, 'Qd2': 4354, 'a6': 6796, 'Nde2': 127, 'Qg7': 552, 'Nef4': 19, 'Ne6': 2053, 'Rcd8': 155, 'Qf2': 971, 'Bc8': 973, 'ef5': 1185, 'cd6': 427, 'Qb6': 3031, 'Ng8': 266, 'Bb2': 2194, 'Rad8': 1270, 'Qd6': 2310, 'dc5': 2390, 'Qc5': 1782, 'Rfd1': 1428, 'a5': 4549, 'Qd1': 1137, 'Nd7': 4947, 'Kg2': 1489, 'Kf8': 1952, 'Kf3': 317, 'Ke7': 1316, 'Bb5': 3933, 'Kd6': 238, 'Na4': 1481, 'gh4': 164, 'Rh4': 431, 'de4': 1734, 'Re4': 1241, 'Rh2': 200, 'fe6': 1058, 'Qd3': 2645, 'Qf5': 1326, 'Rd5': 1381, 'Re7': 1408, 'Ndb5': 264, 'Na3': 735, 'a4': 4995, 'ab5': 1805, 'Qa8': 526, 'Nce3': 57, 'Qb7': 1410, 'Rc5': 1117, 'Qe5': 1773, 'Qg1': 111, 'Qg3': 1185, 'fg3': 233, 'bc3': 2282, 'Rc3': 997, 'Be2': 4655, 'ef3': 276, 'Bf3': 2789, 'Qb3': 2544, 'Nde7': 23, 'Na5': 1794, 'Nec4': 20, 'Rfe1': 1059, 'Qb2': 980, 'Bb1': 469, 'Kh1': 1657, 'e7': 108, 'Rfb8': 195, 'Nc5': 3581, 'Nh3': 443, 'Be5': 2513, 'Rac1': 1340, 'Nce4': 176, 'Nf4': 2056, 'Nd3': 1566, 'Rd3': 1160, 'Qc4': 1796, 'Rf7': 1180, 'Rcd1': 191, 'Rd7': 1885, 'Qe6': 1505, 'ef4': 820, 'gf4': 574, 'Ke6': 322, 'Bf4': 4152, 'Ba8': 384, 'Qc8': 1268, 'Na8': 157, 'Nd2': 3191, 'Ngf3': 240, 'Bf1': 1176, 'gh5': 312, 'Rg1': 827, 'Rd4': 1419, 'Qc6': 1609, 'Bh6': 1695, 'Red8': 192, 'Nb3': 2183, 'Nb7': 442, 'Qe8': 1185, 'Ng5': 1992, 'Bf8': 1730, 'Qa6': 770, 'Rdd7': 30, 'Ngf6': 422, 'Qa3': 829, 'Ne8': 1055, 'Ba4': 1490, 'Nh6': 644, 'Qc1': 786, 'Bb6': 1052, 'Ba2': 442, 'Nd8': 455, 'Nf5': 2413, 'ed3': 79, 'Nb6': 2751, 'cb6': 242, 'Rhe8': 165, 'Nb8': 597, 'cb4': 442, 'ba6': 226, 'Rfb1': 143, 'Rb7': 1054, 'gf5': 775, 'Kh2': 902, 'Qh7': 415, 'Nge2': 604, 'Kd1': 265, 'Kd2': 622, 'Kc3': 148, 'Rhf1': 172, 'Kb2': 205, 'Rae1': 681, 'Kb1': 1198, 'Rh3': 508, 'Qh2': 225, 'fe4': 988, 'Kh7': 1149, 'Rde1': 191, 'Bg8': 84, 'Kg7': 2042, 'Qe3': 1885, 'Kf6': 432, 'Kd8': 655, 'Qf8': 527, 'Kc7': 367, 'Nb4': 1771, 'N3h4': 3, 'Ra3': 639, 'Bh4': 1664, 'Rac8': 1268, 'fe3': 434, 'Rd6': 1448, 'Ra6': 769, 'Nef6': 58, 'Qb1': 522, 'Ngf8': 5, 'Be8': 569, 'Nc8': 484, 'Re2': 1026, 'f7': 55, 'ed6': 506, 'Kd7': 648, 'Rab1': 465, 'Rbc1': 98, 'Qa7': 682, 'Kc6': 151, 'Qb8': 974, 'Nd6': 2039, 'N5b6': 18, 'Kf1': 1058, 'Rb8': 2756, 'Ba5': 588, 'Rdf1': 77, 'Rhg8': 91, 'Nbd5': 187, 'Bh2': 261, 'Rg2': 305, 'Bf2': 829, 'Bg3': 1266, 'Rc6': 1124, 'Ra1': 968, 'Ra5': 666, 'Rb5': 677, 'Rf4': 715, 'Rc7': 1609, 'Rf8': 1426, 'Rfc1': 581, 'cb5': 751, 'Rbe8': 47, 'Rhe1': 413, 'Ng7': 434, 'cd3': 233, 'ab4': 1197, 'Rfc8': 714, 'bc4': 895, 'Rdb1': 19, 'Rbc8': 109, 'R3c7': 1, 'gh3': 104, 'Qh3': 826, 'Kg1': 601, 'Nh4': 1115, 'Nhf5': 26, 'Bc1': 697, 'ab3': 693, 'ab6': 516, 'Kf2': 950, 'N2f3': 30, 'fe5': 1137, 'Nde5': 124, 'Rf5': 745, 'Kf4': 78, 'Kf5': 91, 'Ke5': 89, 'Rf1': 1070, 'Kd4': 68, 'Nba6': 13, 'Rf2': 700, 'Kg6': 335, 'f8=Q': 6, 'Nf7': 679, 'Rhd8': 151, 'Kc2': 338, 'Ra4': 747, 'Ra7': 1151, 'Kd3': 215, 'Re5': 1176, 'Kc4': 68, 'Kd5': 85, 'd2': 58, 'Kb8': 431, 'Ba7': 385, 'Ra2': 723, 'Rha1': 4, 'Ba1': 180, 'Qa1': 329, 'Kg3': 242, 'Nbc3': 106, 'N7e6': 2, 'Rg4': 445, 'Qf1': 259, 'ba4': 398, 'Bh7': 509, 'hg7': 51, 'Rdh1': 22, 'Bg1': 112, 'O-O-O+': 27, 'Ke8': 566, 'Rb6': 772, 'Qh8': 218, 'N6f5': 2, 'Rae8': 584, 'dxc8=Q': 3, 'Nf2': 476, 'Nb5': 1800, 'Qb5': 1116, 'Bf7': 660, 'Rb3': 553, 'Na2': 237, 'hg4': 536, 'Nge7': 384, 'cb3': 256, 'Rb4': 698, 'Nce2': 169, 'Rc2': 1149, 'Nc1': 327, 'Rc4': 1084, 'dc3': 323, 'Rdd8': 58, 'Ke4': 84, 'Kc8': 227, 'Ne1': 511, 'Kb7': 214, 'N8d7': 65, 'de6': 462, 'Rcc8': 50, 'Nh7': 444, 'N7b6': 27, 'Ncd5': 77, 'Rfa1': 50, 'R1a3': 2, 'ba5': 287, 'Rcc3': 8, 'Na1': 70, 'Rcb1': 21, 'Ngf4': 3, 'Qg8': 170, 'Rdc8': 115, 'R1c7': 2, 'Red2': 30, 'R6c7': 8, 'Kh6': 215, 'Rf3': 801, 'Ndf3': 79, 'N7c6': 12, 'Nbc6': 179, 'ed7': 39, 'Rce6': 5, 'Rad7': 9, 'R2e7': 1, 'N3d4': 12, 'Ndf5': 19, 'Ndf6': 163, 'Ka1': 118, 'Rbe1': 44, 'N4a6': 1, 'N3g5': 1, 'dc7': 31, 'Qe1': 828, 'Nbd4': 137, 'Rg5': 408, 'Rae4': 3, 'fg4': 295, 'Rce2': 8, 'Na7': 250, 'Nge4': 72, 'Nbd6': 13, 'Rcc7': 27, 'Ngh6': 3, 'N4f3': 8, 'Rbd1': 83, 'gf3': 601, 'N1c3': 46, 'Bh8': 137, 'Ke1': 220, 'Rae2': 16, 'Red3': 5, 'Nec3': 42, 'N6e5': 5, 'Rhc8': 87, 'Ka7': 62, 'Rec8': 96, 'Ka8': 103, 'Bb8': 211, 'Nb1': 255, 'Ka6': 30, 'Nfe4': 57, 'Nfe5': 57, 'N4a5': 2, 'Kb6': 65, 'Nfe7': 14, 'Rhd3': 7, 'c7': 108, 'Ng2': 200, 'Rbd8': 91, 'fg7': 99, 'b7': 51, 'Ncb8': 6, 'Ng1': 92, 'Ka2': 109, 'Kb3': 89, 'Rdg8': 47, 'Ref1': 28, 'Ndc5': 39, 'R1f2': 9, 'Rcd2': 20, 'R1d2': 25, 'Nfd2': 113, 'Ned6': 14, 'Rec1': 93, 'Raf8': 127, 'Nfd5': 143, 'Rgf7': 12, 'Rff8': 27, 'Ndf8': 12, 'Kh5': 62, 'Kg4': 59, 'Ncb5': 37, 'ba3': 203, 'Kg5': 91, 'Red1': 208, 'Nfe3': 8, 'R6b7': 1, 'Nh1': 35, 'Raf1': 129, 'R6f4': 2, 'Ree6': 15, 'Nce6': 8, 'Rff1': 20, 'Rhc1': 94, 'Nef5': 25, 'Rdf6': 5, 'R5c3': 3, 'Ndc6': 14, 'hg3': 490, 'N1e2': 31, 'Ngh5': 7, 'Reb8': 43, 'Rea4': 4, 'Nce7': 111, 'Rcb5': 1, 'R5b6': 3, 'Rbb7': 27, 'cxd8=Q': 3, 'd8=Q': 18, 'Nfg6': 13, 'N5c3': 12, 'Rdc1': 118, 'N1f3': 33, 'Rge8': 12, 'Rff2': 16, 'Nbc5': 17, 'Ncd6': 18, 'Ndc4': 19, 'Nfg5': 35, 'Nce5': 109, 'Kb5': 27, 'Ka5': 19, 'Kb4': 43, 'Kc5': 66, 'Nhf8': 11, 'de7': 46, 'e8=R': 1, 'Rdg1': 92, 'Nac7': 14, 'ab7': 30, 'Nec6': 61, 'R5f6': 5, 'N3g4': 2, 'N3d5': 8, 'Rdf8': 54, 'Rhf8': 70, 'cb2': 44, 'fe7': 30, 'bxa1=Q': 6, 'Rag8': 36, 'Rgc8': 6, 'R1d6': 5, 'de2': 12, 'R1c5': 3, 'Rcc1': 41, 'ef7': 106, 'Rec7': 11, 'Nde8': 3, 'Nfd6': 12, 'N7g6': 11, 'ba7': 12, 'a8=R': 1, 'Rea1': 22, 'Rbb4': 7, 'Rgg8': 12, 'Rgc1': 3, 'R5d2': 9, 'R3d2': 16, 'Rff3': 3, 'Rca8': 11, 'Rcc6': 8, 'Qh1': 117, 'Nfd4': 103, 'Rce1': 59, 'Rce8': 64, 'R6e7': 6, 'Nge6': 11, 'Kh4': 42, 'R1e2': 18, 'Rce3': 13, 'R1c2': 22, 'Nde6': 10, 'Rce4': 7, 'Reb3': 4, 'Rda5': 3, 'R5a7': 1, 'Ned7': 77, 'Ned5': 75, 'Nab5': 17, 'Rdb8': 27, 'Raa7': 21, 'fg2': 27, 'Ncb6': 17, 'c2': 51, 'cxb1=Q': 1, 'gh7': 41, 'N3d2': 10, 'Bh1': 68, 'N2g3': 8, 'Nhf6': 96, 'gf7': 57, 'fxe8=Q': 4, 'Nec8': 6, 'R1d7': 1, 'Rcg1': 6, 'R1g7': 1, 'Nge5': 98, 'N3e2': 4, 'Nac4': 38, 'Nbc4': 28, 'Rde4': 6, 'ab2': 19, 'a2': 20, 'Kh3': 116, 'Nfg8': 9, 'Neg4': 29, 'Ndf4': 18, 'Ncd7': 71, 'Nef3': 9, 'Rfc7': 6, 'Rfc5': 1, 'N7f6': 53, 'Ree3': 13, 'Nhg6': 6, 'N5f6': 41, 'Ngh7': 1, 'R7e6': 1, 'Rfe7': 8, 'Ree7': 28, 'Rda1': 13, 'Ngf5': 16, 'Rdd3': 13, 'Rda3': 1, 'N2b3': 25, 'Rec2': 6, 'Rdd1': 50, 'N1h2': 15, 'Rgb8': 3, 'Rca1': 10, 'Rdg7': 1, 'Kc1': 172, 'Rdh8': 12, 'R4h6': 1, 'Ncb1': 2, 'dc2': 10, 'gf2': 13, 'Rde8': 98, 'Rgf3': 6, 'R5d4': 3, 'cb7': 48, 'ba2': 14, 'Ree8': 60, 'Rcd3': 12, 'Rbe7': 3, 'Rcb8': 36, 'Ree2': 21, 'Rdd6': 14, 'Rbb8': 37, 'a7': 60, 'c8=Q': 11, 'N5f3': 16, 'Reb1': 35, 'bc2': 16, 'N1d2': 30, 'R1e3': 12, 'Ned3': 17, 'Rfg3': 2, 'e2': 27, 'e1=Q': 4, 'Rdd2': 21, 'Raa8': 42, 'de3': 114, 'b8=Q': 11, 'Rcd4': 3, 'Rag1': 26, 'R5a2': 1, 'd8=R': 2, 'h7': 29, 'Raf7': 12, 'Nab4': 7, 'Nfg4': 28, 'Rah8': 10, 'Rhc7': 2, 'Rhg3': 4, 'R1g2': 6, 'Nh8': 48, 'Ned4': 54, 'N1a2': 9, 'R1e5': 2, 'O-O+': 7, 'Neg5': 45, 'Nbc7': 6, 'Rdh2': 4, 'R2h4': 1, 'R2b7': 3, 'Nde4': 87, 'Rbb1': 9, 'Nba5': 5, 'N8e6': 2, 'Nfe6': 10, 'Raa1': 28, 'N4c6': 3, 'Rcc5': 5, 'Rah1': 7, 'Ndf1': 12, 'N1g3': 2, 'N5e4': 6, 'Reg1': 4, 'hg2': 17, 'Rgd8': 10, 'Ree1': 30, 'Rcb3': 5, 'R3b4': 1, 'Rba8': 9, 'Rbe3': 6, 'Reg4': 4, 'Rhb8': 22, 'Nhg4': 10, 'N8c6': 13, 'Rcb7': 8, 'a8=Q': 10, 'R8c3': 3, 'Ngh2': 3, 'Nac6': 24, 'N7d5': 1, 'Rbf8': 18, 'R7g3': 1, 'Rhh1': 7, 'Rca4': 4, 'Rda7': 3, 'Rfa8': 43, 'Rag2': 2, 'N6d7': 35, 'R3d4': 1, 'R4d7': 3, 'g7': 28, 'dxc8=N': 1, 'Raf3': 2, 'Rcf1': 22, 'ef2': 26, 'Ncd2': 16, 'R2e5': 1, 'R8d4': 3, 'exd8=B': 2, 'R4d2': 11, 'Ned2': 29, 'Rfd2': 11, 'b2': 25, 'Rdc7': 23, 'Rae7': 10, 'bc7': 5, 'Rea2': 2, 'R5a3': 3, 'Rfd7': 7, 'R4c3': 2, 'N5d4': 10, 'Rbf7': 5, 'Rhg7': 9, 'Rcd7': 10, 'N5a4': 4, 'Rge1': 18, 'Rba1': 2, 'N7c5': 3, 'R6b5': 1, 'R1b2': 5, 'Raa6': 6, 'Rcb6': 3, 'R4e2': 6, 'Rcc4': 10, 'Rdf5': 2, 'R4g6': 1, 'Neg3': 7, 'Ncb4': 31, 'N8f6': 1, 'N4c3': 4, 'Rde7': 11, 'R5d3': 2, 'Nhf4': 23, 'Nhg2': 3, 'cd2': 10, 'N7b5': 1, 'Raa5': 1, 'Ref2': 7, 'Ncd4': 47, 'Nba3': 4, 'g2': 11, 'Rbg1': 8, 'Rbh8': 1, 'Nec5': 21, 'cd7': 16, 'Rea7': 2, 'Nac1': 4, 'Rhh5': 3, 'Rhf5': 2, 'Rae6': 5, 'Ndb4': 28, 'e8=Q': 11, 'e8=N': 1, 'Ree5': 6, 'R7f5': 2, 'Ka3': 32, 'Ngf2': 3, 'N2e4': 2, 'Rbd5': 3, 'Rcc2': 29, 'f2': 14, 'Nhf7': 7, 'exf8=Q': 8, 'Rcf8': 24, 'Nef2': 3, 'R6e2': 1, 'N6g5': 5, 'Rea8': 13, 'Rbc6': 2, 'R8c7': 17, 'R1g4': 2, 'Rah2': 2, 'R4e3': 2, 'Nge3': 7, 'R3c6': 2, 'Rgd1': 13, 'Nhg3': 5, 'R4f2': 3, 'Ndc3': 15, 'R8e6': 5, 'R5c4': 4, 'Ndb6': 9, 'Neg6': 13, 'R3b2': 2, 'R8d5': 5, 'Reh5': 1, 'Rhb1': 33, 'c1=Q': 4, 'Rhf6': 5, 'Rgg7': 6, 'N5c6': 9, 'R8c5': 3, 'R1b3': 3, 'fe2': 6, 'Reg7': 3, 'Nac8': 3, 'R4f5': 3, 'Rfe4': 7, 'Rcg8': 10, 'N5g6': 5, 'Rgg2': 3, 'Rgd2': 3, 'Rhd2': 4, 'Rac6': 3, 'gh2': 12, 'Rag3': 1, 'Nbc1': 6, 'R1f3': 3, 'Rdd5': 6, 'Rec3': 6, 'Rbd3': 2, 'Rad2': 10, 'Ncd3': 17, 'Nfe1': 7, 'N7a6': 3, 'Rfd6': 10, 'N6a5': 2, 'N6h5': 4, 'h2': 7, 'Ngf7': 4, 'Rba7': 3, 'Rac2': 6, 'R1d4': 8, 'Rde2': 13, 'R8f6': 1, 'Reg8': 7, 'Rfg2': 1, 'Rga7': 1, 'R1a7': 3, 'Reh1': 5, 'Rbb2': 11, 'Rdf2': 12, 'Red7': 12, 'R8f7': 5, 'Rgd6': 1, 'Rhf7': 1, 'Rhh7': 2, 'N2f4': 2, 'Rhh8': 8, 'R8e7': 6, 'R6d2': 6, 'Rae3': 6, 'Nfd3': 8, 'N8h7': 8, 'R4e6': 2, 'R1c6': 4, 'Ref8': 14, 'Rcd6': 8, 'R5e7': 7, 'Rac7': 10, 'Nfe8': 6, 'R4a2': 3, 'Nac5': 44, 'Rde3': 6, 'Rca5': 3, 'Rdh7': 1, 'N5e6': 1, 'Nac2': 7, 'Ncd1': 7, 'Rfd4': 3, 'R4d5': 2, 'R3d5': 3, 'R4c5': 4, 'Rff7': 19, 'Rdc2': 16, 'Neg1': 1, 'N2c4': 5, 'Nab1': 5, 'Rhe3': 3, 'Raa2': 10, 'Rbe6': 1, 'Rbb6': 3, 'Ndb3': 12, 'Rhc4': 1, 'g8=Q': 2, 'R2c7': 2, 'Rff6': 9, 'b1=Q': 4, 'R2f6': 1, 'Rab3': 9, 'Rcf6': 2, 'Nhf3': 13, 'fxe1=Q': 1, 'Nhf1': 3, 'N3h2': 15, 'R5a4': 1, 'Rbd7': 4, 'Rac4': 3, 'Rca7': 1, 'fxe8=N': 1, 'Ndc2': 1, 'Rdc3': 7, 'Rec4': 3, 'Rfe5': 2, 'Rdc5': 6, 'Rga8': 2, 'Rab7': 5, 'Nec7': 13, 'Rhd6': 4, 'Rff4': 2, 'Ref6': 3, 'Rgf6': 3, 'R6f7': 3, 'Red5': 5, 'Rba4': 2, 'Rfb2': 6, 'Rfh7': 1, 'Nce8': 16, 'Rad6': 6, 'N7e5': 4, 'Nbd3': 7, 'Nac3': 10, 'Rbf1': 14, 'R1c3': 6, 'g1=Q': 3, 'Rcf2': 5, 'Rdg5': 1, 'R6e4': 1, 'Nca2': 1, 'R5b3': 1, 'Nde3': 10, 'Nbc8': 5, 'Rcd5': 3, 'R5f4': 2, 'N3e4': 8, 'Rfh1': 2, 'cxd1=Q': 2, 'Rgg1': 5, 'Rda6': 1, 'Rbc7': 10, 'Nab8': 4, 'R1f6': 4, 'R7d6': 1, 'Rgd5': 3, 'Rbe5': 3, 'R5e2': 3, 'R1e4': 4, 'Rfd3': 6, 'R1f7': 1, 'R5d7': 4, 'Nef7': 2, 'R6d3': 3, 'Rhd4': 2, 'Rgg3': 6, 'Nfe2': 8, 'Reb2': 5, 'Ka4': 12, 'Rce5': 2, 'Rhf4': 1, 'R4f7': 1, 'Nde1': 1, 'hxg8=Q': 1, 'Rdd4': 7, 'Neg8': 5, 'Rda2': 2, 'N4e5': 4, 'Nfh5': 5, 'R7a6': 4, 'Rbc2': 7, 'Nce1': 4, 'Rad3': 1, 'Nfg3': 3, 'Rbd2': 7, 'Rgf8': 9, 'R3c2': 4, 'R4e5': 1, 'gxf8=Q': 3, 'N3e5': 1, 'Ndf7': 6, 'Rgh8': 2, 'R5h7': 1, 'R6g7': 2, 'Rdb2': 3, 'Ndc8': 1, 'R5c6': 1, 'Rfg8': 6, 'R2c5': 2, 'Rhe4': 1, 'Rdg4': 2, 'R8g7': 2, 'Rhc2': 1, 'N4b3': 4, 'Reh8': 2, 'R8b7': 6, 'Ngh3': 1, 'Rde6': 6, 'Ndf2': 2, 'Rhe6': 1, 'Nca5': 2, 'N8a6': 4, 'Rce7': 12, 'Rfh8': 2, 'R5f3': 2, 'N4d5': 16, 'R3f2': 8, 'Ngh4': 1, 'Ncb3': 4, 'R8a6': 2, 'R2a3': 1, 'R4a7': 1, 'Reg6': 2, 'R3e5': 3, 'Nba8': 1, 'Rba6': 1, 'N2c3': 13, 'Rbd4': 2, 'Rba5': 4, 'R7a3': 2, 'bxc8=Q': 3, 'Rda8': 6, 'Reb7': 5, 'R8a7': 7, 'R4b5': 3, 'Reh4': 1, 'N6h7': 4, 'Nba4': 8, 'R8e5': 3, 'Nab6': 5, 'Rgd3': 2, 'R2d5': 3, 'R4c6': 5, 'Rfg4': 3, 'Nbc2': 6, 'Neg2': 2, 'N8e7': 13, 'Rdh5': 1, 'R8e2': 2, 'Rgg6': 2, 'Rae5': 4, 'Rec6': 3, 'R5f7': 2, 'Ndb1': 3, 'Nba7': 2, 'dxe8=Q': 2, 'N4g5': 1, 'Rfe2': 4, 'Rdb7': 5, 'R1d3': 3, 'Rbe4': 1, 'Red4': 2, 'N3a2': 1, 'R2c3': 3, 'Ndc7': 4, 'R3d7': 1, 'R8d3': 3, 'R3e2': 7, 'Rdc4': 5, 'Rch1': 9, 'Rhh6': 1, 'Ref7': 4, 'Qcc1': 1, 'Rdf4': 6, 'N6b5': 3, 'Ree4': 2, 'Raa4': 3, 'Nab2': 2, 'Rdc6': 2, 'R8d7': 10, 'Nhg5': 5, 'Neg7': 2, 'Rgb1': 1, 'Rbe2': 2, 'Nhg7': 2, 'N6d5': 7, 'ed2': 8, 'R6d5': 4, 'N6e7': 5, 'N4g3': 2, 'Rgd4': 1, 'Rhh3': 2, 'Rbg8': 2, 'R1h3': 2, 'Rbc3': 5, 'Rdg3': 1, 'R2f5': 2, 'Nge8': 2, 'N6d4': 2, 'Rhc6': 1, 'Rhc5': 1, 'Rde5': 5, 'Rfg1': 9, 'N8f7': 2, 'Rgh5': 1, 'Ref3': 4, 'R6a7': 4, 'Rcg2': 2, 'R8g6': 2, 'Rdf3': 2, 'Rhh4': 2, 'Nfd8': 1, 'Rfc2': 3, 'Rad5': 1, 'Ndb8': 5, 'Reg2': 1, 'Ncb7': 3, 'R8d6': 5, 'R4g2': 1, 'Rfa7': 2, 'R5b2': 1, 'bxa8=Q': 1, 'N4f5': 6, 'R1e7': 1, 'N3c5': 3, 'R1e6': 1, 'Ncd8': 7, 'N4c5': 1, 'R1a2': 6, 'N6b4': 1, 'f8=N': 2, 'Ned1': 1, 'Rdb3': 1, 'R7b6': 3, 'R8b6': 3, 'Rab6': 2, 'R7d5': 1, 'R1h4': 2, 'Rgc7': 2, 'Rhg6': 1, 'R7f2': 3, 'R1a5': 2, 'Nca6': 3, 'h1=Q': 1, 'Nca3': 3, 'R7e3': 3, 'Rac3': 2, 'R6d7': 3, 'Rhf3': 2, 'N6g4': 1, 'N6c5': 1, 'Rbb5': 1, 'R4e7': 5, 'Rgg5': 2, 'Rge7': 2, 'Rba3': 4, 'R6c4': 1, 'Rba2': 2, 'Rbd6': 4, 'Rfh6': 1, 'Reg3': 3, 'R7f3': 1, 'Rcf3': 3, 'R5e6': 5, 'Rbc5': 2, 'R1c4': 2, 'Rfd5': 3, 'Nbd8': 1, 'Rfe6': 3, 'R3a5': 1, 'N8a7': 1, 'R8c6': 3, 'R8a4': 1, 'Rga1': 1, 'Ngf1': 3, 'R2d3': 1, 'Rfe3': 3, 'cxb8=Q': 1, 'Nfh3': 1, 'Rgh3': 4, 'Rgf2': 4, 'Rfh2': 1, 'R7b3': 1, 'R4c2': 11, 'N3c4': 1, 'Ned8': 1, 'R2f3': 1, 'Rgh1': 3, 'R1h2': 1, 'R2e4': 3, 'Rhh2': 2, 'Rhg2': 1, 'R6d4': 2, 'Rdb6': 1, 'Red6': 6, 'Rha5': 1, 'R5c7': 1, 'Rhg5': 1, 'Rhb2': 1, 'R8b2': 1, 'Nca4': 1, 'R4d6': 1, 'Rfb3': 1, 'Rea3': 1, 'N3a4': 1, 'f8=R': 1, 'Nab3': 5, 'R3e7': 1, 'R3c5': 1, 'R5g3': 1, 'h8=Q': 2, 'N3f5': 1, 'R2b3': 1, 'Rfc6': 1, 'Ref4': 3, 'Rgh4': 3, 'R1h6': 1, 'R4c7': 5, 'R1a6': 2, 'Rea5': 1, 'R8c2': 1, 'Rdg2': 2, 'R1d5': 2, 'Rcg7': 1, 'Rag7': 2, 'exd8=Q': 2, 'fxg8=Q': 1, 'Q1c2': 1, 'R8e3': 3, 'Nfg7': 1, 'Rbf3': 2, 'Rea6': 1, 'Rab2': 3, 'Rbb3': 1, 'Rhe5': 2, 'Rcf7': 1, 'Rab5': 3, 'R8h4': 2, 'Raf6': 1, 'R7e4': 1, 'R5e4': 2, 'Rbf2': 4, 'R6f5': 1, 'Nec2': 1, 'Rca2': 1, 'R6e5': 2, 'R8c4': 3, 'R5f2': 1, 'Rfh3': 1, 'Nhg8': 1, 'Rcb4': 2, 'R8f4': 1, 'Rac5': 1, 'R7f4': 1, 'N2e3': 1, 'R4a3': 1, 'Rad4': 1, 'R8a5': 1, 'Nfd1': 1, 'R4h3': 1, 'Rgf5': 3, 'Rhb3': 1, 'R1f5': 2, 'N8g7': 1, 'N3b4': 1, 'N8b7': 1, 'R2d6': 1, 'R8d2': 1, 'N1b2': 1, 'bxc1=Q': 1, 'R7c4': 1, 'R3g4': 1, 'R1a4': 1, 'Rgb5': 1, 'a1=Q': 1, 'Nfh6': 2, 'N5h6': 1, 'Rff5': 1, 'R7e5': 2, 'R8a3': 2, 'Rgg4': 1, 'd1=Q': 1, 'R1f4': 1, 'N3b5': 1, 'R5c2': 1, 'N4h5': 1, 'Rdb5': 1, 'R7g5': 1, 'Rhg4': 1, 'R6b2': 1, 'Rge3': 1, 'R7h5': 1, 'Nba2': 1, 'Ncb2': 1, 'N8c7': 1, 'Rge5': 1, 'Nca1': 1, 'Rfa4': 1, 'Nba1': 2, 'Rfg6': 1, 'N6c7': 1, 'Rfc3': 2, 'Rbh2': 1, 'R8f3': 1, 'Rfb7': 1, 'Rch3': 1, 'Rca6': 2, 'Rch4': 1, 'R7h4': 1, 'Nab7': 1, 'R7c3': 2, 'Rge2': 1, 'Rab4': 1, 'R8e4': 1, 'N4b5': 1, 'R5g7': 2, 'R4f3': 3, 'Rah3': 1, 'Rge4': 1, 'R2a5': 1, 'Reg5': 1, 'Rcg5': 1, 'Rbc4': 2, 'Raf5': 1, 'R2d4': 1, 'Rhd5': 1, 'Nhf2': 1, 'R3a2': 1, 'R5d6': 1, 'R2d7': 2, 'Nef8': 1, 'Ndb7': 1, 'Ref5': 2, 'N2d4': 1, 'R7a2': 1, 'Nfh2': 1, 'N4e6': 1, 'R4g3': 1, 'N6c4': 2, 'N4d6': 1, 'Rgb4': 1, 'R7b5': 1, 'Rhc3': 1, 'N5h4': 1, 'R1h7': 2, 'Rdf7': 1, 'Raf2': 2, 'exd1=Q': 1, 'Rch8': 1, 'Nef1': 1, 'N1c2': 1, 'N5e3': 1, 'Qeg5': 1, 'R8h6': 1, 'Rcf5': 1}\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["Save the dictionary calculated"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"82ef6831-1e12-40c8-91f6-3d2319a85a80"}}},{"cell_type":"code","source":["with open(\"/tmp/dic_freq_moves_completeGames_15k.txt\", \"w+\") as fp:\n    json.dump(dic_moves, fp)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fb6f866c-363a-495f-8c96-5919e8a1b982"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.cp(\"file:/tmp/dic_freq_moves_completeGames_15k.txt\", \"dbfs:/tmp/dic_freq_moves_completeGames_15k.txt\")\ndisplay(dbutils.fs.ls(\"/tmp/dic_freq_moves_completeGames_15k.txt\"))\ndbutils.fs.cp(\"dbfs:/tmp/dic_freq_moves_completeGames_15k.txt\", \"/FileStore/dic_freq_moves_completeGames_15k.txt\")\n#community.cloud.databricks.com/files/dic_freq_moves_completeGames_15k.txt?o=5604700270830977"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8ab016ee-136b-48b0-81eb-2edb0282491f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Encoding of the frequency dictionary and adding at the end of the dictionary an element that we will use for padding."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b55258a3-deaa-4a4c-83cb-5e3ad8cbe2c2"}}},{"cell_type":"code","source":["int_moves=dict((c, i) for i, c in enumerate(dic_moves.keys()))\nint_moves[\"fill\"]=len(int_moves)\nprint(int_moves)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6da363a6-2e08-4042-8b69-95a2767c87cb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"{'d4': 0, 'd5': 1, 'c4': 2, 'e6': 3, 'Nc3': 4, 'Nf6': 5, 'cd5': 6, 'ed5': 7, 'Bg5': 8, 'Be7': 9, 'e3': 10, 'Ne4': 11, 'Bd8': 12, 'Nd1': 13, 'Bc7': 14, 'Nb2': 15, 'Rb1': 16, 'Nc4': 17, 'Bc4': 18, 'dc4': 19, 'Ne2': 20, 'O-O': 21, 'b6': 22, 'Na6': 23, 'Bd6': 24, 'Rd8': 25, 'Ba3': 26, 'Bb7': 27, 'e4': 28, 'f6': 29, 'Ke2': 30, 'Nc7': 31, 'Rhd1': 32, 'Ba6': 33, 'Ke3': 34, 'Kf7': 35, 'g4': 36, 'g5': 37, 'h4': 38, 'h6': 39, 'Rh1': 40, 'Re8': 41, 'f3': 42, 'hg5': 43, 'fg5': 44, 'd6': 45, 'Nd5': 46, 'Bd5': 47, 'Rh6': 48, 'c3': 49, 'd7': 50, 'Re6': 51, 'Rh7': 52, 'Kg8': 53, 'Rbh1': 54, 'Bc6': 55, 'Rh8': 56, 'Ra8': 57, 'Bd7': 58, 'Qd5': 59, 'Qa5': 60, 'Nf3': 61, 'c6': 62, 'Ne5': 63, 'Bf5': 64, 'Be4': 65, 'a3': 66, 'Nbd7': 67, 'Be3': 68, 'de5': 69, 'Ng4': 70, 'Bd4': 71, 'b4': 72, 'Qd8': 73, 'Ne3': 74, 'dc6': 75, 'bc6': 76, 'Rd1': 77, 'Rab8': 78, 'Rc1': 79, 'Rfd8': 80, 'c5': 81, 'Bc5': 82, 'bc5': 83, 'g6': 84, 'Rb2': 85, 'Rd2': 86, 'e5': 87, 'Nc6': 88, 'd3': 89, 'Bb3': 90, 'Nbd2': 91, 'Be6': 92, 'Qd7': 93, 'Re1': 94, 'Rfe8': 95, 'Nf1': 96, 'Ne7': 97, 'Ng3': 98, 'Bg4': 99, 'h3': 100, 'Kh8': 101, 'Bf6': 102, 'gf6': 103, 'ed4': 104, 'cd4': 105, 'Bb4': 106, 'Re3': 107, 'Rg8': 108, 'Bh3': 109, 'Qd4': 110, 'Rg6': 111, 'Qb4': 112, 'Qc3': 113, 'Bc2': 114, 'Nh2': 115, 'b5': 116, 'Rc8': 117, 'Bd3': 118, 'Bh5': 119, 'Nh5': 120, 'Rh5': 121, 'Qf6': 122, 'Bd1': 123, 'Qf3': 124, 'Rad1': 125, 'Nf8': 126, 'Ng6': 127, 'Qc7': 128, 'Qe7': 129, 'h5': 130, 'gh6': 131, 'Qf4': 132, 'Qh6': 133, 'f5': 134, 'ef6': 135, 'Qf7': 136, 'Rg3': 137, 'Rg7': 138, 'Nd4': 139, 'Bd2': 140, 'Nc2': 141, 'Bc3': 142, 'Qg5': 143, 'Qe2': 144, 'Qg2': 145, 'O-O-O': 146, 'Qe4': 147, 'Rhg1': 148, 'f4': 149, 'Rgf1': 150, 'Qh4': 151, 'Be1': 152, 'Qa4': 153, 'Rf6': 154, 'Qg4': 155, 'Qa2': 156, 'Bg6': 157, 'hg6': 158, 'fg6': 159, 'Qg6': 160, 'Qh5': 161, 'g3': 162, 'Bg7': 163, 'Bg2': 164, 'Nfd7': 165, 'b3': 166, 'Qc2': 167, 'Qd2': 168, 'a6': 169, 'Nde2': 170, 'Qg7': 171, 'Nef4': 172, 'Ne6': 173, 'Rcd8': 174, 'Qf2': 175, 'Bc8': 176, 'ef5': 177, 'cd6': 178, 'Qb6': 179, 'Ng8': 180, 'Bb2': 181, 'Rad8': 182, 'Qd6': 183, 'dc5': 184, 'Qc5': 185, 'Rfd1': 186, 'a5': 187, 'Qd1': 188, 'Nd7': 189, 'Kg2': 190, 'Kf8': 191, 'Kf3': 192, 'Ke7': 193, 'Bb5': 194, 'Kd6': 195, 'Na4': 196, 'gh4': 197, 'Rh4': 198, 'de4': 199, 'Re4': 200, 'Rh2': 201, 'fe6': 202, 'Qd3': 203, 'Qf5': 204, 'Rd5': 205, 'Re7': 206, 'Ndb5': 207, 'Na3': 208, 'a4': 209, 'ab5': 210, 'Qa8': 211, 'Nce3': 212, 'Qb7': 213, 'Rc5': 214, 'Qe5': 215, 'Qg1': 216, 'Qg3': 217, 'fg3': 218, 'bc3': 219, 'Rc3': 220, 'Be2': 221, 'ef3': 222, 'Bf3': 223, 'Qb3': 224, 'Nde7': 225, 'Na5': 226, 'Nec4': 227, 'Rfe1': 228, 'Qb2': 229, 'Bb1': 230, 'Kh1': 231, 'e7': 232, 'Rfb8': 233, 'Nc5': 234, 'Nh3': 235, 'Be5': 236, 'Rac1': 237, 'Nce4': 238, 'Nf4': 239, 'Nd3': 240, 'Rd3': 241, 'Qc4': 242, 'Rf7': 243, 'Rcd1': 244, 'Rd7': 245, 'Qe6': 246, 'ef4': 247, 'gf4': 248, 'Ke6': 249, 'Bf4': 250, 'Ba8': 251, 'Qc8': 252, 'Na8': 253, 'Nd2': 254, 'Ngf3': 255, 'Bf1': 256, 'gh5': 257, 'Rg1': 258, 'Rd4': 259, 'Qc6': 260, 'Bh6': 261, 'Red8': 262, 'Nb3': 263, 'Nb7': 264, 'Qe8': 265, 'Ng5': 266, 'Bf8': 267, 'Qa6': 268, 'Rdd7': 269, 'Ngf6': 270, 'Qa3': 271, 'Ne8': 272, 'Ba4': 273, 'Nh6': 274, 'Qc1': 275, 'Bb6': 276, 'Ba2': 277, 'Nd8': 278, 'Nf5': 279, 'ed3': 280, 'Nb6': 281, 'cb6': 282, 'Rhe8': 283, 'Nb8': 284, 'cb4': 285, 'ba6': 286, 'Rfb1': 287, 'Rb7': 288, 'gf5': 289, 'Kh2': 290, 'Qh7': 291, 'Nge2': 292, 'Kd1': 293, 'Kd2': 294, 'Kc3': 295, 'Rhf1': 296, 'Kb2': 297, 'Rae1': 298, 'Kb1': 299, 'Rh3': 300, 'Qh2': 301, 'fe4': 302, 'Kh7': 303, 'Rde1': 304, 'Bg8': 305, 'Kg7': 306, 'Qe3': 307, 'Kf6': 308, 'Kd8': 309, 'Qf8': 310, 'Kc7': 311, 'Nb4': 312, 'N3h4': 313, 'Ra3': 314, 'Bh4': 315, 'Rac8': 316, 'fe3': 317, 'Rd6': 318, 'Ra6': 319, 'Nef6': 320, 'Qb1': 321, 'Ngf8': 322, 'Be8': 323, 'Nc8': 324, 'Re2': 325, 'f7': 326, 'ed6': 327, 'Kd7': 328, 'Rab1': 329, 'Rbc1': 330, 'Qa7': 331, 'Kc6': 332, 'Qb8': 333, 'Nd6': 334, 'N5b6': 335, 'Kf1': 336, 'Rb8': 337, 'Ba5': 338, 'Rdf1': 339, 'Rhg8': 340, 'Nbd5': 341, 'Bh2': 342, 'Rg2': 343, 'Bf2': 344, 'Bg3': 345, 'Rc6': 346, 'Ra1': 347, 'Ra5': 348, 'Rb5': 349, 'Rf4': 350, 'Rc7': 351, 'Rf8': 352, 'Rfc1': 353, 'cb5': 354, 'Rbe8': 355, 'Rhe1': 356, 'Ng7': 357, 'cd3': 358, 'ab4': 359, 'Rfc8': 360, 'bc4': 361, 'Rdb1': 362, 'Rbc8': 363, 'R3c7': 364, 'gh3': 365, 'Qh3': 366, 'Kg1': 367, 'Nh4': 368, 'Nhf5': 369, 'Bc1': 370, 'ab3': 371, 'ab6': 372, 'Kf2': 373, 'N2f3': 374, 'fe5': 375, 'Nde5': 376, 'Rf5': 377, 'Kf4': 378, 'Kf5': 379, 'Ke5': 380, 'Rf1': 381, 'Kd4': 382, 'Nba6': 383, 'Rf2': 384, 'Kg6': 385, 'f8=Q': 386, 'Nf7': 387, 'Rhd8': 388, 'Kc2': 389, 'Ra4': 390, 'Ra7': 391, 'Kd3': 392, 'Re5': 393, 'Kc4': 394, 'Kd5': 395, 'd2': 396, 'Kb8': 397, 'Ba7': 398, 'Ra2': 399, 'Rha1': 400, 'Ba1': 401, 'Qa1': 402, 'Kg3': 403, 'Nbc3': 404, 'N7e6': 405, 'Rg4': 406, 'Qf1': 407, 'ba4': 408, 'Bh7': 409, 'hg7': 410, 'Rdh1': 411, 'Bg1': 412, 'O-O-O+': 413, 'Ke8': 414, 'Rb6': 415, 'Qh8': 416, 'N6f5': 417, 'Rae8': 418, 'dxc8=Q': 419, 'Nf2': 420, 'Nb5': 421, 'Qb5': 422, 'Bf7': 423, 'Rb3': 424, 'Na2': 425, 'hg4': 426, 'Nge7': 427, 'cb3': 428, 'Rb4': 429, 'Nce2': 430, 'Rc2': 431, 'Nc1': 432, 'Rc4': 433, 'dc3': 434, 'Rdd8': 435, 'Ke4': 436, 'Kc8': 437, 'Ne1': 438, 'Kb7': 439, 'N8d7': 440, 'de6': 441, 'Rcc8': 442, 'Nh7': 443, 'N7b6': 444, 'Ncd5': 445, 'Rfa1': 446, 'R1a3': 447, 'ba5': 448, 'Rcc3': 449, 'Na1': 450, 'Rcb1': 451, 'Ngf4': 452, 'Qg8': 453, 'Rdc8': 454, 'R1c7': 455, 'Red2': 456, 'R6c7': 457, 'Kh6': 458, 'Rf3': 459, 'Ndf3': 460, 'N7c6': 461, 'Nbc6': 462, 'ed7': 463, 'Rce6': 464, 'Rad7': 465, 'R2e7': 466, 'N3d4': 467, 'Ndf5': 468, 'Ndf6': 469, 'Ka1': 470, 'Rbe1': 471, 'N4a6': 472, 'N3g5': 473, 'dc7': 474, 'Qe1': 475, 'Nbd4': 476, 'Rg5': 477, 'Rae4': 478, 'fg4': 479, 'Rce2': 480, 'Na7': 481, 'Nge4': 482, 'Nbd6': 483, 'Rcc7': 484, 'Ngh6': 485, 'N4f3': 486, 'Rbd1': 487, 'gf3': 488, 'N1c3': 489, 'Bh8': 490, 'Ke1': 491, 'Rae2': 492, 'Red3': 493, 'Nec3': 494, 'N6e5': 495, 'Rhc8': 496, 'Ka7': 497, 'Rec8': 498, 'Ka8': 499, 'Bb8': 500, 'Nb1': 501, 'Ka6': 502, 'Nfe4': 503, 'Nfe5': 504, 'N4a5': 505, 'Kb6': 506, 'Nfe7': 507, 'Rhd3': 508, 'c7': 509, 'Ng2': 510, 'Rbd8': 511, 'fg7': 512, 'b7': 513, 'Ncb8': 514, 'Ng1': 515, 'Ka2': 516, 'Kb3': 517, 'Rdg8': 518, 'Ref1': 519, 'Ndc5': 520, 'R1f2': 521, 'Rcd2': 522, 'R1d2': 523, 'Nfd2': 524, 'Ned6': 525, 'Rec1': 526, 'Raf8': 527, 'Nfd5': 528, 'Rgf7': 529, 'Rff8': 530, 'Ndf8': 531, 'Kh5': 532, 'Kg4': 533, 'Ncb5': 534, 'ba3': 535, 'Kg5': 536, 'Red1': 537, 'Nfe3': 538, 'R6b7': 539, 'Nh1': 540, 'Raf1': 541, 'R6f4': 542, 'Ree6': 543, 'Nce6': 544, 'Rff1': 545, 'Rhc1': 546, 'Nef5': 547, 'Rdf6': 548, 'R5c3': 549, 'Ndc6': 550, 'hg3': 551, 'N1e2': 552, 'Ngh5': 553, 'Reb8': 554, 'Rea4': 555, 'Nce7': 556, 'Rcb5': 557, 'R5b6': 558, 'Rbb7': 559, 'cxd8=Q': 560, 'd8=Q': 561, 'Nfg6': 562, 'N5c3': 563, 'Rdc1': 564, 'N1f3': 565, 'Rge8': 566, 'Rff2': 567, 'Nbc5': 568, 'Ncd6': 569, 'Ndc4': 570, 'Nfg5': 571, 'Nce5': 572, 'Kb5': 573, 'Ka5': 574, 'Kb4': 575, 'Kc5': 576, 'Nhf8': 577, 'de7': 578, 'e8=R': 579, 'Rdg1': 580, 'Nac7': 581, 'ab7': 582, 'Nec6': 583, 'R5f6': 584, 'N3g4': 585, 'N3d5': 586, 'Rdf8': 587, 'Rhf8': 588, 'cb2': 589, 'fe7': 590, 'bxa1=Q': 591, 'Rag8': 592, 'Rgc8': 593, 'R1d6': 594, 'de2': 595, 'R1c5': 596, 'Rcc1': 597, 'ef7': 598, 'Rec7': 599, 'Nde8': 600, 'Nfd6': 601, 'N7g6': 602, 'ba7': 603, 'a8=R': 604, 'Rea1': 605, 'Rbb4': 606, 'Rgg8': 607, 'Rgc1': 608, 'R5d2': 609, 'R3d2': 610, 'Rff3': 611, 'Rca8': 612, 'Rcc6': 613, 'Qh1': 614, 'Nfd4': 615, 'Rce1': 616, 'Rce8': 617, 'R6e7': 618, 'Nge6': 619, 'Kh4': 620, 'R1e2': 621, 'Rce3': 622, 'R1c2': 623, 'Nde6': 624, 'Rce4': 625, 'Reb3': 626, 'Rda5': 627, 'R5a7': 628, 'Ned7': 629, 'Ned5': 630, 'Nab5': 631, 'Rdb8': 632, 'Raa7': 633, 'fg2': 634, 'Ncb6': 635, 'c2': 636, 'cxb1=Q': 637, 'gh7': 638, 'N3d2': 639, 'Bh1': 640, 'N2g3': 641, 'Nhf6': 642, 'gf7': 643, 'fxe8=Q': 644, 'Nec8': 645, 'R1d7': 646, 'Rcg1': 647, 'R1g7': 648, 'Nge5': 649, 'N3e2': 650, 'Nac4': 651, 'Nbc4': 652, 'Rde4': 653, 'ab2': 654, 'a2': 655, 'Kh3': 656, 'Nfg8': 657, 'Neg4': 658, 'Ndf4': 659, 'Ncd7': 660, 'Nef3': 661, 'Rfc7': 662, 'Rfc5': 663, 'N7f6': 664, 'Ree3': 665, 'Nhg6': 666, 'N5f6': 667, 'Ngh7': 668, 'R7e6': 669, 'Rfe7': 670, 'Ree7': 671, 'Rda1': 672, 'Ngf5': 673, 'Rdd3': 674, 'Rda3': 675, 'N2b3': 676, 'Rec2': 677, 'Rdd1': 678, 'N1h2': 679, 'Rgb8': 680, 'Rca1': 681, 'Rdg7': 682, 'Kc1': 683, 'Rdh8': 684, 'R4h6': 685, 'Ncb1': 686, 'dc2': 687, 'gf2': 688, 'Rde8': 689, 'Rgf3': 690, 'R5d4': 691, 'cb7': 692, 'ba2': 693, 'Ree8': 694, 'Rcd3': 695, 'Rbe7': 696, 'Rcb8': 697, 'Ree2': 698, 'Rdd6': 699, 'Rbb8': 700, 'a7': 701, 'c8=Q': 702, 'N5f3': 703, 'Reb1': 704, 'bc2': 705, 'N1d2': 706, 'R1e3': 707, 'Ned3': 708, 'Rfg3': 709, 'e2': 710, 'e1=Q': 711, 'Rdd2': 712, 'Raa8': 713, 'de3': 714, 'b8=Q': 715, 'Rcd4': 716, 'Rag1': 717, 'R5a2': 718, 'd8=R': 719, 'h7': 720, 'Raf7': 721, 'Nab4': 722, 'Nfg4': 723, 'Rah8': 724, 'Rhc7': 725, 'Rhg3': 726, 'R1g2': 727, 'Nh8': 728, 'Ned4': 729, 'N1a2': 730, 'R1e5': 731, 'O-O+': 732, 'Neg5': 733, 'Nbc7': 734, 'Rdh2': 735, 'R2h4': 736, 'R2b7': 737, 'Nde4': 738, 'Rbb1': 739, 'Nba5': 740, 'N8e6': 741, 'Nfe6': 742, 'Raa1': 743, 'N4c6': 744, 'Rcc5': 745, 'Rah1': 746, 'Ndf1': 747, 'N1g3': 748, 'N5e4': 749, 'Reg1': 750, 'hg2': 751, 'Rgd8': 752, 'Ree1': 753, 'Rcb3': 754, 'R3b4': 755, 'Rba8': 756, 'Rbe3': 757, 'Reg4': 758, 'Rhb8': 759, 'Nhg4': 760, 'N8c6': 761, 'Rcb7': 762, 'a8=Q': 763, 'R8c3': 764, 'Ngh2': 765, 'Nac6': 766, 'N7d5': 767, 'Rbf8': 768, 'R7g3': 769, 'Rhh1': 770, 'Rca4': 771, 'Rda7': 772, 'Rfa8': 773, 'Rag2': 774, 'N6d7': 775, 'R3d4': 776, 'R4d7': 777, 'g7': 778, 'dxc8=N': 779, 'Raf3': 780, 'Rcf1': 781, 'ef2': 782, 'Ncd2': 783, 'R2e5': 784, 'R8d4': 785, 'exd8=B': 786, 'R4d2': 787, 'Ned2': 788, 'Rfd2': 789, 'b2': 790, 'Rdc7': 791, 'Rae7': 792, 'bc7': 793, 'Rea2': 794, 'R5a3': 795, 'Rfd7': 796, 'R4c3': 797, 'N5d4': 798, 'Rbf7': 799, 'Rhg7': 800, 'Rcd7': 801, 'N5a4': 802, 'Rge1': 803, 'Rba1': 804, 'N7c5': 805, 'R6b5': 806, 'R1b2': 807, 'Raa6': 808, 'Rcb6': 809, 'R4e2': 810, 'Rcc4': 811, 'Rdf5': 812, 'R4g6': 813, 'Neg3': 814, 'Ncb4': 815, 'N8f6': 816, 'N4c3': 817, 'Rde7': 818, 'R5d3': 819, 'Nhf4': 820, 'Nhg2': 821, 'cd2': 822, 'N7b5': 823, 'Raa5': 824, 'Ref2': 825, 'Ncd4': 826, 'Nba3': 827, 'g2': 828, 'Rbg1': 829, 'Rbh8': 830, 'Nec5': 831, 'cd7': 832, 'Rea7': 833, 'Nac1': 834, 'Rhh5': 835, 'Rhf5': 836, 'Rae6': 837, 'Ndb4': 838, 'e8=Q': 839, 'e8=N': 840, 'Ree5': 841, 'R7f5': 842, 'Ka3': 843, 'Ngf2': 844, 'N2e4': 845, 'Rbd5': 846, 'Rcc2': 847, 'f2': 848, 'Nhf7': 849, 'exf8=Q': 850, 'Rcf8': 851, 'Nef2': 852, 'R6e2': 853, 'N6g5': 854, 'Rea8': 855, 'Rbc6': 856, 'R8c7': 857, 'R1g4': 858, 'Rah2': 859, 'R4e3': 860, 'Nge3': 861, 'R3c6': 862, 'Rgd1': 863, 'Nhg3': 864, 'R4f2': 865, 'Ndc3': 866, 'R8e6': 867, 'R5c4': 868, 'Ndb6': 869, 'Neg6': 870, 'R3b2': 871, 'R8d5': 872, 'Reh5': 873, 'Rhb1': 874, 'c1=Q': 875, 'Rhf6': 876, 'Rgg7': 877, 'N5c6': 878, 'R8c5': 879, 'R1b3': 880, 'fe2': 881, 'Reg7': 882, 'Nac8': 883, 'R4f5': 884, 'Rfe4': 885, 'Rcg8': 886, 'N5g6': 887, 'Rgg2': 888, 'Rgd2': 889, 'Rhd2': 890, 'Rac6': 891, 'gh2': 892, 'Rag3': 893, 'Nbc1': 894, 'R1f3': 895, 'Rdd5': 896, 'Rec3': 897, 'Rbd3': 898, 'Rad2': 899, 'Ncd3': 900, 'Nfe1': 901, 'N7a6': 902, 'Rfd6': 903, 'N6a5': 904, 'N6h5': 905, 'h2': 906, 'Ngf7': 907, 'Rba7': 908, 'Rac2': 909, 'R1d4': 910, 'Rde2': 911, 'R8f6': 912, 'Reg8': 913, 'Rfg2': 914, 'Rga7': 915, 'R1a7': 916, 'Reh1': 917, 'Rbb2': 918, 'Rdf2': 919, 'Red7': 920, 'R8f7': 921, 'Rgd6': 922, 'Rhf7': 923, 'Rhh7': 924, 'N2f4': 925, 'Rhh8': 926, 'R8e7': 927, 'R6d2': 928, 'Rae3': 929, 'Nfd3': 930, 'N8h7': 931, 'R4e6': 932, 'R1c6': 933, 'Ref8': 934, 'Rcd6': 935, 'R5e7': 936, 'Rac7': 937, 'Nfe8': 938, 'R4a2': 939, 'Nac5': 940, 'Rde3': 941, 'Rca5': 942, 'Rdh7': 943, 'N5e6': 944, 'Nac2': 945, 'Ncd1': 946, 'Rfd4': 947, 'R4d5': 948, 'R3d5': 949, 'R4c5': 950, 'Rff7': 951, 'Rdc2': 952, 'Neg1': 953, 'N2c4': 954, 'Nab1': 955, 'Rhe3': 956, 'Raa2': 957, 'Rbe6': 958, 'Rbb6': 959, 'Ndb3': 960, 'Rhc4': 961, 'g8=Q': 962, 'R2c7': 963, 'Rff6': 964, 'b1=Q': 965, 'R2f6': 966, 'Rab3': 967, 'Rcf6': 968, 'Nhf3': 969, 'fxe1=Q': 970, 'Nhf1': 971, 'N3h2': 972, 'R5a4': 973, 'Rbd7': 974, 'Rac4': 975, 'Rca7': 976, 'fxe8=N': 977, 'Ndc2': 978, 'Rdc3': 979, 'Rec4': 980, 'Rfe5': 981, 'Rdc5': 982, 'Rga8': 983, 'Rab7': 984, 'Nec7': 985, 'Rhd6': 986, 'Rff4': 987, 'Ref6': 988, 'Rgf6': 989, 'R6f7': 990, 'Red5': 991, 'Rba4': 992, 'Rfb2': 993, 'Rfh7': 994, 'Nce8': 995, 'Rad6': 996, 'N7e5': 997, 'Nbd3': 998, 'Nac3': 999, 'Rbf1': 1000, 'R1c3': 1001, 'g1=Q': 1002, 'Rcf2': 1003, 'Rdg5': 1004, 'R6e4': 1005, 'Nca2': 1006, 'R5b3': 1007, 'Nde3': 1008, 'Nbc8': 1009, 'Rcd5': 1010, 'R5f4': 1011, 'N3e4': 1012, 'Rfh1': 1013, 'cxd1=Q': 1014, 'Rgg1': 1015, 'Rda6': 1016, 'Rbc7': 1017, 'Nab8': 1018, 'R1f6': 1019, 'R7d6': 1020, 'Rgd5': 1021, 'Rbe5': 1022, 'R5e2': 1023, 'R1e4': 1024, 'Rfd3': 1025, 'R1f7': 1026, 'R5d7': 1027, 'Nef7': 1028, 'R6d3': 1029, 'Rhd4': 1030, 'Rgg3': 1031, 'Nfe2': 1032, 'Reb2': 1033, 'Ka4': 1034, 'Rce5': 1035, 'Rhf4': 1036, 'R4f7': 1037, 'Nde1': 1038, 'hxg8=Q': 1039, 'Rdd4': 1040, 'Neg8': 1041, 'Rda2': 1042, 'N4e5': 1043, 'Nfh5': 1044, 'R7a6': 1045, 'Rbc2': 1046, 'Nce1': 1047, 'Rad3': 1048, 'Nfg3': 1049, 'Rbd2': 1050, 'Rgf8': 1051, 'R3c2': 1052, 'R4e5': 1053, 'gxf8=Q': 1054, 'N3e5': 1055, 'Ndf7': 1056, 'Rgh8': 1057, 'R5h7': 1058, 'R6g7': 1059, 'Rdb2': 1060, 'Ndc8': 1061, 'R5c6': 1062, 'Rfg8': 1063, 'R2c5': 1064, 'Rhe4': 1065, 'Rdg4': 1066, 'R8g7': 1067, 'Rhc2': 1068, 'N4b3': 1069, 'Reh8': 1070, 'R8b7': 1071, 'Ngh3': 1072, 'Rde6': 1073, 'Ndf2': 1074, 'Rhe6': 1075, 'Nca5': 1076, 'N8a6': 1077, 'Rce7': 1078, 'Rfh8': 1079, 'R5f3': 1080, 'N4d5': 1081, 'R3f2': 1082, 'Ngh4': 1083, 'Ncb3': 1084, 'R8a6': 1085, 'R2a3': 1086, 'R4a7': 1087, 'Reg6': 1088, 'R3e5': 1089, 'Nba8': 1090, 'Rba6': 1091, 'N2c3': 1092, 'Rbd4': 1093, 'Rba5': 1094, 'R7a3': 1095, 'bxc8=Q': 1096, 'Rda8': 1097, 'Reb7': 1098, 'R8a7': 1099, 'R4b5': 1100, 'Reh4': 1101, 'N6h7': 1102, 'Nba4': 1103, 'R8e5': 1104, 'Nab6': 1105, 'Rgd3': 1106, 'R2d5': 1107, 'R4c6': 1108, 'Rfg4': 1109, 'Nbc2': 1110, 'Neg2': 1111, 'N8e7': 1112, 'Rdh5': 1113, 'R8e2': 1114, 'Rgg6': 1115, 'Rae5': 1116, 'Rec6': 1117, 'R5f7': 1118, 'Ndb1': 1119, 'Nba7': 1120, 'dxe8=Q': 1121, 'N4g5': 1122, 'Rfe2': 1123, 'Rdb7': 1124, 'R1d3': 1125, 'Rbe4': 1126, 'Red4': 1127, 'N3a2': 1128, 'R2c3': 1129, 'Ndc7': 1130, 'R3d7': 1131, 'R8d3': 1132, 'R3e2': 1133, 'Rdc4': 1134, 'Rch1': 1135, 'Rhh6': 1136, 'Ref7': 1137, 'Qcc1': 1138, 'Rdf4': 1139, 'N6b5': 1140, 'Ree4': 1141, 'Raa4': 1142, 'Nab2': 1143, 'Rdc6': 1144, 'R8d7': 1145, 'Nhg5': 1146, 'Neg7': 1147, 'Rgb1': 1148, 'Rbe2': 1149, 'Nhg7': 1150, 'N6d5': 1151, 'ed2': 1152, 'R6d5': 1153, 'N6e7': 1154, 'N4g3': 1155, 'Rgd4': 1156, 'Rhh3': 1157, 'Rbg8': 1158, 'R1h3': 1159, 'Rbc3': 1160, 'Rdg3': 1161, 'R2f5': 1162, 'Nge8': 1163, 'N6d4': 1164, 'Rhc6': 1165, 'Rhc5': 1166, 'Rde5': 1167, 'Rfg1': 1168, 'N8f7': 1169, 'Rgh5': 1170, 'Ref3': 1171, 'R6a7': 1172, 'Rcg2': 1173, 'R8g6': 1174, 'Rdf3': 1175, 'Rhh4': 1176, 'Nfd8': 1177, 'Rfc2': 1178, 'Rad5': 1179, 'Ndb8': 1180, 'Reg2': 1181, 'Ncb7': 1182, 'R8d6': 1183, 'R4g2': 1184, 'Rfa7': 1185, 'R5b2': 1186, 'bxa8=Q': 1187, 'N4f5': 1188, 'R1e7': 1189, 'N3c5': 1190, 'R1e6': 1191, 'Ncd8': 1192, 'N4c5': 1193, 'R1a2': 1194, 'N6b4': 1195, 'f8=N': 1196, 'Ned1': 1197, 'Rdb3': 1198, 'R7b6': 1199, 'R8b6': 1200, 'Rab6': 1201, 'R7d5': 1202, 'R1h4': 1203, 'Rgc7': 1204, 'Rhg6': 1205, 'R7f2': 1206, 'R1a5': 1207, 'Nca6': 1208, 'h1=Q': 1209, 'Nca3': 1210, 'R7e3': 1211, 'Rac3': 1212, 'R6d7': 1213, 'Rhf3': 1214, 'N6g4': 1215, 'N6c5': 1216, 'Rbb5': 1217, 'R4e7': 1218, 'Rgg5': 1219, 'Rge7': 1220, 'Rba3': 1221, 'R6c4': 1222, 'Rba2': 1223, 'Rbd6': 1224, 'Rfh6': 1225, 'Reg3': 1226, 'R7f3': 1227, 'Rcf3': 1228, 'R5e6': 1229, 'Rbc5': 1230, 'R1c4': 1231, 'Rfd5': 1232, 'Nbd8': 1233, 'Rfe6': 1234, 'R3a5': 1235, 'N8a7': 1236, 'R8c6': 1237, 'R8a4': 1238, 'Rga1': 1239, 'Ngf1': 1240, 'R2d3': 1241, 'Rfe3': 1242, 'cxb8=Q': 1243, 'Nfh3': 1244, 'Rgh3': 1245, 'Rgf2': 1246, 'Rfh2': 1247, 'R7b3': 1248, 'R4c2': 1249, 'N3c4': 1250, 'Ned8': 1251, 'R2f3': 1252, 'Rgh1': 1253, 'R1h2': 1254, 'R2e4': 1255, 'Rhh2': 1256, 'Rhg2': 1257, 'R6d4': 1258, 'Rdb6': 1259, 'Red6': 1260, 'Rha5': 1261, 'R5c7': 1262, 'Rhg5': 1263, 'Rhb2': 1264, 'R8b2': 1265, 'Nca4': 1266, 'R4d6': 1267, 'Rfb3': 1268, 'Rea3': 1269, 'N3a4': 1270, 'f8=R': 1271, 'Nab3': 1272, 'R3e7': 1273, 'R3c5': 1274, 'R5g3': 1275, 'h8=Q': 1276, 'N3f5': 1277, 'R2b3': 1278, 'Rfc6': 1279, 'Ref4': 1280, 'Rgh4': 1281, 'R1h6': 1282, 'R4c7': 1283, 'R1a6': 1284, 'Rea5': 1285, 'R8c2': 1286, 'Rdg2': 1287, 'R1d5': 1288, 'Rcg7': 1289, 'Rag7': 1290, 'exd8=Q': 1291, 'fxg8=Q': 1292, 'Q1c2': 1293, 'R8e3': 1294, 'Nfg7': 1295, 'Rbf3': 1296, 'Rea6': 1297, 'Rab2': 1298, 'Rbb3': 1299, 'Rhe5': 1300, 'Rcf7': 1301, 'Rab5': 1302, 'R8h4': 1303, 'Raf6': 1304, 'R7e4': 1305, 'R5e4': 1306, 'Rbf2': 1307, 'R6f5': 1308, 'Nec2': 1309, 'Rca2': 1310, 'R6e5': 1311, 'R8c4': 1312, 'R5f2': 1313, 'Rfh3': 1314, 'Nhg8': 1315, 'Rcb4': 1316, 'R8f4': 1317, 'Rac5': 1318, 'R7f4': 1319, 'N2e3': 1320, 'R4a3': 1321, 'Rad4': 1322, 'R8a5': 1323, 'Nfd1': 1324, 'R4h3': 1325, 'Rgf5': 1326, 'Rhb3': 1327, 'R1f5': 1328, 'N8g7': 1329, 'N3b4': 1330, 'N8b7': 1331, 'R2d6': 1332, 'R8d2': 1333, 'N1b2': 1334, 'bxc1=Q': 1335, 'R7c4': 1336, 'R3g4': 1337, 'R1a4': 1338, 'Rgb5': 1339, 'a1=Q': 1340, 'Nfh6': 1341, 'N5h6': 1342, 'Rff5': 1343, 'R7e5': 1344, 'R8a3': 1345, 'Rgg4': 1346, 'd1=Q': 1347, 'R1f4': 1348, 'N3b5': 1349, 'R5c2': 1350, 'N4h5': 1351, 'Rdb5': 1352, 'R7g5': 1353, 'Rhg4': 1354, 'R6b2': 1355, 'Rge3': 1356, 'R7h5': 1357, 'Nba2': 1358, 'Ncb2': 1359, 'N8c7': 1360, 'Rge5': 1361, 'Nca1': 1362, 'Rfa4': 1363, 'Nba1': 1364, 'Rfg6': 1365, 'N6c7': 1366, 'Rfc3': 1367, 'Rbh2': 1368, 'R8f3': 1369, 'Rfb7': 1370, 'Rch3': 1371, 'Rca6': 1372, 'Rch4': 1373, 'R7h4': 1374, 'Nab7': 1375, 'R7c3': 1376, 'Rge2': 1377, 'Rab4': 1378, 'R8e4': 1379, 'N4b5': 1380, 'R5g7': 1381, 'R4f3': 1382, 'Rah3': 1383, 'Rge4': 1384, 'R2a5': 1385, 'Reg5': 1386, 'Rcg5': 1387, 'Rbc4': 1388, 'Raf5': 1389, 'R2d4': 1390, 'Rhd5': 1391, 'Nhf2': 1392, 'R3a2': 1393, 'R5d6': 1394, 'R2d7': 1395, 'Nef8': 1396, 'Ndb7': 1397, 'Ref5': 1398, 'N2d4': 1399, 'R7a2': 1400, 'Nfh2': 1401, 'N4e6': 1402, 'R4g3': 1403, 'N6c4': 1404, 'N4d6': 1405, 'Rgb4': 1406, 'R7b5': 1407, 'Rhc3': 1408, 'N5h4': 1409, 'R1h7': 1410, 'Rdf7': 1411, 'Raf2': 1412, 'exd1=Q': 1413, 'Rch8': 1414, 'Nef1': 1415, 'N1c2': 1416, 'N5e3': 1417, 'Qeg5': 1418, 'R8h6': 1419, 'Rcf5': 1420, 'fill': 1421}\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["{'d4': 0, 'd5': 1, 'c4': 2, 'e6': 3, 'Nc3': 4, 'Nf6': 5, 'cd5': 6, 'ed5': 7, 'Bg5': 8, 'Be7': 9, 'e3': 10, 'Ne4': 11, 'Bd8': 12, 'Nd1': 13, 'Bc7': 14, 'Nb2': 15, 'Rb1': 16, 'Nc4': 17, 'Bc4': 18, 'dc4': 19, 'Ne2': 20, 'O-O': 21, 'b6': 22, 'Na6': 23, 'Bd6': 24, 'Rd8': 25, 'Ba3': 26, 'Bb7': 27, 'e4': 28, 'f6': 29, 'Ke2': 30, 'Nc7': 31, 'Rhd1': 32, 'Ba6': 33, 'Ke3': 34, 'Kf7': 35, 'g4': 36, 'g5': 37, 'h4': 38, 'h6': 39, 'Rh1': 40, 'Re8': 41, 'f3': 42, 'hg5': 43, 'fg5': 44, 'd6': 45, 'Nd5': 46, 'Bd5': 47, 'Rh6': 48, 'c3': 49, 'd7': 50, 'Re6': 51, 'Rh7': 52, 'Kg8': 53, 'Rbh1': 54, 'Bc6': 55, 'Rh8': 56, 'Ra8': 57, 'Bd7': 58, 'Qd5': 59, 'Qa5': 60, 'Nf3': 61, 'c6': 62, 'Ne5': 63, 'Bf5': 64, 'Be4': 65, 'a3': 66, 'Nbd7': 67, 'Be3': 68, 'de5': 69, 'Ng4': 70, 'Bd4': 71, 'b4': 72, 'Qd8': 73, 'Ne3': 74, 'dc6': 75, 'bc6': 76, 'Rd1': 77, 'Rab8': 78, 'Rc1': 79, 'Rfd8': 80, 'c5': 81, 'Bc5': 82, 'bc5': 83, 'g6': 84, 'Rb2': 85, 'Rd2': 86, 'e5': 87, 'Nc6': 88, 'd3': 89, 'Bb3': 90, 'Nbd2': 91, 'Be6': 92, 'Qd7': 93, 'Re1': 94, 'Rfe8': 95, 'Nf1': 96, 'Ne7': 97, 'Ng3': 98, 'Bg4': 99, 'h3': 100, 'Kh8': 101, 'Bf6': 102, 'gf6': 103, 'ed4': 104, 'cd4': 105, 'Bb4': 106, 'Re3': 107, 'Rg8': 108, 'Bh3': 109, 'Qd4': 110, 'Rg6': 111, 'Qb4': 112, 'Qc3': 113, 'Bc2': 114, 'Nh2': 115, 'b5': 116, 'Rc8': 117, 'Bd3': 118, 'Bh5': 119, 'Nh5': 120, 'Rh5': 121, 'Qf6': 122, 'Bd1': 123, 'Qf3': 124, 'Rad1': 125, 'Nf8': 126, 'Ng6': 127, 'Qc7': 128, 'Qe7': 129, 'h5': 130, 'gh6': 131, 'Qf4': 132, 'Qh6': 133, 'f5': 134, 'ef6': 135, 'Qf7': 136, 'Rg3': 137, 'Rg7': 138, 'Nd4': 139, 'Bd2': 140, 'Nc2': 141, 'Bc3': 142, 'Qg5': 143, 'Qe2': 144, 'Qg2': 145, 'O-O-O': 146, 'Qe4': 147, 'Rhg1': 148, 'f4': 149, 'Rgf1': 150, 'Qh4': 151, 'Be1': 152, 'Qa4': 153, 'Rf6': 154, 'Qg4': 155, 'Qa2': 156, 'Bg6': 157, 'hg6': 158, 'fg6': 159, 'Qg6': 160, 'Qh5': 161, 'g3': 162, 'Bg7': 163, 'Bg2': 164, 'Nfd7': 165, 'b3': 166, 'Qc2': 167, 'Qd2': 168, 'a6': 169, 'Nde2': 170, 'Qg7': 171, 'Nef4': 172, 'Ne6': 173, 'Rcd8': 174, 'Qf2': 175, 'Bc8': 176, 'ef5': 177, 'cd6': 178, 'Qb6': 179, 'Ng8': 180, 'Bb2': 181, 'Rad8': 182, 'Qd6': 183, 'dc5': 184, 'Qc5': 185, 'Rfd1': 186, 'a5': 187, 'Qd1': 188, 'Nd7': 189, 'Kg2': 190, 'Kf8': 191, 'Kf3': 192, 'Ke7': 193, 'Bb5': 194, 'Kd6': 195, 'Na4': 196, 'gh4': 197, 'Rh4': 198, 'de4': 199, 'Re4': 200, 'Rh2': 201, 'fe6': 202, 'Qd3': 203, 'Qf5': 204, 'Rd5': 205, 'Re7': 206, 'Ndb5': 207, 'Na3': 208, 'a4': 209, 'ab5': 210, 'Qa8': 211, 'Nce3': 212, 'Qb7': 213, 'Rc5': 214, 'Qe5': 215, 'Qg1': 216, 'Qg3': 217, 'fg3': 218, 'bc3': 219, 'Rc3': 220, 'Be2': 221, 'ef3': 222, 'Bf3': 223, 'Qb3': 224, 'Nde7': 225, 'Na5': 226, 'Nec4': 227, 'Rfe1': 228, 'Qb2': 229, 'Bb1': 230, 'Kh1': 231, 'e7': 232, 'Rfb8': 233, 'Nc5': 234, 'Nh3': 235, 'Be5': 236, 'Rac1': 237, 'Nce4': 238, 'Nf4': 239, 'Nd3': 240, 'Rd3': 241, 'Qc4': 242, 'Rf7': 243, 'Rcd1': 244, 'Rd7': 245, 'Qe6': 246, 'ef4': 247, 'gf4': 248, 'Ke6': 249, 'Bf4': 250, 'Ba8': 251, 'Qc8': 252, 'Na8': 253, 'Nd2': 254, 'Ngf3': 255, 'Bf1': 256, 'gh5': 257, 'Rg1': 258, 'Rd4': 259, 'Qc6': 260, 'Bh6': 261, 'Red8': 262, 'Nb3': 263, 'Nb7': 264, 'Qe8': 265, 'Ng5': 266, 'Bf8': 267, 'Qa6': 268, 'Rdd7': 269, 'Ngf6': 270, 'Qa3': 271, 'Ne8': 272, 'Ba4': 273, 'Nh6': 274, 'Qc1': 275, 'Bb6': 276, 'Ba2': 277, 'Nd8': 278, 'Nf5': 279, 'ed3': 280, 'Nb6': 281, 'cb6': 282, 'Rhe8': 283, 'Nb8': 284, 'cb4': 285, 'ba6': 286, 'Rfb1': 287, 'Rb7': 288, 'gf5': 289, 'Kh2': 290, 'Qh7': 291, 'Nge2': 292, 'Kd1': 293, 'Kd2': 294, 'Kc3': 295, 'Rhf1': 296, 'Kb2': 297, 'Rae1': 298, 'Kb1': 299, 'Rh3': 300, 'Qh2': 301, 'fe4': 302, 'Kh7': 303, 'Rde1': 304, 'Bg8': 305, 'Kg7': 306, 'Qe3': 307, 'Kf6': 308, 'Kd8': 309, 'Qf8': 310, 'Kc7': 311, 'Nb4': 312, 'N3h4': 313, 'Ra3': 314, 'Bh4': 315, 'Rac8': 316, 'fe3': 317, 'Rd6': 318, 'Ra6': 319, 'Nef6': 320, 'Qb1': 321, 'Ngf8': 322, 'Be8': 323, 'Nc8': 324, 'Re2': 325, 'f7': 326, 'ed6': 327, 'Kd7': 328, 'Rab1': 329, 'Rbc1': 330, 'Qa7': 331, 'Kc6': 332, 'Qb8': 333, 'Nd6': 334, 'N5b6': 335, 'Kf1': 336, 'Rb8': 337, 'Ba5': 338, 'Rdf1': 339, 'Rhg8': 340, 'Nbd5': 341, 'Bh2': 342, 'Rg2': 343, 'Bf2': 344, 'Bg3': 345, 'Rc6': 346, 'Ra1': 347, 'Ra5': 348, 'Rb5': 349, 'Rf4': 350, 'Rc7': 351, 'Rf8': 352, 'Rfc1': 353, 'cb5': 354, 'Rbe8': 355, 'Rhe1': 356, 'Ng7': 357, 'cd3': 358, 'ab4': 359, 'Rfc8': 360, 'bc4': 361, 'Rdb1': 362, 'Rbc8': 363, 'R3c7': 364, 'gh3': 365, 'Qh3': 366, 'Kg1': 367, 'Nh4': 368, 'Nhf5': 369, 'Bc1': 370, 'ab3': 371, 'ab6': 372, 'Kf2': 373, 'N2f3': 374, 'fe5': 375, 'Nde5': 376, 'Rf5': 377, 'Kf4': 378, 'Kf5': 379, 'Ke5': 380, 'Rf1': 381, 'Kd4': 382, 'Nba6': 383, 'Rf2': 384, 'Kg6': 385, 'f8=Q': 386, 'Nf7': 387, 'Rhd8': 388, 'Kc2': 389, 'Ra4': 390, 'Ra7': 391, 'Kd3': 392, 'Re5': 393, 'Kc4': 394, 'Kd5': 395, 'd2': 396, 'Kb8': 397, 'Ba7': 398, 'Ra2': 399, 'Rha1': 400, 'Ba1': 401, 'Qa1': 402, 'Kg3': 403, 'Nbc3': 404, 'N7e6': 405, 'Rg4': 406, 'Qf1': 407, 'ba4': 408, 'Bh7': 409, 'hg7': 410, 'Rdh1': 411, 'Bg1': 412, 'O-O-O+': 413, 'Ke8': 414, 'Rb6': 415, 'Qh8': 416, 'N6f5': 417, 'Rae8': 418, 'dxc8=Q': 419, 'Nf2': 420, 'Nb5': 421, 'Qb5': 422, 'Bf7': 423, 'Rb3': 424, 'Na2': 425, 'hg4': 426, 'Nge7': 427, 'cb3': 428, 'Rb4': 429, 'Nce2': 430, 'Rc2': 431, 'Nc1': 432, 'Rc4': 433, 'dc3': 434, 'Rdd8': 435, 'Ke4': 436, 'Kc8': 437, 'Ne1': 438, 'Kb7': 439, 'N8d7': 440, 'de6': 441, 'Rcc8': 442, 'Nh7': 443, 'N7b6': 444, 'Ncd5': 445, 'Rfa1': 446, 'R1a3': 447, 'ba5': 448, 'Rcc3': 449, 'Na1': 450, 'Rcb1': 451, 'Ngf4': 452, 'Qg8': 453, 'Rdc8': 454, 'R1c7': 455, 'Red2': 456, 'R6c7': 457, 'Kh6': 458, 'Rf3': 459, 'Ndf3': 460, 'N7c6': 461, 'Nbc6': 462, 'ed7': 463, 'Rce6': 464, 'Rad7': 465, 'R2e7': 466, 'N3d4': 467, 'Ndf5': 468, 'Ndf6': 469, 'Ka1': 470, 'Rbe1': 471, 'N4a6': 472, 'N3g5': 473, 'dc7': 474, 'Qe1': 475, 'Nbd4': 476, 'Rg5': 477, 'Rae4': 478, 'fg4': 479, 'Rce2': 480, 'Na7': 481, 'Nge4': 482, 'Nbd6': 483, 'Rcc7': 484, 'Ngh6': 485, 'N4f3': 486, 'Rbd1': 487, 'gf3': 488, 'N1c3': 489, 'Bh8': 490, 'Ke1': 491, 'Rae2': 492, 'Red3': 493, 'Nec3': 494, 'N6e5': 495, 'Rhc8': 496, 'Ka7': 497, 'Rec8': 498, 'Ka8': 499, 'Bb8': 500, 'Nb1': 501, 'Ka6': 502, 'Nfe4': 503, 'Nfe5': 504, 'N4a5': 505, 'Kb6': 506, 'Nfe7': 507, 'Rhd3': 508, 'c7': 509, 'Ng2': 510, 'Rbd8': 511, 'fg7': 512, 'b7': 513, 'Ncb8': 514, 'Ng1': 515, 'Ka2': 516, 'Kb3': 517, 'Rdg8': 518, 'Ref1': 519, 'Ndc5': 520, 'R1f2': 521, 'Rcd2': 522, 'R1d2': 523, 'Nfd2': 524, 'Ned6': 525, 'Rec1': 526, 'Raf8': 527, 'Nfd5': 528, 'Rgf7': 529, 'Rff8': 530, 'Ndf8': 531, 'Kh5': 532, 'Kg4': 533, 'Ncb5': 534, 'ba3': 535, 'Kg5': 536, 'Red1': 537, 'Nfe3': 538, 'R6b7': 539, 'Nh1': 540, 'Raf1': 541, 'R6f4': 542, 'Ree6': 543, 'Nce6': 544, 'Rff1': 545, 'Rhc1': 546, 'Nef5': 547, 'Rdf6': 548, 'R5c3': 549, 'Ndc6': 550, 'hg3': 551, 'N1e2': 552, 'Ngh5': 553, 'Reb8': 554, 'Rea4': 555, 'Nce7': 556, 'Rcb5': 557, 'R5b6': 558, 'Rbb7': 559, 'cxd8=Q': 560, 'd8=Q': 561, 'Nfg6': 562, 'N5c3': 563, 'Rdc1': 564, 'N1f3': 565, 'Rge8': 566, 'Rff2': 567, 'Nbc5': 568, 'Ncd6': 569, 'Ndc4': 570, 'Nfg5': 571, 'Nce5': 572, 'Kb5': 573, 'Ka5': 574, 'Kb4': 575, 'Kc5': 576, 'Nhf8': 577, 'de7': 578, 'e8=R': 579, 'Rdg1': 580, 'Nac7': 581, 'ab7': 582, 'Nec6': 583, 'R5f6': 584, 'N3g4': 585, 'N3d5': 586, 'Rdf8': 587, 'Rhf8': 588, 'cb2': 589, 'fe7': 590, 'bxa1=Q': 591, 'Rag8': 592, 'Rgc8': 593, 'R1d6': 594, 'de2': 595, 'R1c5': 596, 'Rcc1': 597, 'ef7': 598, 'Rec7': 599, 'Nde8': 600, 'Nfd6': 601, 'N7g6': 602, 'ba7': 603, 'a8=R': 604, 'Rea1': 605, 'Rbb4': 606, 'Rgg8': 607, 'Rgc1': 608, 'R5d2': 609, 'R3d2': 610, 'Rff3': 611, 'Rca8': 612, 'Rcc6': 613, 'Qh1': 614, 'Nfd4': 615, 'Rce1': 616, 'Rce8': 617, 'R6e7': 618, 'Nge6': 619, 'Kh4': 620, 'R1e2': 621, 'Rce3': 622, 'R1c2': 623, 'Nde6': 624, 'Rce4': 625, 'Reb3': 626, 'Rda5': 627, 'R5a7': 628, 'Ned7': 629, 'Ned5': 630, 'Nab5': 631, 'Rdb8': 632, 'Raa7': 633, 'fg2': 634, 'Ncb6': 635, 'c2': 636, 'cxb1=Q': 637, 'gh7': 638, 'N3d2': 639, 'Bh1': 640, 'N2g3': 641, 'Nhf6': 642, 'gf7': 643, 'fxe8=Q': 644, 'Nec8': 645, 'R1d7': 646, 'Rcg1': 647, 'R1g7': 648, 'Nge5': 649, 'N3e2': 650, 'Nac4': 651, 'Nbc4': 652, 'Rde4': 653, 'ab2': 654, 'a2': 655, 'Kh3': 656, 'Nfg8': 657, 'Neg4': 658, 'Ndf4': 659, 'Ncd7': 660, 'Nef3': 661, 'Rfc7': 662, 'Rfc5': 663, 'N7f6': 664, 'Ree3': 665, 'Nhg6': 666, 'N5f6': 667, 'Ngh7': 668, 'R7e6': 669, 'Rfe7': 670, 'Ree7': 671, 'Rda1': 672, 'Ngf5': 673, 'Rdd3': 674, 'Rda3': 675, 'N2b3': 676, 'Rec2': 677, 'Rdd1': 678, 'N1h2': 679, 'Rgb8': 680, 'Rca1': 681, 'Rdg7': 682, 'Kc1': 683, 'Rdh8': 684, 'R4h6': 685, 'Ncb1': 686, 'dc2': 687, 'gf2': 688, 'Rde8': 689, 'Rgf3': 690, 'R5d4': 691, 'cb7': 692, 'ba2': 693, 'Ree8': 694, 'Rcd3': 695, 'Rbe7': 696, 'Rcb8': 697, 'Ree2': 698, 'Rdd6': 699, 'Rbb8': 700, 'a7': 701, 'c8=Q': 702, 'N5f3': 703, 'Reb1': 704, 'bc2': 705, 'N1d2': 706, 'R1e3': 707, 'Ned3': 708, 'Rfg3': 709, 'e2': 710, 'e1=Q': 711, 'Rdd2': 712, 'Raa8': 713, 'de3': 714, 'b8=Q': 715, 'Rcd4': 716, 'Rag1': 717, 'R5a2': 718, 'd8=R': 719, 'h7': 720, 'Raf7': 721, 'Nab4': 722, 'Nfg4': 723, 'Rah8': 724, 'Rhc7': 725, 'Rhg3': 726, 'R1g2': 727, 'Nh8': 728, 'Ned4': 729, 'N1a2': 730, 'R1e5': 731, 'O-O+': 732, 'Neg5': 733, 'Nbc7': 734, 'Rdh2': 735, 'R2h4': 736, 'R2b7': 737, 'Nde4': 738, 'Rbb1': 739, 'Nba5': 740, 'N8e6': 741, 'Nfe6': 742, 'Raa1': 743, 'N4c6': 744, 'Rcc5': 745, 'Rah1': 746, 'Ndf1': 747, 'N1g3': 748, 'N5e4': 749, 'Reg1': 750, 'hg2': 751, 'Rgd8': 752, 'Ree1': 753, 'Rcb3': 754, 'R3b4': 755, 'Rba8': 756, 'Rbe3': 757, 'Reg4': 758, 'Rhb8': 759, 'Nhg4': 760, 'N8c6': 761, 'Rcb7': 762, 'a8=Q': 763, 'R8c3': 764, 'Ngh2': 765, 'Nac6': 766, 'N7d5': 767, 'Rbf8': 768, 'R7g3': 769, 'Rhh1': 770, 'Rca4': 771, 'Rda7': 772, 'Rfa8': 773, 'Rag2': 774, 'N6d7': 775, 'R3d4': 776, 'R4d7': 777, 'g7': 778, 'dxc8=N': 779, 'Raf3': 780, 'Rcf1': 781, 'ef2': 782, 'Ncd2': 783, 'R2e5': 784, 'R8d4': 785, 'exd8=B': 786, 'R4d2': 787, 'Ned2': 788, 'Rfd2': 789, 'b2': 790, 'Rdc7': 791, 'Rae7': 792, 'bc7': 793, 'Rea2': 794, 'R5a3': 795, 'Rfd7': 796, 'R4c3': 797, 'N5d4': 798, 'Rbf7': 799, 'Rhg7': 800, 'Rcd7': 801, 'N5a4': 802, 'Rge1': 803, 'Rba1': 804, 'N7c5': 805, 'R6b5': 806, 'R1b2': 807, 'Raa6': 808, 'Rcb6': 809, 'R4e2': 810, 'Rcc4': 811, 'Rdf5': 812, 'R4g6': 813, 'Neg3': 814, 'Ncb4': 815, 'N8f6': 816, 'N4c3': 817, 'Rde7': 818, 'R5d3': 819, 'Nhf4': 820, 'Nhg2': 821, 'cd2': 822, 'N7b5': 823, 'Raa5': 824, 'Ref2': 825, 'Ncd4': 826, 'Nba3': 827, 'g2': 828, 'Rbg1': 829, 'Rbh8': 830, 'Nec5': 831, 'cd7': 832, 'Rea7': 833, 'Nac1': 834, 'Rhh5': 835, 'Rhf5': 836, 'Rae6': 837, 'Ndb4': 838, 'e8=Q': 839, 'e8=N': 840, 'Ree5': 841, 'R7f5': 842, 'Ka3': 843, 'Ngf2': 844, 'N2e4': 845, 'Rbd5': 846, 'Rcc2': 847, 'f2': 848, 'Nhf7': 849, 'exf8=Q': 850, 'Rcf8': 851, 'Nef2': 852, 'R6e2': 853, 'N6g5': 854, 'Rea8': 855, 'Rbc6': 856, 'R8c7': 857, 'R1g4': 858, 'Rah2': 859, 'R4e3': 860, 'Nge3': 861, 'R3c6': 862, 'Rgd1': 863, 'Nhg3': 864, 'R4f2': 865, 'Ndc3': 866, 'R8e6': 867, 'R5c4': 868, 'Ndb6': 869, 'Neg6': 870, 'R3b2': 871, 'R8d5': 872, 'Reh5': 873, 'Rhb1': 874, 'c1=Q': 875, 'Rhf6': 876, 'Rgg7': 877, 'N5c6': 878, 'R8c5': 879, 'R1b3': 880, 'fe2': 881, 'Reg7': 882, 'Nac8': 883, 'R4f5': 884, 'Rfe4': 885, 'Rcg8': 886, 'N5g6': 887, 'Rgg2': 888, 'Rgd2': 889, 'Rhd2': 890, 'Rac6': 891, 'gh2': 892, 'Rag3': 893, 'Nbc1': 894, 'R1f3': 895, 'Rdd5': 896, 'Rec3': 897, 'Rbd3': 898, 'Rad2': 899, 'Ncd3': 900, 'Nfe1': 901, 'N7a6': 902, 'Rfd6': 903, 'N6a5': 904, 'N6h5': 905, 'h2': 906, 'Ngf7': 907, 'Rba7': 908, 'Rac2': 909, 'R1d4': 910, 'Rde2': 911, 'R8f6': 912, 'Reg8': 913, 'Rfg2': 914, 'Rga7': 915, 'R1a7': 916, 'Reh1': 917, 'Rbb2': 918, 'Rdf2': 919, 'Red7': 920, 'R8f7': 921, 'Rgd6': 922, 'Rhf7': 923, 'Rhh7': 924, 'N2f4': 925, 'Rhh8': 926, 'R8e7': 927, 'R6d2': 928, 'Rae3': 929, 'Nfd3': 930, 'N8h7': 931, 'R4e6': 932, 'R1c6': 933, 'Ref8': 934, 'Rcd6': 935, 'R5e7': 936, 'Rac7': 937, 'Nfe8': 938, 'R4a2': 939, 'Nac5': 940, 'Rde3': 941, 'Rca5': 942, 'Rdh7': 943, 'N5e6': 944, 'Nac2': 945, 'Ncd1': 946, 'Rfd4': 947, 'R4d5': 948, 'R3d5': 949, 'R4c5': 950, 'Rff7': 951, 'Rdc2': 952, 'Neg1': 953, 'N2c4': 954, 'Nab1': 955, 'Rhe3': 956, 'Raa2': 957, 'Rbe6': 958, 'Rbb6': 959, 'Ndb3': 960, 'Rhc4': 961, 'g8=Q': 962, 'R2c7': 963, 'Rff6': 964, 'b1=Q': 965, 'R2f6': 966, 'Rab3': 967, 'Rcf6': 968, 'Nhf3': 969, 'fxe1=Q': 970, 'Nhf1': 971, 'N3h2': 972, 'R5a4': 973, 'Rbd7': 974, 'Rac4': 975, 'Rca7': 976, 'fxe8=N': 977, 'Ndc2': 978, 'Rdc3': 979, 'Rec4': 980, 'Rfe5': 981, 'Rdc5': 982, 'Rga8': 983, 'Rab7': 984, 'Nec7': 985, 'Rhd6': 986, 'Rff4': 987, 'Ref6': 988, 'Rgf6': 989, 'R6f7': 990, 'Red5': 991, 'Rba4': 992, 'Rfb2': 993, 'Rfh7': 994, 'Nce8': 995, 'Rad6': 996, 'N7e5': 997, 'Nbd3': 998, 'Nac3': 999, 'Rbf1': 1000, 'R1c3': 1001, 'g1=Q': 1002, 'Rcf2': 1003, 'Rdg5': 1004, 'R6e4': 1005, 'Nca2': 1006, 'R5b3': 1007, 'Nde3': 1008, 'Nbc8': 1009, 'Rcd5': 1010, 'R5f4': 1011, 'N3e4': 1012, 'Rfh1': 1013, 'cxd1=Q': 1014, 'Rgg1': 1015, 'Rda6': 1016, 'Rbc7': 1017, 'Nab8': 1018, 'R1f6': 1019, 'R7d6': 1020, 'Rgd5': 1021, 'Rbe5': 1022, 'R5e2': 1023, 'R1e4': 1024, 'Rfd3': 1025, 'R1f7': 1026, 'R5d7': 1027, 'Nef7': 1028, 'R6d3': 1029, 'Rhd4': 1030, 'Rgg3': 1031, 'Nfe2': 1032, 'Reb2': 1033, 'Ka4': 1034, 'Rce5': 1035, 'Rhf4': 1036, 'R4f7': 1037, 'Nde1': 1038, 'hxg8=Q': 1039, 'Rdd4': 1040, 'Neg8': 1041, 'Rda2': 1042, 'N4e5': 1043, 'Nfh5': 1044, 'R7a6': 1045, 'Rbc2': 1046, 'Nce1': 1047, 'Rad3': 1048, 'Nfg3': 1049, 'Rbd2': 1050, 'Rgf8': 1051, 'R3c2': 1052, 'R4e5': 1053, 'gxf8=Q': 1054, 'N3e5': 1055, 'Ndf7': 1056, 'Rgh8': 1057, 'R5h7': 1058, 'R6g7': 1059, 'Rdb2': 1060, 'Ndc8': 1061, 'R5c6': 1062, 'Rfg8': 1063, 'R2c5': 1064, 'Rhe4': 1065, 'Rdg4': 1066, 'R8g7': 1067, 'Rhc2': 1068, 'N4b3': 1069, 'Reh8': 1070, 'R8b7': 1071, 'Ngh3': 1072, 'Rde6': 1073, 'Ndf2': 1074, 'Rhe6': 1075, 'Nca5': 1076, 'N8a6': 1077, 'Rce7': 1078, 'Rfh8': 1079, 'R5f3': 1080, 'N4d5': 1081, 'R3f2': 1082, 'Ngh4': 1083, 'Ncb3': 1084, 'R8a6': 1085, 'R2a3': 1086, 'R4a7': 1087, 'Reg6': 1088, 'R3e5': 1089, 'Nba8': 1090, 'Rba6': 1091, 'N2c3': 1092, 'Rbd4': 1093, 'Rba5': 1094, 'R7a3': 1095, 'bxc8=Q': 1096, 'Rda8': 1097, 'Reb7': 1098, 'R8a7': 1099, 'R4b5': 1100, 'Reh4': 1101, 'N6h7': 1102, 'Nba4': 1103, 'R8e5': 1104, 'Nab6': 1105, 'Rgd3': 1106, 'R2d5': 1107, 'R4c6': 1108, 'Rfg4': 1109, 'Nbc2': 1110, 'Neg2': 1111, 'N8e7': 1112, 'Rdh5': 1113, 'R8e2': 1114, 'Rgg6': 1115, 'Rae5': 1116, 'Rec6': 1117, 'R5f7': 1118, 'Ndb1': 1119, 'Nba7': 1120, 'dxe8=Q': 1121, 'N4g5': 1122, 'Rfe2': 1123, 'Rdb7': 1124, 'R1d3': 1125, 'Rbe4': 1126, 'Red4': 1127, 'N3a2': 1128, 'R2c3': 1129, 'Ndc7': 1130, 'R3d7': 1131, 'R8d3': 1132, 'R3e2': 1133, 'Rdc4': 1134, 'Rch1': 1135, 'Rhh6': 1136, 'Ref7': 1137, 'Qcc1': 1138, 'Rdf4': 1139, 'N6b5': 1140, 'Ree4': 1141, 'Raa4': 1142, 'Nab2': 1143, 'Rdc6': 1144, 'R8d7': 1145, 'Nhg5': 1146, 'Neg7': 1147, 'Rgb1': 1148, 'Rbe2': 1149, 'Nhg7': 1150, 'N6d5': 1151, 'ed2': 1152, 'R6d5': 1153, 'N6e7': 1154, 'N4g3': 1155, 'Rgd4': 1156, 'Rhh3': 1157, 'Rbg8': 1158, 'R1h3': 1159, 'Rbc3': 1160, 'Rdg3': 1161, 'R2f5': 1162, 'Nge8': 1163, 'N6d4': 1164, 'Rhc6': 1165, 'Rhc5': 1166, 'Rde5': 1167, 'Rfg1': 1168, 'N8f7': 1169, 'Rgh5': 1170, 'Ref3': 1171, 'R6a7': 1172, 'Rcg2': 1173, 'R8g6': 1174, 'Rdf3': 1175, 'Rhh4': 1176, 'Nfd8': 1177, 'Rfc2': 1178, 'Rad5': 1179, 'Ndb8': 1180, 'Reg2': 1181, 'Ncb7': 1182, 'R8d6': 1183, 'R4g2': 1184, 'Rfa7': 1185, 'R5b2': 1186, 'bxa8=Q': 1187, 'N4f5': 1188, 'R1e7': 1189, 'N3c5': 1190, 'R1e6': 1191, 'Ncd8': 1192, 'N4c5': 1193, 'R1a2': 1194, 'N6b4': 1195, 'f8=N': 1196, 'Ned1': 1197, 'Rdb3': 1198, 'R7b6': 1199, 'R8b6': 1200, 'Rab6': 1201, 'R7d5': 1202, 'R1h4': 1203, 'Rgc7': 1204, 'Rhg6': 1205, 'R7f2': 1206, 'R1a5': 1207, 'Nca6': 1208, 'h1=Q': 1209, 'Nca3': 1210, 'R7e3': 1211, 'Rac3': 1212, 'R6d7': 1213, 'Rhf3': 1214, 'N6g4': 1215, 'N6c5': 1216, 'Rbb5': 1217, 'R4e7': 1218, 'Rgg5': 1219, 'Rge7': 1220, 'Rba3': 1221, 'R6c4': 1222, 'Rba2': 1223, 'Rbd6': 1224, 'Rfh6': 1225, 'Reg3': 1226, 'R7f3': 1227, 'Rcf3': 1228, 'R5e6': 1229, 'Rbc5': 1230, 'R1c4': 1231, 'Rfd5': 1232, 'Nbd8': 1233, 'Rfe6': 1234, 'R3a5': 1235, 'N8a7': 1236, 'R8c6': 1237, 'R8a4': 1238, 'Rga1': 1239, 'Ngf1': 1240, 'R2d3': 1241, 'Rfe3': 1242, 'cxb8=Q': 1243, 'Nfh3': 1244, 'Rgh3': 1245, 'Rgf2': 1246, 'Rfh2': 1247, 'R7b3': 1248, 'R4c2': 1249, 'N3c4': 1250, 'Ned8': 1251, 'R2f3': 1252, 'Rgh1': 1253, 'R1h2': 1254, 'R2e4': 1255, 'Rhh2': 1256, 'Rhg2': 1257, 'R6d4': 1258, 'Rdb6': 1259, 'Red6': 1260, 'Rha5': 1261, 'R5c7': 1262, 'Rhg5': 1263, 'Rhb2': 1264, 'R8b2': 1265, 'Nca4': 1266, 'R4d6': 1267, 'Rfb3': 1268, 'Rea3': 1269, 'N3a4': 1270, 'f8=R': 1271, 'Nab3': 1272, 'R3e7': 1273, 'R3c5': 1274, 'R5g3': 1275, 'h8=Q': 1276, 'N3f5': 1277, 'R2b3': 1278, 'Rfc6': 1279, 'Ref4': 1280, 'Rgh4': 1281, 'R1h6': 1282, 'R4c7': 1283, 'R1a6': 1284, 'Rea5': 1285, 'R8c2': 1286, 'Rdg2': 1287, 'R1d5': 1288, 'Rcg7': 1289, 'Rag7': 1290, 'exd8=Q': 1291, 'fxg8=Q': 1292, 'Q1c2': 1293, 'R8e3': 1294, 'Nfg7': 1295, 'Rbf3': 1296, 'Rea6': 1297, 'Rab2': 1298, 'Rbb3': 1299, 'Rhe5': 1300, 'Rcf7': 1301, 'Rab5': 1302, 'R8h4': 1303, 'Raf6': 1304, 'R7e4': 1305, 'R5e4': 1306, 'Rbf2': 1307, 'R6f5': 1308, 'Nec2': 1309, 'Rca2': 1310, 'R6e5': 1311, 'R8c4': 1312, 'R5f2': 1313, 'Rfh3': 1314, 'Nhg8': 1315, 'Rcb4': 1316, 'R8f4': 1317, 'Rac5': 1318, 'R7f4': 1319, 'N2e3': 1320, 'R4a3': 1321, 'Rad4': 1322, 'R8a5': 1323, 'Nfd1': 1324, 'R4h3': 1325, 'Rgf5': 1326, 'Rhb3': 1327, 'R1f5': 1328, 'N8g7': 1329, 'N3b4': 1330, 'N8b7': 1331, 'R2d6': 1332, 'R8d2': 1333, 'N1b2': 1334, 'bxc1=Q': 1335, 'R7c4': 1336, 'R3g4': 1337, 'R1a4': 1338, 'Rgb5': 1339, 'a1=Q': 1340, 'Nfh6': 1341, 'N5h6': 1342, 'Rff5': 1343, 'R7e5': 1344, 'R8a3': 1345, 'Rgg4': 1346, 'd1=Q': 1347, 'R1f4': 1348, 'N3b5': 1349, 'R5c2': 1350, 'N4h5': 1351, 'Rdb5': 1352, 'R7g5': 1353, 'Rhg4': 1354, 'R6b2': 1355, 'Rge3': 1356, 'R7h5': 1357, 'Nba2': 1358, 'Ncb2': 1359, 'N8c7': 1360, 'Rge5': 1361, 'Nca1': 1362, 'Rfa4': 1363, 'Nba1': 1364, 'Rfg6': 1365, 'N6c7': 1366, 'Rfc3': 1367, 'Rbh2': 1368, 'R8f3': 1369, 'Rfb7': 1370, 'Rch3': 1371, 'Rca6': 1372, 'Rch4': 1373, 'R7h4': 1374, 'Nab7': 1375, 'R7c3': 1376, 'Rge2': 1377, 'Rab4': 1378, 'R8e4': 1379, 'N4b5': 1380, 'R5g7': 1381, 'R4f3': 1382, 'Rah3': 1383, 'Rge4': 1384, 'R2a5': 1385, 'Reg5': 1386, 'Rcg5': 1387, 'Rbc4': 1388, 'Raf5': 1389, 'R2d4': 1390, 'Rhd5': 1391, 'Nhf2': 1392, 'R3a2': 1393, 'R5d6': 1394, 'R2d7': 1395, 'Nef8': 1396, 'Ndb7': 1397, 'Ref5': 1398, 'N2d4': 1399, 'R7a2': 1400, 'Nfh2': 1401, 'N4e6': 1402, 'R4g3': 1403, 'N6c4': 1404, 'N4d6': 1405, 'Rgb4': 1406, 'R7b5': 1407, 'Rhc3': 1408, 'N5h4': 1409, 'R1h7': 1410, 'Rdf7': 1411, 'Raf2': 1412, 'exd1=Q': 1413, 'Rch8': 1414, 'Nef1': 1415, 'N1c2': 1416, 'N5e3': 1417, 'Qeg5': 1418, 'R8h6': 1419, 'Rcf5': 1420, 'fill': 1421}\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["Saving of the encoding dictionary"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b947fd32-f4ad-48aa-8992-8876bf0e90b4"}}},{"cell_type":"code","source":["import json\nwith open(\"/tmp/dic_moves_completeGames_15k.txt\", \"w+\") as fp:\n    json.dump(int_moves, fp)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"66ac6b7b-298c-48a6-85a6-158adc873347"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.cp(\"file:/tmp/dic_moves_completeGames_15k.txt\", \"dbfs:/tmp/dic_moves_completeGames_15k.txt\")\ndisplay(dbutils.fs.ls(\"/tmp/dic_moves_completeGames_15k.txt\"))\ndbutils.fs.cp(\"dbfs:/tmp/dic_moves_completeGames_15k.txt\", \"/FileStore/dic_moves_completeGames_15k.txt\")\n#community.cloud.databricks.com/files/dic_moves_completeGames_15k.txt?o=5604700270830977"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6a2830b1-e50f-459b-8818-3bf0d4f5bb79"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.cp(\"dbfs:/tmp/dic_freq_moves_completeGames_15k.txt\", \"file:/tmp/dic_freq_moves_completeGames_15k.txt\")\nwith open('/tmp/dic_freq_moves_completeGames_15k.txt') as json_file:\n    dic_moves = json.load(json_file)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fb0eeddf-8b4a-4c18-ada9-b9f8c7c823a5"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.cp(\"dbfs:/tmp/dic_moves_completeGames_15k.txt\", \"file:/tmp/dic_moves_completeGames_15k.txt\")\nwith open('/tmp/dic_moves_completeGames_15k.txt') as json_file:\n    int_moves = json.load(json_file)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0369de8e-1b6b-4e02-b137-e18981f31dae"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Creating x and y by dividing by 5 timesteps and within the sliding windows we put the integer coding of the moves."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fc7713d1-9d07-43f7-b456-50469aba2133"}}},{"cell_type":"code","source":["n_steps=5\nmanipulated_x=[]\nmanipulated_y=[]\nconta=0\nfor el in game_man:\n    el2=[int(int_moves[e]) for e in el]\n    n = medium-len(el2)\n    lfill=[int(int_moves[\"fill\"]) for i in range(0,n)]\n    lista=el2+lfill\n    seq=lista\n    for i in range(len(seq)):\n          #get the last index\n          lastIndex = i + n_steps\n          #if lastIndex is greater than length of sequence then break\n          if lastIndex > len(seq) - 1:\n              break\n          #Create input and output sequence\n          seq_X, seq_y = seq[i:lastIndex], seq[lastIndex]\n          manipulated_x.append(seq_X)\n          manipulated_y.append(seq_y)\n          pass\n    if conta%500==0:\n      print(conta)\n    conta+=1"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7b3ec86b-da72-4a6f-a3c1-85a13d705196"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"0\n500\n1000\n1500\n2000\n2500\n3000\n3500\n4000\n4500\n5000\n5500\n6000\n6500\n7000\n7500\n8000\n8500\n9000\n9500\n10000\n10500\n11000\n11500\n12000\n12500\n13000\n13500\n14000\n14500\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["0\n500\n1000\n1500\n2000\n2500\n3000\n3500\n4000\n4500\n5000\n5500\n6000\n6500\n7000\n7500\n8000\n8500\n9000\n9500\n10000\n10500\n11000\n11500\n12000\n12500\n13000\n13500\n14000\n14500\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["Saving of X and Y to the DBFS."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aebd426f-98c1-4d33-ba0a-e2e3f14f6d45"}}},{"cell_type":"code","source":["with open(\"/tmp/X_slide_windows_10-75_5steps_CompleteGames_15K.txt\", \"w+\") as f:\n    for el in manipulated_x:\n      s=str(el[0])+\",\"+str(el[1])+\",\"+str(el[2])+\",\"+str(el[3])+\",\"+str(el[4])\n      f.write(\"{}\\n\".format(s))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a1a97546-ea24-4d77-b667-04002d5167db"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.cp(\"file:/tmp/X_slide_windows_10-75_5steps_CompleteGames_15K.txt\", \"dbfs:/tmp/X_slide_windows_10-75_5steps_CompleteGames_15K.txt\")\ndisplay(dbutils.fs.ls(\"/tmp/X_slide_windows_10-75_5steps_CompleteGames_15K.txt\"))\ndbutils.fs.cp(\"dbfs:/tmp/X_slide_windows_10-75_5steps_CompleteGames_15K.txt\", \"/FileStore/X_slide_windows_10-75_5steps_CompleteGames_15K.txt\")\n#community.cloud.databricks.com/files/X_slide_windows_10-75_5steps_CompleteGames_15K.txt?o=5604700270830977"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8645abc8-e1d8-43b4-9252-d6c65ecf00e8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["dbfs:/tmp/X_slide_windows_10-75_5steps_CompleteGames_15K.txt","X_slide_windows_10-75_5steps_CompleteGames_15K.txt",19988577,1654292953000]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"size","type":"\"long\"","metadata":"{}"},{"name":"modificationTime","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/tmp/X_slide_windows_10-75_5steps_CompleteGames_15K.txt</td><td>X_slide_windows_10-75_5steps_CompleteGames_15K.txt</td><td>19988577</td><td>1654292953000</td></tr></tbody></table></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[17]: True","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[17]: True"]}}],"execution_count":0},{"cell_type":"code","source":["with open(\"/tmp/Y_slide_windows_10-75_5steps_CompleteGames_15K.txt\", \"w+\") as f:\n    for el in manipulated_y:\n      f.write(\"{}\\n\".format(str(el)))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fbfc6988-9b1d-40d0-975e-757400911ae9"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.cp(\"file:/tmp/Y_slide_windows_10-75_5steps_CompleteGames_15K.txt\", \"dbfs:/tmp/Y_slide_windows_10-75_5steps_CompleteGames_15K.txt\")\ndisplay(dbutils.fs.ls(\"/tmp/Y_slide_windows_10-75_5steps_CompleteGames_15K.txt\"))\ndbutils.fs.cp(\"dbfs:/tmp/Y_slide_windows_10-75_5steps_CompleteGames_15K.txt\", \"/FileStore/Y_slide_windows_10-75_5steps_CompleteGames_15K.txt\")\n#community.cloud.databricks.com/files/Y_slide_windows_10-75_5steps_CompleteGames_15K.txt?o=5604700270830977"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1bd0b3c2-e5c2-41e6-bd26-f80dd462d3be"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["dbfs:/tmp/Y_slide_windows_10-75_5steps_CompleteGames_15K.txt","Y_slide_windows_10-75_5steps_CompleteGames_15K.txt",4102906,1654293000000]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"size","type":"\"long\"","metadata":"{}"},{"name":"modificationTime","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/tmp/Y_slide_windows_10-75_5steps_CompleteGames_15K.txt</td><td>Y_slide_windows_10-75_5steps_CompleteGames_15K.txt</td><td>4102906</td><td>1654293000000</td></tr></tbody></table></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[19]: True","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[19]: True"]}}],"execution_count":0},{"cell_type":"markdown","source":["Code for read X and Y from the DBFS."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"922bbd1e-d223-454e-a9c7-51c9478f22db"}}},{"cell_type":"code","source":["dbutils.fs.cp(\"dbfs:/tmp/X_slide_windows_10-75_5steps_CompleteGames_15K.txt\", \"file:/tmp/X_slide_windows_10-75_5steps_CompleteGames_15K.txt\")\nx=[]\nc=0\nfor line in open(\"/tmp/X_slide_windows_10-75_5steps_CompleteGames_15K.txt\",\"r\"):\n    stripped_line = line.strip()\n    s=stripped_line.split(\",\")\n    s=[int(e) for e in s]\n    x.append(s)\n\nx = np.array(x, dtype=int)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bb77cd17-3375-456b-8779-cf1474905d76"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.cp(\"dbfs:/tmp/Y_slide_windows_10-75_5steps_CompleteGames_15K.txt\", \"file:/tmp/Y_slide_windows_10-75_5steps_CompleteGames_15K.txt\")\ny=[]\nfor line in open(\"/tmp/Y_slide_windows_10-75_5steps_CompleteGames_15K.txt\",\"r\"):\n  stripped_line = line.strip()\n  y.append(int(stripped_line))\ny = np.array(y, dtype=int)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d4ae5e25-2883-4e3d-a1c0-8df8855e42ab"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["In the following cells, the train and test set are created. 20% of the matches are used for the test set."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9e8a1f49-dfd0-403e-84ea-99736791609b"}}},{"cell_type":"code","source":["print(\"TRAIN X\")\ntrain_x= x[:int(len(x)*0.8)]\nprint(\"The number of elements in train_x is\",len(train_x), \"(80% of the dataset)\")\n\nprint(\"TRAIN Y\")\ntrain_y = y[:int(len(y)*0.8)]\nprint(\"The number of elements in train_y is\",len(train_y), \"(80% of the dataset)\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7131fba2-ea4b-4358-9f35-608af52eda41"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"TRAIN X\nThe number of elements in train_x are 840000 (80% of the dataset)\nTRAIN Y\nThe number of elements in train_y are 840000 (80% of the dataset)\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["TRAIN X\nThe number of elements in train_x are 840000 (80% of the dataset)\nTRAIN Y\nThe number of elements in train_y are 840000 (80% of the dataset)\n"]}}],"execution_count":0},{"cell_type":"code","source":["print(\"TEST X\")\ntest_x = x[-int(len(x)*0.2):] \nprint(\"The number of elements in test_x is\",len(test_x), \"(20% of the dataset)\")\n\nprint(\"TEST Y\")\ntest_y= y[-int(len(y)*0.2):] \nprint(\"The number of elements in test_y is\",len(test_y), \"(20% of the dataset)\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"90908bea-45d0-426a-806f-0d0f4d97a481"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"TEST X\nThe number of elements in test_x are 210000 (20% of the dataset)\nTEST Y\nThe number of elements in test_y are 210000 (20% of the dataset)\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["TEST X\nThe number of elements in test_x are 210000 (20% of the dataset)\nTEST Y\nThe number of elements in test_y are 210000 (20% of the dataset)\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["In the following cell, One Hot Encoding is applied on the labels of the train set."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1d886b6d-46f2-4364-a8b4-35ad344eaa1d"}}},{"cell_type":"code","source":["train_y = to_categorical(train_y)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5fe6c2cd-275e-4254-ab71-3efaed827cc5"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["In the following cell we define values that will be used for embedding layer of the model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"da861b74-3fcc-458d-a5a2-fecbce9380d5"}}},{"cell_type":"code","source":["seq_len=x.shape[1]\nvocab_size=len(int_moves)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7b76832b-b637-4083-879c-3c3c552745e3"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Defining a callback to prevent overfitting"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bba3a03a-02cb-4ece-b68c-597d32eb0beb"}}},{"cell_type":"code","source":["from tensorflow.keras.callbacks import EarlyStopping\ncallback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"40c4aab6-08ec-4f38-a058-122f8b102d2c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["In the next cell we have the model used for the train performed on kaggle. The Neural Network used is the same of the first model."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"379285d4-e1ef-4204-99e0-f8037c7b6d5b"}}},{"cell_type":"code","source":["model = Sequential()\nmodel.add(Embedding(vocab_size, n_steps, input_length=seq_len))\nmodel.add(LSTM(100, return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(100))\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dense(vocab_size, activation='softmax'))\nprint(model.summary())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"93ecb224-8dbe-433a-94a8-1fd21a201e15"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Model: \"sequential_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n embedding_2 (Embedding)     (None, 5, 5)              7110      \n                                                                 \n lstm_4 (LSTM)               (None, 5, 100)            42400     \n                                                                 \n dropout_2 (Dropout)         (None, 5, 100)            0         \n                                                                 \n lstm_5 (LSTM)               (None, 100)               80400     \n                                                                 \n dense_4 (Dense)             (None, 100)               10100     \n                                                                 \n dense_5 (Dense)             (None, 1422)              143622    \n                                                                 \n=================================================================\nTotal params: 283,632\nTrainable params: 283,632\nNon-trainable params: 0\n_________________________________________________________________\nNone\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Model: \"sequential_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n embedding_2 (Embedding)     (None, 5, 5)              7110      \n                                                                 \n lstm_4 (LSTM)               (None, 5, 100)            42400     \n                                                                 \n dropout_2 (Dropout)         (None, 5, 100)            0         \n                                                                 \n lstm_5 (LSTM)               (None, 100)               80400     \n                                                                 \n dense_4 (Dense)             (None, 100)               10100     \n                                                                 \n dense_5 (Dense)             (None, 1422)              143622    \n                                                                 \n=================================================================\nTotal params: 283,632\nTrainable params: 283,632\nNon-trainable params: 0\n_________________________________________________________________\nNone\n"]}}],"execution_count":0},{"cell_type":"code","source":["# compile model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n# fit model\nhistory=model.fit(train_x, train_y, batch_size=64, epochs=50, callbacks=[callback])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b40dee0d-4c61-4c1e-a504-df73925392d1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Epoch 1/50\n\r    1/13125 [..............................] - ETA: 27:41:26 - loss: 7.2599 - accuracy: 0.0000e+00\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    3/13125 [..............................] - ETA: 7:38 - loss: 7.2584 - accuracy: 0.1615        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    5/13125 [..............................] - ETA: 6:53 - loss: 7.2567 - accuracy: 0.2094\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    7/13125 [..............................] - ETA: 6:33 - loss: 7.2546 - accuracy: 0.2210\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    9/13125 [..............................] - ETA: 6:24 - loss: 7.2521 - accuracy: 0.2378\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   11/13125 [..............................] - ETA: 6:20 - loss: 7.2486 - accuracy: 0.2472\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   13/13125 [..............................] - ETA: 6:18 - loss: 7.2444 - accuracy: 0.2548\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   15/13125 [..............................] - ETA: 6:15 - loss: 7.2381 - accuracy: 0.2677\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   17/13125 [..............................] - ETA: 6:13 - loss: 7.2302 - accuracy: 0.2711\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   19/13125 [..............................] - ETA: 6:12 - loss: 7.2216 - accuracy: 0.2640","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Epoch 1/50\n\r    1/13125 [..............................] - ETA: 27:41:26 - loss: 7.2599 - accuracy: 0.0000e+00\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    3/13125 [..............................] - ETA: 7:38 - loss: 7.2584 - accuracy: 0.1615        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    5/13125 [..............................] - ETA: 6:53 - loss: 7.2567 - accuracy: 0.2094\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    7/13125 [..............................] - ETA: 6:33 - loss: 7.2546 - accuracy: 0.2210\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    9/13125 [..............................] - ETA: 6:24 - loss: 7.2521 - accuracy: 0.2378\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   11/13125 [..............................] - ETA: 6:20 - loss: 7.2486 - accuracy: 0.2472\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   13/13125 [..............................] - ETA: 6:18 - loss: 7.2444 - accuracy: 0.2548\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   15/13125 [..............................] - ETA: 6:15 - loss: 7.2381 - accuracy: 0.2677\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   17/13125 [..............................] - ETA: 6:13 - loss: 7.2302 - accuracy: 0.2711\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   19/13125 [..............................] - ETA: 6:12 - loss: 7.2216 - accuracy: 0.2640"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Cancelled","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["We upload the trained model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"982b269d-beb1-4ca3-a135-3b8867978e33"}}},{"cell_type":"code","source":["%sh wget https://www.dropbox.com/s/c47y4dxcoit4bbk/Kaggle_model_completeGames_15K.h5?dl=1 -O /tmp/Kaggle_NoFreq_completeGames.csv"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"318bc27c-0d6d-4a41-b350-a533b4144edd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"--2022-06-04 07:56:10--  https://www.dropbox.com/s/c47y4dxcoit4bbk/Kaggle_model_completeGames_15K.h5?dl=1\nResolving www.dropbox.com (www.dropbox.com)... 162.125.8.18, 2620:100:6016:18::a27d:112\nConnecting to www.dropbox.com (www.dropbox.com)|162.125.8.18|:443... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: /s/dl/c47y4dxcoit4bbk/Kaggle_model_completeGames_15K.h5 [following]\n--2022-06-04 07:56:11--  https://www.dropbox.com/s/dl/c47y4dxcoit4bbk/Kaggle_model_completeGames_15K.h5\nReusing existing connection to www.dropbox.com:443.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://ucc48614cd436b32cdf98c7fae05.dl.dropboxusercontent.com/cd/0/get/BmjyXfV59cuhDEdWLndI7t4tBEQgg-ya6Oty2YFPPRws5C5MrBii1HxcbiyKOEJK0Au5aAXXC9cqQFdX9OklpPgnWi1gJSjT61AwGm0dHJK7WaHwbWDIbcu4z0vF8-6TeURsG7z0DCc_4-Y5X25hgq_UmTxowGwcQ9H-WoOg3J5Yoxizq0XTadnsmB_ae2fi1FM/file?dl=1# [following]\n--2022-06-04 07:56:11--  https://ucc48614cd436b32cdf98c7fae05.dl.dropboxusercontent.com/cd/0/get/BmjyXfV59cuhDEdWLndI7t4tBEQgg-ya6Oty2YFPPRws5C5MrBii1HxcbiyKOEJK0Au5aAXXC9cqQFdX9OklpPgnWi1gJSjT61AwGm0dHJK7WaHwbWDIbcu4z0vF8-6TeURsG7z0DCc_4-Y5X25hgq_UmTxowGwcQ9H-WoOg3J5Yoxizq0XTadnsmB_ae2fi1FM/file?dl=1\nResolving ucc48614cd436b32cdf98c7fae05.dl.dropboxusercontent.com (ucc48614cd436b32cdf98c7fae05.dl.dropboxusercontent.com)... 162.125.2.15, 2620:100:6016:15::a27d:10f\nConnecting to ucc48614cd436b32cdf98c7fae05.dl.dropboxusercontent.com (ucc48614cd436b32cdf98c7fae05.dl.dropboxusercontent.com)|162.125.2.15|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 3461728 (3.3M) [application/binary]\nSaving to: /tmp/Kaggle_NoFreq_completeGames.csv\n\n     0K .......... .......... .......... .......... ..........  1%  830K 4s\n    50K .......... .......... .......... .......... ..........  2% 1.62M 3s\n   100K .......... .......... .......... .......... ..........  4% 2.01M 2s\n   150K .......... .......... .......... .......... ..........  5% 2.54M 2s\n   200K .......... .......... .......... .......... ..........  7% 5.05M 2s\n   250K .......... .......... .......... .......... ..........  8% 5.16M 2s\n   300K .......... .......... .......... .......... .......... 10% 4.42M 1s\n   350K .......... .......... .......... .......... .......... 11% 5.48M 1s\n   400K .......... .......... .......... .......... .......... 13% 8.46M 1s\n   450K .......... .......... .......... .......... .......... 14% 10.0M 1s\n   500K .......... .......... .......... .......... .......... 16% 10.3M 1s\n   550K .......... .......... .......... .......... .......... 17% 12.1M 1s\n   600K .......... .......... .......... .......... .......... 19% 15.5M 1s\n   650K .......... .......... .......... .......... .......... 20% 13.5M 1s\n   700K .......... .......... .......... .......... .......... 22% 13.8M 1s\n   750K .......... .......... .......... .......... .......... 23% 10.6M 1s\n   800K .......... .......... .......... .......... .......... 25% 18.8M 1s\n   850K .......... .......... .......... .......... .......... 26% 16.2M 1s\n   900K .......... .......... .......... .......... .......... 28% 17.3M 1s\n   950K .......... .......... .......... .......... .......... 29% 16.7M 1s\n  1000K .......... .......... .......... .......... .......... 31% 18.8M 0s\n  1050K .......... .......... .......... .......... .......... 32% 21.9M 0s\n  1100K .......... .......... .......... .......... .......... 34% 24.5M 0s\n  1150K .......... .......... .......... .......... .......... 35% 18.0M 0s\n  1200K .......... .......... .......... .......... .......... 36% 25.4M 0s\n  1250K .......... .......... .......... .......... .......... 38% 28.5M 0s\n  1300K .......... .......... .......... .......... .......... 39% 22.6M 0s\n  1350K .......... .......... .......... .......... .......... 41% 40.5M 0s\n  1400K .......... .......... .......... .......... .......... 42% 22.4M 0s\n  1450K .......... .......... .......... .......... .......... 44% 42.4M 0s\n  1500K .......... .......... .......... .......... .......... 45% 44.5M 0s\n  1550K .......... .......... .......... .......... .......... 47% 25.9M 0s\n  1600K .......... .......... .......... .......... .......... 48% 29.9M 0s\n  1650K .......... .......... .......... .......... .......... 50% 21.7M 0s\n  1700K .......... .......... .......... .......... .......... 51% 75.6M 0s\n  1750K .......... .......... .......... .......... .......... 53% 5.00M 0s\n  1800K .......... .......... .......... .......... .......... 54% 96.9M 0s\n  1850K .......... .......... .......... .......... .......... 56%  110M 0s\n  1900K .......... .......... .......... .......... .......... 57% 81.0M 0s\n  1950K .......... .......... .......... .......... .......... 59% 86.5M 0s\n  2000K .......... .......... .......... .......... .......... 60%  136M 0s\n  2050K .......... .......... .......... .......... .......... 62% 99.2M 0s\n  2100K .......... .......... .......... .......... .......... 63% 3.77M 0s\n  2150K .......... .......... .......... .......... .......... 65% 82.3M 0s\n  2200K .......... .......... .......... .......... .......... 66%  139M 0s\n  2250K .......... .......... .......... .......... .......... 68%  141M 0s\n  2300K .......... .......... .......... .......... .......... 69% 92.8M 0s\n  2350K .......... .......... .......... .......... .......... 70%  116M 0s\n  2400K .......... .......... .......... .......... .......... 72%  115M 0s\n  2450K .......... .......... .......... .......... .......... 73% 78.6M 0s\n  2500K .......... .......... .......... .......... .......... 75% 10.5M 0s\n  2550K .......... .......... .......... .......... .......... 76%  113M 0s\n  2600K .......... .......... .......... .......... .......... 78% 74.6M 0s\n  2650K .......... .......... .......... .......... .......... 79%  142M 0s\n  2700K .......... .......... .......... .......... .......... 81%  134M 0s\n  2750K .......... .......... .......... .......... .......... 82% 89.7M 0s\n  2800K .......... .......... .......... .......... .......... 84%  130M 0s\n  2850K .......... .......... .......... .......... .......... 85%  131M 0s\n  2900K .......... .......... .......... .......... .......... 87%  141M 0s\n  2950K .......... .......... .......... .......... .......... 88% 3.88M 0s\n  3000K .......... .......... .......... .......... .......... 90%  128M 0s\n  3050K .......... .......... .......... .......... .......... 91%  129M 0s\n  3100K .......... .......... .......... .......... .......... 93%  129M 0s\n  3150K .......... .......... .......... .......... .......... 94%  109M 0s\n  3200K .......... .......... .......... .......... .......... 96%  133M 0s\n  3250K .......... .......... .......... .......... .......... 97% 6.34M 0s\n  3300K .......... .......... .......... .......... .......... 99%  131M 0s\n  3350K .......... .......... ..........                      100%  131M=0.3s\n\n2022-06-04 07:56:12 (10.8 MB/s) - /tmp/Kaggle_NoFreq_completeGames.csv saved [3461728/3461728]\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["--2022-06-04 07:56:10--  https://www.dropbox.com/s/c47y4dxcoit4bbk/Kaggle_model_completeGames_15K.h5?dl=1\nResolving www.dropbox.com (www.dropbox.com)... 162.125.8.18, 2620:100:6016:18::a27d:112\nConnecting to www.dropbox.com (www.dropbox.com)|162.125.8.18|:443... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: /s/dl/c47y4dxcoit4bbk/Kaggle_model_completeGames_15K.h5 [following]\n--2022-06-04 07:56:11--  https://www.dropbox.com/s/dl/c47y4dxcoit4bbk/Kaggle_model_completeGames_15K.h5\nReusing existing connection to www.dropbox.com:443.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://ucc48614cd436b32cdf98c7fae05.dl.dropboxusercontent.com/cd/0/get/BmjyXfV59cuhDEdWLndI7t4tBEQgg-ya6Oty2YFPPRws5C5MrBii1HxcbiyKOEJK0Au5aAXXC9cqQFdX9OklpPgnWi1gJSjT61AwGm0dHJK7WaHwbWDIbcu4z0vF8-6TeURsG7z0DCc_4-Y5X25hgq_UmTxowGwcQ9H-WoOg3J5Yoxizq0XTadnsmB_ae2fi1FM/file?dl=1# [following]\n--2022-06-04 07:56:11--  https://ucc48614cd436b32cdf98c7fae05.dl.dropboxusercontent.com/cd/0/get/BmjyXfV59cuhDEdWLndI7t4tBEQgg-ya6Oty2YFPPRws5C5MrBii1HxcbiyKOEJK0Au5aAXXC9cqQFdX9OklpPgnWi1gJSjT61AwGm0dHJK7WaHwbWDIbcu4z0vF8-6TeURsG7z0DCc_4-Y5X25hgq_UmTxowGwcQ9H-WoOg3J5Yoxizq0XTadnsmB_ae2fi1FM/file?dl=1\nResolving ucc48614cd436b32cdf98c7fae05.dl.dropboxusercontent.com (ucc48614cd436b32cdf98c7fae05.dl.dropboxusercontent.com)... 162.125.2.15, 2620:100:6016:15::a27d:10f\nConnecting to ucc48614cd436b32cdf98c7fae05.dl.dropboxusercontent.com (ucc48614cd436b32cdf98c7fae05.dl.dropboxusercontent.com)|162.125.2.15|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 3461728 (3.3M) [application/binary]\nSaving to: /tmp/Kaggle_NoFreq_completeGames.csv\n\n     0K .......... .......... .......... .......... ..........  1%  830K 4s\n    50K .......... .......... .......... .......... ..........  2% 1.62M 3s\n   100K .......... .......... .......... .......... ..........  4% 2.01M 2s\n   150K .......... .......... .......... .......... ..........  5% 2.54M 2s\n   200K .......... .......... .......... .......... ..........  7% 5.05M 2s\n   250K .......... .......... .......... .......... ..........  8% 5.16M 2s\n   300K .......... .......... .......... .......... .......... 10% 4.42M 1s\n   350K .......... .......... .......... .......... .......... 11% 5.48M 1s\n   400K .......... .......... .......... .......... .......... 13% 8.46M 1s\n   450K .......... .......... .......... .......... .......... 14% 10.0M 1s\n   500K .......... .......... .......... .......... .......... 16% 10.3M 1s\n   550K .......... .......... .......... .......... .......... 17% 12.1M 1s\n   600K .......... .......... .......... .......... .......... 19% 15.5M 1s\n   650K .......... .......... .......... .......... .......... 20% 13.5M 1s\n   700K .......... .......... .......... .......... .......... 22% 13.8M 1s\n   750K .......... .......... .......... .......... .......... 23% 10.6M 1s\n   800K .......... .......... .......... .......... .......... 25% 18.8M 1s\n   850K .......... .......... .......... .......... .......... 26% 16.2M 1s\n   900K .......... .......... .......... .......... .......... 28% 17.3M 1s\n   950K .......... .......... .......... .......... .......... 29% 16.7M 1s\n  1000K .......... .......... .......... .......... .......... 31% 18.8M 0s\n  1050K .......... .......... .......... .......... .......... 32% 21.9M 0s\n  1100K .......... .......... .......... .......... .......... 34% 24.5M 0s\n  1150K .......... .......... .......... .......... .......... 35% 18.0M 0s\n  1200K .......... .......... .......... .......... .......... 36% 25.4M 0s\n  1250K .......... .......... .......... .......... .......... 38% 28.5M 0s\n  1300K .......... .......... .......... .......... .......... 39% 22.6M 0s\n  1350K .......... .......... .......... .......... .......... 41% 40.5M 0s\n  1400K .......... .......... .......... .......... .......... 42% 22.4M 0s\n  1450K .......... .......... .......... .......... .......... 44% 42.4M 0s\n  1500K .......... .......... .......... .......... .......... 45% 44.5M 0s\n  1550K .......... .......... .......... .......... .......... 47% 25.9M 0s\n  1600K .......... .......... .......... .......... .......... 48% 29.9M 0s\n  1650K .......... .......... .......... .......... .......... 50% 21.7M 0s\n  1700K .......... .......... .......... .......... .......... 51% 75.6M 0s\n  1750K .......... .......... .......... .......... .......... 53% 5.00M 0s\n  1800K .......... .......... .......... .......... .......... 54% 96.9M 0s\n  1850K .......... .......... .......... .......... .......... 56%  110M 0s\n  1900K .......... .......... .......... .......... .......... 57% 81.0M 0s\n  1950K .......... .......... .......... .......... .......... 59% 86.5M 0s\n  2000K .......... .......... .......... .......... .......... 60%  136M 0s\n  2050K .......... .......... .......... .......... .......... 62% 99.2M 0s\n  2100K .......... .......... .......... .......... .......... 63% 3.77M 0s\n  2150K .......... .......... .......... .......... .......... 65% 82.3M 0s\n  2200K .......... .......... .......... .......... .......... 66%  139M 0s\n  2250K .......... .......... .......... .......... .......... 68%  141M 0s\n  2300K .......... .......... .......... .......... .......... 69% 92.8M 0s\n  2350K .......... .......... .......... .......... .......... 70%  116M 0s\n  2400K .......... .......... .......... .......... .......... 72%  115M 0s\n  2450K .......... .......... .......... .......... .......... 73% 78.6M 0s\n  2500K .......... .......... .......... .......... .......... 75% 10.5M 0s\n  2550K .......... .......... .......... .......... .......... 76%  113M 0s\n  2600K .......... .......... .......... .......... .......... 78% 74.6M 0s\n  2650K .......... .......... .......... .......... .......... 79%  142M 0s\n  2700K .......... .......... .......... .......... .......... 81%  134M 0s\n  2750K .......... .......... .......... .......... .......... 82% 89.7M 0s\n  2800K .......... .......... .......... .......... .......... 84%  130M 0s\n  2850K .......... .......... .......... .......... .......... 85%  131M 0s\n  2900K .......... .......... .......... .......... .......... 87%  141M 0s\n  2950K .......... .......... .......... .......... .......... 88% 3.88M 0s\n  3000K .......... .......... .......... .......... .......... 90%  128M 0s\n  3050K .......... .......... .......... .......... .......... 91%  129M 0s\n  3100K .......... .......... .......... .......... .......... 93%  129M 0s\n  3150K .......... .......... .......... .......... .......... 94%  109M 0s\n  3200K .......... .......... .......... .......... .......... 96%  133M 0s\n  3250K .......... .......... .......... .......... .......... 97% 6.34M 0s\n  3300K .......... .......... .......... .......... .......... 99%  131M 0s\n  3350K .......... .......... ..........                      100%  131M=0.3s\n\n2022-06-04 07:56:12 (10.8 MB/s) - /tmp/Kaggle_NoFreq_completeGames.csv saved [3461728/3461728]\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["model1_completeGames = keras.models.load_model(\"/tmp/Kaggle_NoFreq_completeGames.csv\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0c0f8975-28be-42d5-9162-12f5a66d0b8a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Plotting of the loss and accuracy values during the train"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ff04ae46-fa01-4aab-90fd-762594d3f61f"}}},{"cell_type":"code","source":["plt.plot(history.history['loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train'], loc='upper right')\nplt.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a76fd1a6-b6dd-4af0-a7a3-f854eaa5a5fe"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["<img src=\"https://www.dropbox.com/s/gbvi9ttsyfwsaku/model7_loss.png?dl=1\">"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ef5faea2-a6a9-47d4-a737-d9685ee33f44"}}},{"cell_type":"code","source":["plt.plot(history.history['accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train'], loc='upper left')\nplt.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d57f12fb-ee03-4611-acd0-62e94c33a2b7"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["<img src=\"https://www.dropbox.com/s/g3jcucnmyj5b9rb/Model7_accuracy.png?dl=1\">"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"45f9d4af-89bd-4196-a551-0bd7a240cad6"}}},{"cell_type":"markdown","source":["##### Second Model created using the most frequent moves\nFor the second model in this approach, a new dictionary was calculated by taking only the most frequent moves and giving all others an additional move. With this method I tried to reduce the dictionary to increase performance."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f657689c-6f47-4064-b9c2-33a93691eb4a"}}},{"cell_type":"markdown","source":["In the following cell the average occurrence of the moves is calculated"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"233679a7-f5f8-4ff0-8cc7-7272b68c2459"}}},{"cell_type":"code","source":["s=0\nfor k,v in dic_moves.items():\n  s+=v\navg_freq_completeGames=s/len(dic_moves)\nprint(\"The average of the frequencies of the moves is\",avg_freq_completeGames)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3c755e4f-0410-4243-842d-d8090131b059"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"The average of the frequencies of the moves is 578.707248416608\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["The average of the frequencies of the moves is 578.707248416608\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["In the next cell the set of moves that occur more times than the average is calculated"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cf0be6c4-c0ec-4857-b66a-02cbb123ec3f"}}},{"cell_type":"code","source":["se=set()\nfor k,v in dic_moves.items():\n  if v >=avg_freq_completeGames:\n    se.add(k)\nprint(\"The moves that occur more times than the average are\",len(se))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0d210911-850c-4dc6-9aba-95f34d387359"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"The moves that occur more times than the average are 287\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["The moves that occur more times than the average are 287\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["A dictionary is created in the next cell for encoding the moves just selected"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f6be33fc-6097-477b-bb37-216087be9797"}}},{"cell_type":"code","source":["dict_freq_int=dict((c, i) for i, c in enumerate(se))\ndict_freq_int[\"fill\"]=len(se)\ndict_freq_int[\"other\"]=len(se)+1\nprint(dict_freq_int)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dbeab3bd-3edc-4606-800a-9573b475f00b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"{'Rd6': 0, 'Bd2': 1, 'Ne8': 2, 'gf5': 3, 'g4': 4, 'Qh4': 5, 'Kh1': 6, 'Qe6': 7, 'Nb5': 8, 'Ng5': 9, 'Nc4': 10, 'Bg5': 11, 'Ba4': 12, 'Bc6': 13, 'Qf7': 14, 'Bd5': 15, 'Bg2': 16, 'Rc7': 17, 'Ba6': 18, 'Na5': 19, 'Kg7': 20, 'Rc6': 21, 'Bh6': 22, 'Bb2': 23, 'Bc4': 24, 'Kd2': 25, 'Ne4': 26, 'Qd8': 27, 'Nc6': 28, 'Qd1': 29, 'a5': 30, 'Rd5': 31, 'Bh5': 32, 'Nc5': 33, 'Rc8': 34, 'a3': 35, 'Nd6': 36, 'h3': 37, 'Rf8': 38, 'Nf5': 39, 'Ne6': 40, 'Nh4': 41, 'Qc3': 42, 'Qg6': 43, 'Ra6': 44, 'Qb8': 45, 'Bf5': 46, 'Rb6': 47, 'Bb5': 48, 'Nge2': 49, 'Qc5': 50, 'Be2': 51, 'c6': 52, 'Bf6': 53, 'Qa7': 54, 'de5': 55, 'Bg3': 56, 'Qf3': 57, 'Bc1': 58, 'h4': 59, 'h6': 60, 'Qg5': 61, 'Qb3': 62, 'cb5': 63, 'Kd8': 64, 'Re5': 65, 'Ra8': 66, 'c4': 67, 'ab3': 68, 'a4': 69, 'Rc3': 70, 'bc6': 71, 'Qf4': 72, 'f6': 73, 'Qd4': 74, 'O-O': 75, 'gf3': 76, 'Rfd8': 77, 'g6': 78, 'bc4': 79, 'Rc5': 80, 'Nd3': 81, 'Bc7': 82, 'Nd2': 83, 'Qa4': 84, 'Kh7': 85, 'Rf4': 86, 'Qd6': 87, 'Bc3': 88, 'Qe4': 89, 'gf6': 90, 'Nfd7': 91, 'Ra7': 92, 'Nb8': 93, 'Nb6': 94, 'Bf2': 95, 'Rae1': 96, 'Re7': 97, 'Kg2': 98, 'Rd3': 99, 'b5': 100, 'Qb5': 101, 'Ne5': 102, 'fe6': 103, 'Ba3': 104, 'c3': 105, 'Ng6': 106, 'Qf6': 107, 'Qc6': 108, 'Qc1': 109, 'Bf1': 110, 'ab5': 111, 'Bb4': 112, 'Nf3': 113, 'Qg3': 114, 'Re3': 115, 'Kg8': 116, 'Qa3': 117, 'Qc2': 118, 'Nbd7': 119, 'ab4': 120, 'b3': 121, 'Bh4': 122, 'Re2': 123, 'Bd4': 124, 'O-O-O': 125, 'Bc8': 126, 'f3': 127, 'Qd5': 128, 'Re1': 129, 'Bf8': 130, 'Kf8': 131, 'Ba5': 132, 'd6': 133, 'Kf2': 134, 'e3': 135, 'Qa6': 136, 'Ne7': 137, 'Kf1': 138, 'Qc7': 139, 'Ra3': 140, 'Na6': 141, 'Bc5': 142, 'Bh3': 143, 'Rd2': 144, 'Rfd1': 145, 'Rfe1': 146, 'Qf5': 147, 'Bf3': 148, 'ef5': 149, 'c5': 150, 'dc5': 151, 'f4': 152, 'Rb1': 153, 'Nf6': 154, 'ed4': 155, 'Qh5': 156, 'de4': 157, 'Rb5': 158, 'Kb1': 159, 'Bf7': 160, 'Ra2': 161, 'Bd6': 162, 'Kh8': 163, 'Bc2': 164, 'Ne3': 165, 'Rfe8': 166, 'e4': 167, 'Ke7': 168, 'Qe3': 169, 'Bd7': 170, 'Rc1': 171, 'Re8': 172, 'Rac1': 173, 'cd4': 174, 'Qe2': 175, 'Nbd2': 176, 'Nd4': 177, 'a6': 178, 'Ra5': 179, 'Bg4': 180, 'Rc4': 181, 'Rd1': 182, 'Nd5': 183, 'Rad8': 184, 'Nb4': 185, 'Rd8': 186, 'Na3': 187, 'Rb2': 188, 'Ng3': 189, 'fe5': 190, 'fe4': 191, 'bc3': 192, 'Be7': 193, 'g3': 194, 'Rc2': 195, 'Re6': 196, 'Kd7': 197, 'Na4': 198, 'Qd3': 199, 'Ra4': 200, 'Rg8': 201, 'Qa2': 202, 'Ng4': 203, 'Bg7': 204, 'b4': 205, 'Bd8': 206, 'Qa5': 207, 'dc6': 208, 'Qe7': 209, 'Bb6': 210, 'Be5': 211, 'Qd7': 212, 'Rg1': 213, 'Be6': 214, 'Nh5': 215, 'Qd2': 216, 'h5': 217, 'Qe5': 218, 'Bf4': 219, 'Qb4': 220, 'Qh3': 221, 'Rf1': 222, 'Nb3': 223, 'd4': 224, 'Qb2': 225, 'Bb3': 226, 'Rb7': 227, 'Nf8': 228, 'Qc8': 229, 'b6': 230, 'Rfc1': 231, 'Kf7': 232, 'bc5': 233, 'Rad1': 234, 'Rf7': 235, 'Rb4': 236, 'Qe1': 237, 'Re4': 238, 'Qg4': 239, 'Nh6': 240, 'Be4': 241, 'Bb7': 242, 'e6': 243, 'Qe8': 244, 'Rf6': 245, 'Nf1': 246, 'Rf3': 247, 'hg5': 248, 'Ke2': 249, 'Rb8': 250, 'e5': 251, 'hg6': 252, 'ed5': 253, 'ef6': 254, 'Qf2': 255, 'cd5': 256, 'Nc2': 257, 'Rae8': 258, 'Nf4': 259, 'Rd7': 260, 'Qc4': 261, 'Nd7': 262, 'd3': 263, 'Kg1': 264, 'Nc7': 265, 'Bd3': 266, 'Be3': 267, 'g5': 268, 'dc4': 269, 'Bg6': 270, 'Qb6': 271, 'Qb7': 272, 'Kh2': 273, 'Rf5': 274, 'Rf2': 275, 'd5': 276, 'Nc3': 277, 'ef4': 278, 'Rfc8': 279, 'Nf7': 280, 'Rd4': 281, 'Ne2': 282, 'Rac8': 283, 'Qh6': 284, 'Ra1': 285, 'f5': 286, 'fill': 287, 'other': 288}\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["{'Rd6': 0, 'Bd2': 1, 'Ne8': 2, 'gf5': 3, 'g4': 4, 'Qh4': 5, 'Kh1': 6, 'Qe6': 7, 'Nb5': 8, 'Ng5': 9, 'Nc4': 10, 'Bg5': 11, 'Ba4': 12, 'Bc6': 13, 'Qf7': 14, 'Bd5': 15, 'Bg2': 16, 'Rc7': 17, 'Ba6': 18, 'Na5': 19, 'Kg7': 20, 'Rc6': 21, 'Bh6': 22, 'Bb2': 23, 'Bc4': 24, 'Kd2': 25, 'Ne4': 26, 'Qd8': 27, 'Nc6': 28, 'Qd1': 29, 'a5': 30, 'Rd5': 31, 'Bh5': 32, 'Nc5': 33, 'Rc8': 34, 'a3': 35, 'Nd6': 36, 'h3': 37, 'Rf8': 38, 'Nf5': 39, 'Ne6': 40, 'Nh4': 41, 'Qc3': 42, 'Qg6': 43, 'Ra6': 44, 'Qb8': 45, 'Bf5': 46, 'Rb6': 47, 'Bb5': 48, 'Nge2': 49, 'Qc5': 50, 'Be2': 51, 'c6': 52, 'Bf6': 53, 'Qa7': 54, 'de5': 55, 'Bg3': 56, 'Qf3': 57, 'Bc1': 58, 'h4': 59, 'h6': 60, 'Qg5': 61, 'Qb3': 62, 'cb5': 63, 'Kd8': 64, 'Re5': 65, 'Ra8': 66, 'c4': 67, 'ab3': 68, 'a4': 69, 'Rc3': 70, 'bc6': 71, 'Qf4': 72, 'f6': 73, 'Qd4': 74, 'O-O': 75, 'gf3': 76, 'Rfd8': 77, 'g6': 78, 'bc4': 79, 'Rc5': 80, 'Nd3': 81, 'Bc7': 82, 'Nd2': 83, 'Qa4': 84, 'Kh7': 85, 'Rf4': 86, 'Qd6': 87, 'Bc3': 88, 'Qe4': 89, 'gf6': 90, 'Nfd7': 91, 'Ra7': 92, 'Nb8': 93, 'Nb6': 94, 'Bf2': 95, 'Rae1': 96, 'Re7': 97, 'Kg2': 98, 'Rd3': 99, 'b5': 100, 'Qb5': 101, 'Ne5': 102, 'fe6': 103, 'Ba3': 104, 'c3': 105, 'Ng6': 106, 'Qf6': 107, 'Qc6': 108, 'Qc1': 109, 'Bf1': 110, 'ab5': 111, 'Bb4': 112, 'Nf3': 113, 'Qg3': 114, 'Re3': 115, 'Kg8': 116, 'Qa3': 117, 'Qc2': 118, 'Nbd7': 119, 'ab4': 120, 'b3': 121, 'Bh4': 122, 'Re2': 123, 'Bd4': 124, 'O-O-O': 125, 'Bc8': 126, 'f3': 127, 'Qd5': 128, 'Re1': 129, 'Bf8': 130, 'Kf8': 131, 'Ba5': 132, 'd6': 133, 'Kf2': 134, 'e3': 135, 'Qa6': 136, 'Ne7': 137, 'Kf1': 138, 'Qc7': 139, 'Ra3': 140, 'Na6': 141, 'Bc5': 142, 'Bh3': 143, 'Rd2': 144, 'Rfd1': 145, 'Rfe1': 146, 'Qf5': 147, 'Bf3': 148, 'ef5': 149, 'c5': 150, 'dc5': 151, 'f4': 152, 'Rb1': 153, 'Nf6': 154, 'ed4': 155, 'Qh5': 156, 'de4': 157, 'Rb5': 158, 'Kb1': 159, 'Bf7': 160, 'Ra2': 161, 'Bd6': 162, 'Kh8': 163, 'Bc2': 164, 'Ne3': 165, 'Rfe8': 166, 'e4': 167, 'Ke7': 168, 'Qe3': 169, 'Bd7': 170, 'Rc1': 171, 'Re8': 172, 'Rac1': 173, 'cd4': 174, 'Qe2': 175, 'Nbd2': 176, 'Nd4': 177, 'a6': 178, 'Ra5': 179, 'Bg4': 180, 'Rc4': 181, 'Rd1': 182, 'Nd5': 183, 'Rad8': 184, 'Nb4': 185, 'Rd8': 186, 'Na3': 187, 'Rb2': 188, 'Ng3': 189, 'fe5': 190, 'fe4': 191, 'bc3': 192, 'Be7': 193, 'g3': 194, 'Rc2': 195, 'Re6': 196, 'Kd7': 197, 'Na4': 198, 'Qd3': 199, 'Ra4': 200, 'Rg8': 201, 'Qa2': 202, 'Ng4': 203, 'Bg7': 204, 'b4': 205, 'Bd8': 206, 'Qa5': 207, 'dc6': 208, 'Qe7': 209, 'Bb6': 210, 'Be5': 211, 'Qd7': 212, 'Rg1': 213, 'Be6': 214, 'Nh5': 215, 'Qd2': 216, 'h5': 217, 'Qe5': 218, 'Bf4': 219, 'Qb4': 220, 'Qh3': 221, 'Rf1': 222, 'Nb3': 223, 'd4': 224, 'Qb2': 225, 'Bb3': 226, 'Rb7': 227, 'Nf8': 228, 'Qc8': 229, 'b6': 230, 'Rfc1': 231, 'Kf7': 232, 'bc5': 233, 'Rad1': 234, 'Rf7': 235, 'Rb4': 236, 'Qe1': 237, 'Re4': 238, 'Qg4': 239, 'Nh6': 240, 'Be4': 241, 'Bb7': 242, 'e6': 243, 'Qe8': 244, 'Rf6': 245, 'Nf1': 246, 'Rf3': 247, 'hg5': 248, 'Ke2': 249, 'Rb8': 250, 'e5': 251, 'hg6': 252, 'ed5': 253, 'ef6': 254, 'Qf2': 255, 'cd5': 256, 'Nc2': 257, 'Rae8': 258, 'Nf4': 259, 'Rd7': 260, 'Qc4': 261, 'Nd7': 262, 'd3': 263, 'Kg1': 264, 'Nc7': 265, 'Bd3': 266, 'Be3': 267, 'g5': 268, 'dc4': 269, 'Bg6': 270, 'Qb6': 271, 'Qb7': 272, 'Kh2': 273, 'Rf5': 274, 'Rf2': 275, 'd5': 276, 'Nc3': 277, 'ef4': 278, 'Rfc8': 279, 'Nf7': 280, 'Rd4': 281, 'Ne2': 282, 'Rac8': 283, 'Qh6': 284, 'Ra1': 285, 'f5': 286, 'fill': 287, 'other': 288}\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["Saving the dictionary to the DBFS."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1486f732-6fbe-4583-8159-a03e57d540be"}}},{"cell_type":"code","source":["with open(\"/tmp/dic_freq_int_completeGames_15k.txt\", \"w+\") as fp:\n    json.dump(dict_freq_int, fp)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8d8da1e9-f0b8-4891-a696-cd337fef85cb"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.cp(\"file:/tmp/dic_freq_int_completeGames_15k.txt\", \"dbfs:/tmp/dic_freq_int_completeGames_15k.txt\")\ndisplay(dbutils.fs.ls(\"/tmp/dic_freq_int_completeGames_15k.txt\"))\ndbutils.fs.cp(\"dbfs:/tmp/dic_freq_int_completeGames_15k.txt\", \"/FileStore/dic_freq_int_completeGames_15k.txt\")\n#community.cloud.databricks.com/files/dic_freq_int_completeGames_15k.txt?o=5604700270830977"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cdf0dfe4-1e47-4461-8d4b-47fc200676b1"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["I read the moves of the parsed matches for the model without the frequencies and code the matches with the newly computed dictionary<br>\nThen we padding the moves in the match by adding a null move until we bring all the matches to the same length. With the padding added, all that remains is to divide the moves into slide windows of 5 timesteps."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a85b9337-58cc-4897-8bd2-6caa389d36b0"}}},{"cell_type":"code","source":["manipulated_x=[]\nmanipulated_y=[]\nconta=0\nfor el in game_man:\n    el2=[]\n    for e in el:\n      if e in dict_freq_int.keys():\n        el2.append(dict_freq_int[e])\n      else:\n        el2.append(dict_freq_int[\"other\"])\n        \n    n = medium-len(el2)\n    lfill=[int(dict_freq_int[\"fill\"]) for i in range(0,n)]\n    lista=el2+lfill\n    seq=lista\n    for i in range(len(seq)):\n          #get the last index\n          lastIndex = i + n_steps\n          #if lastIndex is greater than length of sequence then break\n          if lastIndex > len(seq) - 1:\n              break\n          #Create input and output sequence\n          seq_X, seq_y = seq[i:lastIndex], seq[lastIndex]\n          manipulated_x.append(seq_X)\n          manipulated_y.append(seq_y)\n          pass\n    if conta%500==0:\n      print(conta)    \n    conta+=1"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8b556882-136d-4b8b-b1fc-a561d66845ae"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"0\n500\n1000\n1500\n2000\n2500\n3000\n3500\n4000\n4500\n5000\n5500\n6000\n6500\n7000\n7500\n8000\n8500\n9000\n9500\n10000\n10500\n11000\n11500\n12000\n12500\n13000\n13500\n14000\n14500\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["0\n500\n1000\n1500\n2000\n2500\n3000\n3500\n4000\n4500\n5000\n5500\n6000\n6500\n7000\n7500\n8000\n8500\n9000\n9500\n10000\n10500\n11000\n11500\n12000\n12500\n13000\n13500\n14000\n14500\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["In this cell we see how the data are structured"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3ccc1c36-0a9c-4a41-a17b-843bf5121521"}}},{"cell_type":"code","source":["for i in range(0,len(manipulated_x[:1000])):\n  print(manipulated_x[i],manipulated_y[i])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5c6c6d11-de18-4801-83eb-e80dbb496d55"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"[224, 276, 67, 243, 277] 154\n[276, 67, 243, 277, 154] 256\n[67, 243, 277, 154, 256] 253\n[243, 277, 154, 256, 253] 11\n[277, 154, 256, 253, 11] 193\n[154, 256, 253, 11, 193] 135\n[256, 253, 11, 193, 135] 26\n[253, 11, 193, 135, 26] 193\n[11, 193, 135, 26, 193] 277\n[193, 135, 26, 193, 277] 206\n[135, 26, 193, 277, 206] 288\n[26, 193, 277, 206, 288] 82\n[193, 277, 206, 288, 82] 288\n[277, 206, 288, 82, 288] 153\n[206, 288, 82, 288, 153] 10\n[288, 82, 288, 153, 10] 24\n[82, 288, 153, 10, 24] 269\n[288, 153, 10, 24, 269] 282\n[153, 10, 24, 269, 282] 75\n[10, 24, 269, 282, 75] 277\n[24, 269, 282, 75, 277] 230\n[269, 282, 75, 277, 230] 276\n[282, 75, 277, 230, 276] 141\n[75, 277, 230, 276, 141] 162\n[277, 230, 276, 141, 162] 186\n[230, 276, 141, 162, 186] 104\n[276, 141, 162, 186, 104] 242\n[141, 162, 186, 104, 242] 167\n[162, 186, 104, 242, 167] 73\n[186, 104, 242, 167, 73] 249\n[104, 242, 167, 73, 249] 265\n[242, 167, 73, 249, 265] 288\n[167, 73, 249, 265, 288] 18\n[73, 249, 265, 288, 18] 288\n[249, 265, 288, 18, 288] 232\n[265, 288, 18, 288, 232] 4\n[288, 18, 288, 232, 4] 268\n[18, 288, 232, 4, 268] 59\n[288, 232, 4, 268, 59] 60\n[232, 4, 268, 59, 60] 288\n[4, 268, 59, 60, 288] 172\n[268, 59, 60, 288, 172] 127\n[59, 60, 288, 172, 127] 242\n[60, 288, 172, 127, 242] 248\n[288, 172, 127, 242, 248] 288\n[172, 127, 242, 248, 288] 133\n[127, 242, 248, 288, 133] 183\n[242, 248, 288, 133, 183] 183\n[248, 288, 133, 183, 183] 15\n[288, 133, 183, 183, 15] 288\n[133, 183, 183, 15, 288] 105\n[183, 183, 15, 288, 105] 288\n[183, 15, 288, 105, 288] 196\n[15, 288, 105, 288, 196] 288\n[288, 105, 288, 196, 288] 116\n[105, 288, 196, 288, 116] 288\n[288, 196, 288, 116, 288] 13\n[196, 288, 116, 288, 13] 288\n[288, 116, 288, 13, 288] 232\n[116, 288, 13, 288, 232] 66\n[288, 13, 288, 232, 66] 170\n[13, 288, 232, 66, 170] 288\n[288, 232, 66, 170, 288] 287\n[232, 66, 170, 288, 287] 287\n[66, 170, 288, 287, 287] 287\n[170, 288, 287, 287, 287] 287\n[288, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[167, 276, 253, 128, 277] 207\n[276, 253, 128, 277, 207] 224\n[253, 128, 277, 207, 224] 154\n[128, 277, 207, 224, 154] 113\n[277, 207, 224, 154, 113] 52\n[207, 224, 154, 113, 52] 102\n[224, 154, 113, 52, 102] 46\n[154, 113, 52, 102, 46] 4\n[113, 52, 102, 46, 4] 241\n[52, 102, 46, 4, 241] 127\n[102, 46, 4, 241, 127] 15\n[46, 4, 241, 127, 15] 35\n[4, 241, 127, 15, 35] 119\n[241, 127, 15, 35, 119] 267\n[127, 15, 35, 119, 267] 102\n[15, 35, 119, 267, 102] 55\n[35, 119, 267, 102, 55] 203\n[119, 267, 102, 55, 203] 124\n[267, 102, 55, 203, 124] 243\n[102, 55, 203, 124, 243] 205\n[55, 203, 124, 243, 205] 27\n[203, 124, 243, 205, 27] 183\n[124, 243, 205, 27, 183] 128\n[243, 205, 27, 183, 128] 67\n[205, 27, 183, 128, 67] 165\n[27, 183, 128, 67, 165] 256\n[183, 128, 67, 165, 256] 288\n[128, 67, 165, 256, 288] 208\n[67, 165, 256, 288, 208] 71\n[165, 256, 288, 208, 71] 182\n[256, 288, 208, 71, 182] 193\n[288, 208, 71, 182, 193] 18\n[208, 71, 182, 193, 18] 75\n[71, 182, 193, 18, 75] 249\n[182, 193, 18, 75, 249] 288\n[193, 18, 75, 249, 288] 171\n[18, 75, 249, 288, 171] 77\n[75, 249, 288, 171, 77] 288\n[249, 288, 171, 77, 288] 150\n[288, 171, 77, 288, 150] 142\n[171, 77, 288, 150, 142] 182\n[77, 288, 150, 142, 182] 182\n[288, 150, 142, 182, 182] 142\n[150, 142, 182, 182, 142] 233\n[142, 182, 182, 142, 233] 78\n[182, 182, 142, 233, 78] 52\n[182, 142, 233, 78, 52] 188\n[142, 233, 78, 52, 188] 144\n[233, 78, 52, 188, 144] 287\n[78, 52, 188, 144, 287] 287\n[52, 188, 144, 287, 287] 287\n[188, 144, 287, 287, 287] 287\n[144, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[167, 251, 113, 28, 24] 142\n[251, 113, 28, 24, 142] 105\n[113, 28, 24, 142, 105] 154\n[28, 24, 142, 105, 154] 263\n[24, 142, 105, 154, 263] 133\n[142, 105, 154, 263, 133] 226\n[105, 154, 263, 133, 226] 75\n[154, 263, 133, 226, 75] 176\n[263, 133, 226, 75, 176] 214\n[133, 226, 75, 176, 214] 75\n[226, 75, 176, 214, 75] 212\n[75, 176, 214, 75, 212] 129\n[176, 214, 75, 212, 129] 166\n[214, 75, 212, 129, 166] 246\n[75, 212, 129, 166, 246] 137\n[212, 129, 166, 246, 137] 189\n[129, 166, 246, 137, 189] 180\n[166, 246, 137, 189, 180] 37\n[246, 137, 189, 180, 37] 214\n[137, 189, 180, 37, 214] 11\n[189, 180, 37, 214, 11] 163\n[180, 37, 214, 11, 163] 53\n[37, 214, 11, 163, 53] 90\n[214, 11, 163, 53, 90] 224\n[11, 163, 53, 90, 224] 155\n[163, 53, 90, 224, 155] 174\n[53, 90, 224, 155, 174] 112\n[90, 224, 155, 174, 112] 115\n[224, 155, 174, 112, 115] 201\n[155, 174, 112, 115, 201] 276\n[174, 112, 115, 201, 276] 143\n[112, 115, 201, 276, 143] 74\n[115, 201, 276, 143, 74] 288\n[201, 276, 143, 74, 288] 220\n[276, 143, 74, 288, 220] 150\n[143, 74, 288, 220, 150] 42\n[74, 288, 220, 150, 42] 180\n[288, 220, 150, 42, 180] 164\n[220, 150, 42, 180, 164] 288\n[150, 42, 180, 164, 288] 288\n[42, 180, 164, 288, 288] 100\n[180, 164, 288, 288, 100] 205\n[164, 288, 288, 100, 205] 34\n[288, 288, 100, 205, 34] 266\n[288, 100, 205, 34, 266] 67\n[100, 205, 34, 266, 67] 164\n[205, 34, 266, 67, 164] 32\n[34, 266, 67, 164, 32] 215\n[266, 67, 164, 32, 215] 288\n[67, 164, 32, 215, 288] 107\n[164, 32, 215, 288, 107] 116\n[32, 215, 288, 107, 116] 288\n[215, 288, 107, 116, 288] 287\n[288, 107, 116, 288, 287] 287\n[107, 116, 288, 287, 287] 287\n[116, 288, 287, 287, 287] 287\n[288, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[167, 276, 253, 128, 277] 207\n[276, 253, 128, 277, 207] 224\n[253, 128, 277, 207, 224] 243\n[128, 277, 207, 224, 243] 113\n[277, 207, 224, 243, 113] 52\n[207, 224, 243, 113, 52] 266\n[224, 243, 113, 52, 266] 154\n[243, 113, 52, 266, 154] 75\n[113, 52, 266, 154, 75] 193\n[52, 266, 154, 75, 193] 129\n[266, 154, 75, 193, 129] 119\n[154, 75, 193, 129, 119] 102\n[75, 193, 129, 119, 102] 75\n[193, 129, 119, 102, 75] 11\n[129, 119, 102, 75, 11] 27\n[119, 102, 75, 11, 27] 57\n[102, 75, 11, 27, 57] 172\n[75, 11, 27, 57, 172] 234\n[11, 27, 57, 172, 234] 228\n[27, 57, 172, 234, 228] 26\n[57, 172, 234, 228, 26] 106\n[172, 234, 228, 26, 106] 59\n[234, 228, 26, 106, 59] 102\n[228, 26, 106, 59, 102] 55\n[26, 106, 59, 102, 55] 26\n[106, 59, 102, 55, 26] 241\n[59, 102, 55, 26, 241] 139\n[102, 55, 26, 241, 139] 193\n[55, 26, 241, 139, 193] 209\n[26, 241, 139, 193, 209] 217\n[241, 139, 193, 209, 217] 170\n[139, 193, 209, 217, 170] 60\n[193, 209, 217, 170, 60] 288\n[209, 217, 170, 60, 288] 72\n[217, 170, 60, 288, 72] 217\n[170, 60, 288, 72, 217] 284\n[60, 288, 72, 217, 284] 286\n[288, 72, 217, 284, 286] 254\n[72, 217, 284, 286, 254] 14\n[217, 284, 286, 254, 14] 115\n[284, 286, 254, 14, 115] 163\n[286, 254, 14, 115, 163] 288\n[254, 14, 115, 163, 288] 201\n[14, 115, 163, 288, 201] 288\n[115, 163, 288, 201, 288] 287\n[163, 288, 201, 288, 287] 287\n[288, 201, 288, 287, 287] 287\n[201, 288, 287, 287, 287] 287\n[288, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[224, 243, 113, 154, 67] 276\n[243, 113, 154, 67, 276] 277\n[113, 154, 67, 276, 277] 269\n[154, 67, 276, 277, 269] 167\n[67, 276, 277, 269, 167] 112\n[276, 277, 269, 167, 112] 11\n[277, 269, 167, 112, 11] 150\n[269, 167, 112, 11, 150] 24\n[167, 112, 11, 150, 24] 174\n[112, 11, 150, 24, 174] 177\n[11, 150, 24, 174, 177] 207\n[150, 24, 174, 177, 207] 1\n[24, 174, 177, 207, 1] 75\n[174, 177, 207, 1, 75] 257\n[177, 207, 1, 75, 257] 88\n[207, 1, 75, 257, 88] 88\n[1, 75, 257, 88, 88] 61\n[75, 257, 88, 88, 61] 175\n[257, 88, 88, 61, 175] 288\n[88, 88, 61, 175, 288] 125\n[88, 61, 175, 288, 125] 89\n[61, 175, 288, 125, 89] 288\n[175, 288, 125, 89, 288] 78\n[288, 125, 89, 288, 78] 165\n[125, 89, 288, 78, 165] 251\n[89, 288, 78, 165, 251] 152\n[288, 78, 165, 251, 152] 214\n[78, 165, 251, 152, 214] 266\n[165, 251, 152, 214, 266] 72\n[251, 152, 214, 266, 72] 288\n[152, 214, 266, 72, 288] 5\n[214, 266, 72, 288, 5] 288\n[266, 72, 288, 5, 288] 84\n[72, 288, 5, 288, 84] 245\n[288, 5, 288, 84, 245] 28\n[5, 288, 84, 245, 28] 196\n[288, 84, 245, 28, 196] 177\n[84, 245, 28, 196, 177] 239\n[245, 28, 196, 177, 239] 202\n[28, 196, 177, 239, 202] 270\n[196, 177, 239, 202, 270] 252\n[177, 239, 202, 270, 252] 288\n[239, 202, 270, 252, 288] 288\n[202, 270, 252, 288, 288] 43\n[270, 252, 288, 288, 43] 163\n[252, 288, 288, 43, 163] 156\n[288, 288, 43, 163, 156] 116\n[288, 43, 163, 156, 116] 287\n[43, 163, 156, 116, 287] 287\n[163, 156, 116, 287, 287] 287\n[156, 116, 287, 287, 287] 287\n[116, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[224, 154, 67, 78, 194] 204\n[154, 67, 78, 194, 204] 16\n[67, 78, 194, 204, 16] 75\n[78, 194, 204, 16, 75] 277\n[194, 204, 16, 75, 277] 133\n[204, 16, 75, 277, 133] 113\n[16, 75, 277, 133, 113] 119\n[75, 277, 133, 113, 119] 75\n[277, 133, 113, 119, 75] 251\n[133, 113, 119, 75, 251] 167\n[113, 119, 75, 251, 167] 155\n[119, 75, 251, 167, 155] 177\n[75, 251, 167, 155, 177] 102\n[251, 167, 155, 177, 102] 37\n[167, 155, 177, 102, 37] 172\n[155, 177, 102, 37, 172] 129\n[177, 102, 37, 172, 129] 91\n[102, 37, 172, 129, 91] 267\n[37, 172, 129, 91, 267] 228\n[172, 129, 91, 267, 228] 121\n[129, 91, 267, 228, 121] 170\n[91, 267, 228, 121, 170] 118\n[267, 228, 121, 170, 118] 28\n[228, 121, 170, 118, 28] 216\n[121, 170, 118, 28, 216] 178\n[170, 118, 28, 216, 178] 234\n[118, 28, 216, 178, 234] 34\n[28, 216, 178, 234, 34] 11\n[216, 178, 234, 34, 11] 53\n[178, 234, 34, 11, 53] 53\n[234, 34, 11, 53, 53] 107\n[34, 11, 53, 53, 107] 288\n[11, 53, 53, 107, 288] 102\n[53, 53, 107, 288, 102] 183\n[53, 107, 288, 102, 183] 288\n[107, 288, 102, 183, 288] 288\n[288, 102, 183, 288, 288] 40\n[102, 183, 288, 288, 40] 40\n[183, 288, 288, 40, 40] 214\n[288, 288, 40, 40, 214] 152\n[288, 40, 40, 214, 152] 28\n[40, 40, 214, 152, 28] 165\n[40, 214, 152, 28, 165] 288\n[214, 152, 28, 165, 288] 255\n[152, 28, 165, 288, 255] 126\n[28, 165, 288, 255, 126] 183\n[165, 288, 255, 126, 183] 286\n[288, 255, 126, 183, 286] 149\n[255, 126, 183, 286, 149] 46\n[126, 183, 286, 149, 46] 4\n[183, 286, 149, 46, 4] 126\n[286, 149, 46, 4, 126] 172\n[149, 46, 4, 126, 172] 172\n[46, 4, 126, 172, 172] 268\n[4, 126, 172, 172, 268] 163\n[126, 172, 172, 268, 163] 154\n[172, 172, 268, 163, 154] 196\n[172, 268, 163, 154, 196] 150\n[268, 163, 154, 196, 150] 137\n[163, 154, 196, 150, 137] 288\n[154, 196, 150, 137, 288] 288\n[196, 150, 137, 288, 288] 271\n[150, 137, 288, 288, 271] 288\n[137, 288, 288, 271, 288] 27\n[288, 288, 271, 288, 27] 245\n[288, 271, 288, 27, 245] 90\n[271, 288, 27, 245, 90] 287\n[288, 27, 245, 90, 287] 287\n[27, 245, 90, 287, 287] 287\n[245, 90, 287, 287, 287] 287\n[67, 243, 194, 276, 16] 154\n[243, 194, 276, 16, 154] 113\n[194, 276, 16, 154, 113] 193\n[276, 16, 154, 113, 193] 121\n[16, 154, 113, 193, 121] 75\n[154, 113, 193, 121, 75] 75\n[113, 193, 121, 75, 75] 150\n[193, 121, 75, 75, 150] 23\n[121, 75, 75, 150, 23] 28\n[75, 75, 150, 23, 28] 135\n[75, 150, 23, 28, 135] 230\n[150, 23, 28, 135, 230] 277\n[23, 28, 135, 230, 277] 242\n[28, 135, 230, 277, 242] 256\n[135, 230, 277, 242, 256] 183\n[230, 277, 242, 256, 183] 183\n[277, 242, 256, 183, 183] 128\n[242, 256, 183, 183, 128] 224\n[256, 183, 183, 128, 224] 184\n[183, 183, 128, 224, 184] 102\n[183, 128, 224, 184, 102] 87\n[128, 224, 184, 102, 87] 151\n[224, 184, 102, 87, 151] 50\n[184, 102, 87, 151, 50] 175\n[102, 87, 151, 50, 175] 102\n[87, 151, 50, 175, 102] 242\n[151, 50, 175, 102, 242] 139\n[50, 175, 102, 242, 139] 16\n[175, 102, 242, 139, 16] 142\n[102, 242, 139, 16, 142] 145\n[242, 139, 16, 142, 145] 30\n[139, 16, 142, 145, 30] 186\n[16, 142, 145, 30, 186] 186\n[142, 145, 30, 186, 186] 182\n[145, 30, 186, 186, 182] 182\n[30, 186, 186, 182, 182] 29\n[186, 186, 182, 182, 29] 212\n[186, 182, 182, 29, 212] 212\n[182, 182, 29, 212, 212] 262\n[182, 29, 212, 212, 262] 13\n[29, 212, 212, 262, 13] 154\n[212, 212, 262, 13, 154] 98\n[212, 262, 13, 154, 98] 131\n[262, 13, 154, 98, 131] 288\n[13, 154, 98, 131, 288] 168\n[154, 98, 131, 288, 168] 4\n[98, 131, 288, 168, 4] 60\n[131, 288, 168, 4, 60] 59\n[288, 168, 4, 60, 59] 78\n[168, 4, 60, 59, 78] 48\n[4, 60, 59, 78, 48] 217\n[60, 59, 78, 48, 217] 268\n[59, 78, 48, 217, 268] 183\n[78, 48, 217, 268, 183] 249\n[48, 217, 268, 183, 249] 265\n[217, 268, 183, 249, 265] 266\n[268, 183, 249, 265, 266] 183\n[183, 249, 265, 266, 183] 24\n[249, 265, 266, 183, 24] 288\n[265, 266, 183, 24, 288] 35\n[266, 183, 24, 288, 35] 137\n[183, 24, 288, 35, 137] 167\n[24, 288, 35, 137, 167] 28\n[288, 35, 137, 167, 28] 152\n[35, 137, 167, 28, 152] 177\n[137, 167, 28, 152, 177] 124\n[167, 28, 152, 177, 124] 124\n[28, 152, 177, 124, 124] 287\n[152, 177, 124, 124, 287] 287\n[177, 124, 124, 287, 287] 287\n[167, 150, 113, 133, 224] 174\n[150, 113, 133, 224, 174] 177\n[113, 133, 224, 174, 177] 154\n[133, 224, 174, 177, 154] 277\n[224, 174, 177, 154, 277] 178\n[174, 177, 154, 277, 178] 267\n[177, 154, 277, 178, 267] 243\n[154, 277, 178, 267, 243] 127\n[277, 178, 267, 243, 127] 100\n[178, 267, 243, 127, 100] 4\n[267, 243, 127, 100, 4] 60\n[243, 127, 100, 4, 60] 216\n[127, 100, 4, 60, 216] 119\n[100, 4, 60, 216, 119] 125\n[4, 60, 216, 119, 125] 242\n[60, 216, 119, 125, 242] 59\n[216, 119, 125, 242, 59] 205\n[119, 125, 242, 59, 205] 198\n[125, 242, 59, 205, 198] 276\n[242, 59, 205, 198, 276] 143\n[59, 205, 198, 276, 143] 268\n[205, 198, 276, 143, 268] 16\n[198, 276, 143, 268, 16] 288\n[276, 143, 268, 16, 288] 288\n[143, 268, 16, 288, 288] 157\n[268, 16, 288, 288, 157] 268\n[16, 288, 288, 157, 268] 183\n[288, 288, 157, 268, 183] 238\n[288, 157, 268, 183, 238] 248\n[157, 268, 183, 238, 248] 11\n[268, 183, 238, 248, 11] 207\n[183, 238, 248, 11, 207] 152\n[238, 248, 11, 207, 152] 288\n[248, 11, 207, 152, 288] 40\n[11, 207, 152, 288, 40] 103\n[207, 152, 288, 40, 103] 196\n[152, 288, 40, 103, 196] 232\n[288, 40, 103, 196, 232] 199\n[40, 103, 196, 232, 199] 204\n[103, 196, 232, 199, 204] 147\n[196, 232, 199, 204, 147] 116\n[232, 199, 204, 147, 116] 31\n[199, 204, 147, 116, 31] 84\n[204, 147, 116, 31, 84] 97\n[147, 116, 31, 84, 97] 287\n[116, 31, 84, 97, 287] 287\n[31, 84, 97, 287, 287] 287\n[84, 97, 287, 287, 287] 287\n[97, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[167, 150, 113, 28, 224] 174\n[150, 113, 28, 224, 174] 177\n[113, 28, 224, 174, 177] 154\n[28, 224, 174, 177, 154] 277\n[224, 174, 177, 154, 277] 251\n[174, 177, 154, 277, 251] 288\n[177, 154, 277, 251, 288] 133\n[154, 277, 251, 288, 133] 11\n[277, 251, 288, 133, 11] 178\n[251, 288, 133, 11, 178] 187\n[288, 133, 11, 178, 187] 100\n[133, 11, 178, 187, 100] 53\n[11, 178, 187, 100, 53] 90\n[178, 187, 100, 53, 90] 183\n[187, 100, 53, 90, 183] 286\n[100, 53, 90, 183, 286] 105\n[53, 90, 183, 286, 105] 204\n[90, 183, 286, 105, 204] 149\n[183, 286, 105, 204, 149] 46\n[286, 105, 204, 149, 46] 257\n[105, 204, 149, 46, 257] 214\n[204, 149, 46, 257, 214] 69\n[149, 46, 257, 214, 69] 75\n[46, 257, 214, 69, 75] 111\n[257, 214, 69, 75, 111] 111\n[214, 69, 75, 111, 111] 66\n[69, 75, 111, 111, 66] 288\n[75, 111, 111, 66, 288] 288\n[111, 111, 66, 288, 288] 272\n[111, 66, 288, 288, 272] 4\n[66, 288, 288, 272, 4] 137\n[288, 288, 272, 4, 137] 16\n[288, 272, 4, 137, 16] 183\n[272, 4, 137, 16, 183] 15\n[4, 137, 16, 183, 15] 209\n[137, 16, 183, 15, 209] 59\n[16, 183, 15, 209, 59] 34\n[183, 15, 209, 59, 34] 268\n[15, 209, 59, 34, 268] 80\n[209, 59, 34, 268, 80] 241\n[59, 34, 268, 80, 241] 276\n[34, 268, 80, 241, 276] 183\n[268, 80, 241, 276, 183] 15\n[80, 241, 276, 183, 15] 15\n[241, 276, 183, 15, 15] 167\n[276, 183, 15, 15, 167] 249\n[183, 15, 15, 167, 249] 218\n[15, 15, 167, 249, 218] 226\n[15, 167, 249, 218, 226] 139\n[167, 249, 218, 226, 139] 288\n[249, 218, 226, 139, 288] 205\n[218, 226, 139, 288, 205] 78\n[226, 139, 288, 205, 78] 252\n[139, 288, 205, 78, 252] 43\n[288, 205, 78, 252, 43] 131\n[205, 78, 252, 43, 131] 114\n[78, 252, 43, 131, 114] 114\n[252, 43, 131, 114, 114] 288\n[43, 131, 114, 114, 288] 192\n[131, 114, 114, 288, 192] 192\n[114, 114, 288, 192, 192] 70\n[114, 288, 192, 192, 70] 15\n[288, 192, 192, 70, 15] 288\n[192, 192, 70, 15, 288] 287\n[192, 70, 15, 288, 287] 287\n[70, 15, 288, 287, 287] 287\n[15, 288, 287, 287, 287] 287\n[288, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[121, 251, 23, 28, 67] 142\n[251, 23, 28, 67, 142] 135\n[23, 28, 67, 142, 135] 133\n[28, 67, 142, 135, 133] 35\n[67, 142, 135, 133, 35] 46\n[142, 135, 133, 35, 46] 277\n[135, 133, 35, 46, 277] 214\n[133, 35, 46, 277, 214] 113\n[35, 46, 277, 214, 113] 60\n[46, 277, 214, 113, 60] 51\n[277, 214, 113, 60, 51] 154\n[214, 113, 60, 51, 154] 75\n[113, 60, 51, 154, 75] 276\n[60, 51, 154, 75, 276] 256\n[51, 154, 75, 276, 256] 15\n[154, 75, 276, 256, 15] 205\n[75, 276, 256, 15, 205] 167\n[276, 256, 15, 205, 167] 183\n[256, 15, 205, 167, 183] 183\n[15, 205, 167, 183, 183] 233\n[205, 167, 183, 183, 233] 288\n[167, 183, 183, 233, 288] 148\n[183, 183, 233, 288, 148] 75\n[183, 233, 288, 148, 75] 62\n[233, 288, 148, 75, 62] 288\n[288, 148, 75, 62, 288] 224\n[148, 75, 62, 288, 224] 19\n[75, 62, 288, 224, 19] 42\n[62, 288, 224, 19, 42] 52\n[288, 224, 19, 42, 52] 276\n[224, 19, 42, 52, 276] 73\n[19, 42, 52, 276, 73] 133\n[42, 52, 276, 73, 133] 106\n[52, 276, 73, 133, 106] 241\n[276, 73, 133, 106, 241] 102\n[73, 133, 106, 241, 102] 152\n[133, 106, 241, 102, 152] 288\n[106, 241, 102, 152, 288] 146\n[241, 102, 152, 288, 146] 288\n[102, 152, 288, 146, 288] 225\n[152, 288, 146, 288, 225] 10\n[288, 146, 288, 225, 10] 42\n[146, 288, 225, 10, 42] 19\n[288, 225, 10, 42, 19] 288\n[225, 10, 42, 19, 288] 230\n[10, 42, 19, 288, 230] 167\n[42, 19, 288, 230, 167] 233\n[19, 288, 230, 167, 233] 251\n[288, 230, 167, 233, 251] 67\n[230, 167, 233, 251, 67] 243\n[167, 233, 251, 67, 243] 271\n[233, 251, 67, 243, 271] 6\n[251, 67, 243, 271, 6] 150\n[67, 243, 271, 6, 150] 288\n[243, 271, 6, 150, 288] 288\n[271, 6, 150, 288, 288] 270\n[6, 150, 288, 288, 270] 287\n[150, 288, 288, 270, 287] 287\n[288, 288, 270, 287, 287] 287\n[288, 270, 287, 287, 287] 287\n[270, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[224, 243, 67, 230, 35] 242\n[243, 67, 230, 35, 242] 277\n[67, 230, 35, 242, 277] 286\n[230, 35, 242, 277, 286] 276\n[35, 242, 277, 286, 276] 154\n[242, 277, 286, 276, 154] 194\n[277, 286, 276, 154, 194] 141\n[286, 276, 154, 194, 141] 16\n[276, 154, 194, 141, 16] 33\n[154, 194, 141, 16, 33] 288\n[194, 141, 16, 33, 288] 162\n[141, 16, 33, 288, 162] 75\n[16, 33, 288, 162, 75] 211\n[33, 288, 162, 75, 211] 118\n[288, 162, 75, 211, 118] 75\n[162, 75, 211, 118, 75] 182\n[75, 211, 118, 75, 182] 209\n[211, 118, 75, 182, 209] 267\n[118, 75, 182, 209, 267] 288\n[75, 182, 209, 267, 288] 173\n[182, 209, 267, 288, 173] 288\n[209, 267, 288, 173, 288] 26\n[267, 288, 173, 288, 26] 26\n[288, 173, 288, 26, 26] 259\n[173, 288, 26, 26, 259] 150\n[288, 26, 26, 259, 150] 208\n[26, 26, 259, 150, 208] 13\n[26, 259, 150, 208, 13] 81\n[259, 150, 208, 13, 81] 53\n[150, 208, 13, 81, 53] 127\n[208, 13, 81, 53, 127] 33\n[13, 81, 53, 127, 33] 205\n[81, 53, 127, 33, 205] 81\n[53, 127, 33, 205, 81] 99\n[127, 33, 205, 81, 99] 276\n[33, 205, 81, 99, 276] 152\n[205, 81, 99, 276, 152] 269\n[81, 99, 276, 152, 269] 261\n[99, 276, 152, 269, 261] 16\n[276, 152, 269, 261, 16] 98\n[152, 269, 261, 16, 98] 235\n[269, 261, 16, 98, 235] 100\n[261, 16, 98, 235, 100] 172\n[16, 98, 235, 100, 172] 288\n[98, 235, 100, 172, 288] 251\n[235, 100, 172, 288, 251] 260\n[100, 172, 288, 251, 260] 7\n[172, 288, 251, 260, 7] 7\n[288, 251, 260, 7, 7] 196\n[251, 260, 7, 7, 196] 288\n[260, 7, 7, 196, 288] 278\n[7, 7, 196, 288, 278] 288\n[7, 196, 288, 278, 288] 260\n[196, 288, 278, 288, 260] 260\n[288, 278, 288, 260, 260] 97\n[278, 288, 260, 260, 97] 97\n[288, 260, 260, 97, 97] 193\n[260, 260, 97, 97, 193] 69\n[260, 97, 97, 193, 69] 232\n[97, 97, 193, 69, 232] 124\n[97, 193, 69, 232, 124] 162\n[193, 69, 232, 124, 162] 167\n[69, 232, 124, 162, 167] 78\n[232, 124, 162, 167, 78] 37\n[124, 162, 167, 78, 37] 288\n[162, 167, 78, 37, 288] 88\n[167, 78, 37, 288, 88] 82\n[78, 37, 288, 88, 82] 112\n[37, 288, 88, 82, 112] 206\n[288, 88, 82, 112, 206] 251\n[167, 251, 113, 28, 24] 154\n[251, 113, 28, 24, 154] 263\n[113, 28, 24, 154, 263] 142\n[28, 24, 154, 263, 142] 105\n[24, 154, 263, 142, 105] 133\n[154, 263, 142, 105, 133] 226\n[263, 142, 105, 133, 226] 214\n[142, 105, 133, 226, 214] 164\n[105, 133, 226, 214, 164] 75\n[133, 226, 214, 164, 75] 75\n[226, 214, 164, 75, 75] 212\n[214, 164, 75, 75, 212] 224\n[164, 75, 75, 212, 224] 155\n[75, 75, 212, 224, 155] 174\n[75, 212, 224, 155, 174] 124\n[212, 224, 155, 174, 124] 177\n[224, 155, 174, 124, 177] 177\n[155, 174, 124, 177, 177] 74\n[174, 124, 177, 177, 74] 150\n[124, 177, 177, 74, 150] 199\n[177, 177, 74, 150, 199] 100\n[177, 74, 150, 199, 100] 182\n[74, 150, 199, 100, 182] 77\n[150, 199, 100, 182, 77] 277\n[199, 100, 182, 77, 277] 24\n[100, 182, 77, 277, 24] 114\n[182, 77, 277, 24, 114] 239\n[77, 277, 24, 114, 239] 219\n[277, 24, 114, 239, 219] 7\n[24, 114, 239, 219, 7] 121\n[114, 239, 219, 7, 121] 51\n[239, 219, 7, 121, 51] 282\n[219, 7, 121, 51, 282] 276\n[7, 121, 51, 282, 276] 11\n[121, 51, 282, 276, 11] 157\n[51, 282, 276, 11, 157] 259\n[282, 276, 11, 157, 259] 218\n[276, 11, 157, 259, 218] 53\n[11, 157, 259, 218, 53] 107\n[157, 259, 218, 53, 107] 241\n[259, 218, 53, 107, 241] 172\n[218, 53, 107, 241, 172] 288\n[53, 107, 241, 172, 288] 66\n[107, 241, 172, 288, 66] 183\n[241, 172, 288, 66, 183] 7\n[172, 288, 66, 183, 7] 265\n[288, 66, 183, 7, 265] 229\n[66, 183, 7, 265, 229] 288\n[183, 7, 265, 229, 288] 287\n[7, 265, 229, 288, 287] 287\n[265, 229, 288, 287, 287] 287\n[229, 288, 287, 287, 287] 287\n[288, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[167, 52, 224, 276, 253] 256\n[52, 224, 276, 253, 256] 266\n[224, 276, 253, 256, 266] 28\n[276, 253, 256, 266, 28] 105\n[253, 256, 266, 28, 105] 154\n[256, 266, 28, 105, 154] 219\n[266, 28, 105, 154, 219] 180\n[28, 105, 154, 219, 180] 62\n[105, 154, 219, 180, 62] 212\n[154, 219, 180, 62, 212] 83\n[219, 180, 62, 212, 83] 243\n[180, 62, 212, 83, 243] 288\n[62, 212, 83, 243, 288] 162\n[212, 83, 243, 288, 162] 102\n[83, 243, 288, 162, 102] 211\n[243, 288, 162, 102, 211] 55\n[288, 162, 102, 211, 55] 215\n[162, 102, 211, 55, 215] 267\n[102, 211, 55, 215, 267] 178\n[211, 55, 215, 267, 178] 37\n[55, 215, 267, 178, 37] 102\n[215, 267, 178, 37, 102] 110\n[267, 178, 37, 102, 110] 46\n[178, 37, 102, 110, 46] 4\n[37, 102, 110, 46, 4] 81\n[102, 110, 46, 4, 81] 266\n[110, 46, 4, 81, 266] 266\n[46, 4, 81, 266, 266] 288\n[4, 81, 266, 266, 288] 34\n[81, 266, 266, 288, 34] 213\n[266, 266, 288, 34, 213] 73\n[266, 288, 34, 213, 73] 60\n[288, 34, 213, 73, 60] 78\n[34, 213, 73, 60, 78] 125\n[213, 73, 60, 78, 125] 224\n[73, 60, 78, 125, 224] 124\n[60, 78, 125, 224, 124] 168\n[78, 125, 224, 124, 168] 113\n[125, 224, 124, 168, 113] 251\n[224, 124, 168, 113, 251] 99\n[124, 168, 113, 251, 99] 155\n[168, 113, 251, 99, 155] 129\n[113, 251, 99, 155, 129] 131\n[251, 99, 155, 129, 131] 281\n[99, 155, 129, 131, 281] 108\n[155, 129, 131, 281, 108] 220\n[129, 131, 281, 108, 220] 287\n[131, 281, 108, 220, 287] 287\n[281, 108, 220, 287, 287] 287\n[108, 220, 287, 287, 287] 287\n[220, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[167, 150, 113, 133, 105] 154\n[150, 113, 133, 105, 154] 51\n[113, 133, 105, 154, 51] 28\n[133, 105, 154, 51, 28] 224\n[105, 154, 51, 28, 224] 174\n[154, 51, 28, 224, 174] 174\n[51, 28, 224, 174, 174] 243\n[28, 224, 174, 174, 243] 277\n[224, 174, 174, 243, 277] 193\n[174, 174, 243, 277, 193] 75\n[174, 243, 277, 193, 75] 75\n[243, 277, 193, 75, 75] 266\n[277, 193, 75, 75, 266] 178\n[193, 75, 75, 266, 178] 35\n[75, 75, 266, 178, 35] 100\n[75, 266, 178, 35, 100] 251\n[266, 178, 35, 100, 251] 55\n[178, 35, 100, 251, 55] 55\n[35, 100, 251, 55, 55] 262\n[100, 251, 55, 55, 262] 118\n[251, 55, 55, 262, 118] 78\n[55, 55, 262, 118, 78] 22\n[55, 262, 118, 78, 22] 172\n[262, 118, 78, 22, 172] 234\n[118, 78, 22, 172, 234] 242\n[78, 22, 172, 234, 242] 146\n[22, 172, 234, 242, 146] 34\n[172, 234, 242, 146, 34] 175\n[234, 242, 146, 34, 175] 139\n[242, 146, 34, 175, 139] 288\n[146, 34, 175, 139, 288] 288\n[34, 175, 139, 288, 288] 59\n[175, 139, 288, 288, 59] 33\n[139, 288, 288, 59, 33] 217\n[288, 288, 59, 33, 217] 182\n[288, 59, 33, 217, 182] 182\n[59, 33, 217, 182, 182] 186\n[33, 217, 182, 182, 186] 186\n[217, 182, 182, 186, 186] 27\n[182, 182, 186, 186, 27] 267\n[182, 186, 186, 27, 267] 139\n[186, 186, 27, 267, 139] 219\n[186, 27, 267, 139, 219] 223\n[27, 267, 139, 219, 223] 29\n[267, 139, 219, 223, 29] 33\n[139, 219, 223, 29, 33] 26\n[219, 223, 29, 33, 26] 26\n[223, 29, 33, 26, 26] 241\n[29, 33, 26, 26, 241] 19\n[33, 26, 26, 241, 19] 242\n[26, 26, 241, 19, 242] 288\n[26, 241, 19, 242, 288] 60\n[241, 19, 242, 288, 60] 33\n[19, 242, 288, 60, 33] 11\n[242, 288, 60, 33, 11] 26\n[288, 60, 33, 11, 26] 193\n[60, 33, 11, 26, 193] 209\n[33, 11, 26, 193, 209] 118\n[11, 26, 193, 209, 118] 33\n[26, 193, 209, 118, 33] 205\n[193, 209, 118, 33, 205] 262\n[209, 118, 33, 205, 262] 139\n[118, 33, 205, 262, 139] 244\n[33, 205, 262, 139, 244] 9\n[205, 262, 139, 244, 9] 228\n[262, 139, 244, 9, 228] 26\n[139, 244, 9, 228, 26] 262\n[244, 9, 228, 26, 262] 212\n[9, 228, 26, 262, 212] 287\n[228, 26, 262, 212, 287] 287\n[167, 251, 113, 28, 24] 142\n[251, 113, 28, 24, 142] 105\n[113, 28, 24, 142, 105] 154\n[28, 24, 142, 105, 154] 263\n[24, 142, 105, 154, 263] 133\n[142, 105, 154, 263, 133] 226\n[105, 154, 263, 133, 226] 75\n[154, 263, 133, 226, 75] 176\n[263, 133, 226, 75, 176] 172\n[133, 226, 75, 176, 172] 37\n[226, 75, 176, 172, 37] 214\n[75, 176, 172, 37, 214] 75\n[176, 172, 37, 214, 75] 276\n[172, 37, 214, 75, 276] 9\n[37, 214, 75, 276, 9] 60\n[214, 75, 276, 9, 60] 40\n[75, 276, 9, 60, 40] 196\n[276, 9, 60, 40, 196] 253\n[9, 60, 40, 196, 253] 183\n[60, 40, 196, 253, 183] 26\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["[224, 276, 67, 243, 277] 154\n[276, 67, 243, 277, 154] 256\n[67, 243, 277, 154, 256] 253\n[243, 277, 154, 256, 253] 11\n[277, 154, 256, 253, 11] 193\n[154, 256, 253, 11, 193] 135\n[256, 253, 11, 193, 135] 26\n[253, 11, 193, 135, 26] 193\n[11, 193, 135, 26, 193] 277\n[193, 135, 26, 193, 277] 206\n[135, 26, 193, 277, 206] 288\n[26, 193, 277, 206, 288] 82\n[193, 277, 206, 288, 82] 288\n[277, 206, 288, 82, 288] 153\n[206, 288, 82, 288, 153] 10\n[288, 82, 288, 153, 10] 24\n[82, 288, 153, 10, 24] 269\n[288, 153, 10, 24, 269] 282\n[153, 10, 24, 269, 282] 75\n[10, 24, 269, 282, 75] 277\n[24, 269, 282, 75, 277] 230\n[269, 282, 75, 277, 230] 276\n[282, 75, 277, 230, 276] 141\n[75, 277, 230, 276, 141] 162\n[277, 230, 276, 141, 162] 186\n[230, 276, 141, 162, 186] 104\n[276, 141, 162, 186, 104] 242\n[141, 162, 186, 104, 242] 167\n[162, 186, 104, 242, 167] 73\n[186, 104, 242, 167, 73] 249\n[104, 242, 167, 73, 249] 265\n[242, 167, 73, 249, 265] 288\n[167, 73, 249, 265, 288] 18\n[73, 249, 265, 288, 18] 288\n[249, 265, 288, 18, 288] 232\n[265, 288, 18, 288, 232] 4\n[288, 18, 288, 232, 4] 268\n[18, 288, 232, 4, 268] 59\n[288, 232, 4, 268, 59] 60\n[232, 4, 268, 59, 60] 288\n[4, 268, 59, 60, 288] 172\n[268, 59, 60, 288, 172] 127\n[59, 60, 288, 172, 127] 242\n[60, 288, 172, 127, 242] 248\n[288, 172, 127, 242, 248] 288\n[172, 127, 242, 248, 288] 133\n[127, 242, 248, 288, 133] 183\n[242, 248, 288, 133, 183] 183\n[248, 288, 133, 183, 183] 15\n[288, 133, 183, 183, 15] 288\n[133, 183, 183, 15, 288] 105\n[183, 183, 15, 288, 105] 288\n[183, 15, 288, 105, 288] 196\n[15, 288, 105, 288, 196] 288\n[288, 105, 288, 196, 288] 116\n[105, 288, 196, 288, 116] 288\n[288, 196, 288, 116, 288] 13\n[196, 288, 116, 288, 13] 288\n[288, 116, 288, 13, 288] 232\n[116, 288, 13, 288, 232] 66\n[288, 13, 288, 232, 66] 170\n[13, 288, 232, 66, 170] 288\n[288, 232, 66, 170, 288] 287\n[232, 66, 170, 288, 287] 287\n[66, 170, 288, 287, 287] 287\n[170, 288, 287, 287, 287] 287\n[288, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[167, 276, 253, 128, 277] 207\n[276, 253, 128, 277, 207] 224\n[253, 128, 277, 207, 224] 154\n[128, 277, 207, 224, 154] 113\n[277, 207, 224, 154, 113] 52\n[207, 224, 154, 113, 52] 102\n[224, 154, 113, 52, 102] 46\n[154, 113, 52, 102, 46] 4\n[113, 52, 102, 46, 4] 241\n[52, 102, 46, 4, 241] 127\n[102, 46, 4, 241, 127] 15\n[46, 4, 241, 127, 15] 35\n[4, 241, 127, 15, 35] 119\n[241, 127, 15, 35, 119] 267\n[127, 15, 35, 119, 267] 102\n[15, 35, 119, 267, 102] 55\n[35, 119, 267, 102, 55] 203\n[119, 267, 102, 55, 203] 124\n[267, 102, 55, 203, 124] 243\n[102, 55, 203, 124, 243] 205\n[55, 203, 124, 243, 205] 27\n[203, 124, 243, 205, 27] 183\n[124, 243, 205, 27, 183] 128\n[243, 205, 27, 183, 128] 67\n[205, 27, 183, 128, 67] 165\n[27, 183, 128, 67, 165] 256\n[183, 128, 67, 165, 256] 288\n[128, 67, 165, 256, 288] 208\n[67, 165, 256, 288, 208] 71\n[165, 256, 288, 208, 71] 182\n[256, 288, 208, 71, 182] 193\n[288, 208, 71, 182, 193] 18\n[208, 71, 182, 193, 18] 75\n[71, 182, 193, 18, 75] 249\n[182, 193, 18, 75, 249] 288\n[193, 18, 75, 249, 288] 171\n[18, 75, 249, 288, 171] 77\n[75, 249, 288, 171, 77] 288\n[249, 288, 171, 77, 288] 150\n[288, 171, 77, 288, 150] 142\n[171, 77, 288, 150, 142] 182\n[77, 288, 150, 142, 182] 182\n[288, 150, 142, 182, 182] 142\n[150, 142, 182, 182, 142] 233\n[142, 182, 182, 142, 233] 78\n[182, 182, 142, 233, 78] 52\n[182, 142, 233, 78, 52] 188\n[142, 233, 78, 52, 188] 144\n[233, 78, 52, 188, 144] 287\n[78, 52, 188, 144, 287] 287\n[52, 188, 144, 287, 287] 287\n[188, 144, 287, 287, 287] 287\n[144, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[167, 251, 113, 28, 24] 142\n[251, 113, 28, 24, 142] 105\n[113, 28, 24, 142, 105] 154\n[28, 24, 142, 105, 154] 263\n[24, 142, 105, 154, 263] 133\n[142, 105, 154, 263, 133] 226\n[105, 154, 263, 133, 226] 75\n[154, 263, 133, 226, 75] 176\n[263, 133, 226, 75, 176] 214\n[133, 226, 75, 176, 214] 75\n[226, 75, 176, 214, 75] 212\n[75, 176, 214, 75, 212] 129\n[176, 214, 75, 212, 129] 166\n[214, 75, 212, 129, 166] 246\n[75, 212, 129, 166, 246] 137\n[212, 129, 166, 246, 137] 189\n[129, 166, 246, 137, 189] 180\n[166, 246, 137, 189, 180] 37\n[246, 137, 189, 180, 37] 214\n[137, 189, 180, 37, 214] 11\n[189, 180, 37, 214, 11] 163\n[180, 37, 214, 11, 163] 53\n[37, 214, 11, 163, 53] 90\n[214, 11, 163, 53, 90] 224\n[11, 163, 53, 90, 224] 155\n[163, 53, 90, 224, 155] 174\n[53, 90, 224, 155, 174] 112\n[90, 224, 155, 174, 112] 115\n[224, 155, 174, 112, 115] 201\n[155, 174, 112, 115, 201] 276\n[174, 112, 115, 201, 276] 143\n[112, 115, 201, 276, 143] 74\n[115, 201, 276, 143, 74] 288\n[201, 276, 143, 74, 288] 220\n[276, 143, 74, 288, 220] 150\n[143, 74, 288, 220, 150] 42\n[74, 288, 220, 150, 42] 180\n[288, 220, 150, 42, 180] 164\n[220, 150, 42, 180, 164] 288\n[150, 42, 180, 164, 288] 288\n[42, 180, 164, 288, 288] 100\n[180, 164, 288, 288, 100] 205\n[164, 288, 288, 100, 205] 34\n[288, 288, 100, 205, 34] 266\n[288, 100, 205, 34, 266] 67\n[100, 205, 34, 266, 67] 164\n[205, 34, 266, 67, 164] 32\n[34, 266, 67, 164, 32] 215\n[266, 67, 164, 32, 215] 288\n[67, 164, 32, 215, 288] 107\n[164, 32, 215, 288, 107] 116\n[32, 215, 288, 107, 116] 288\n[215, 288, 107, 116, 288] 287\n[288, 107, 116, 288, 287] 287\n[107, 116, 288, 287, 287] 287\n[116, 288, 287, 287, 287] 287\n[288, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[167, 276, 253, 128, 277] 207\n[276, 253, 128, 277, 207] 224\n[253, 128, 277, 207, 224] 243\n[128, 277, 207, 224, 243] 113\n[277, 207, 224, 243, 113] 52\n[207, 224, 243, 113, 52] 266\n[224, 243, 113, 52, 266] 154\n[243, 113, 52, 266, 154] 75\n[113, 52, 266, 154, 75] 193\n[52, 266, 154, 75, 193] 129\n[266, 154, 75, 193, 129] 119\n[154, 75, 193, 129, 119] 102\n[75, 193, 129, 119, 102] 75\n[193, 129, 119, 102, 75] 11\n[129, 119, 102, 75, 11] 27\n[119, 102, 75, 11, 27] 57\n[102, 75, 11, 27, 57] 172\n[75, 11, 27, 57, 172] 234\n[11, 27, 57, 172, 234] 228\n[27, 57, 172, 234, 228] 26\n[57, 172, 234, 228, 26] 106\n[172, 234, 228, 26, 106] 59\n[234, 228, 26, 106, 59] 102\n[228, 26, 106, 59, 102] 55\n[26, 106, 59, 102, 55] 26\n[106, 59, 102, 55, 26] 241\n[59, 102, 55, 26, 241] 139\n[102, 55, 26, 241, 139] 193\n[55, 26, 241, 139, 193] 209\n[26, 241, 139, 193, 209] 217\n[241, 139, 193, 209, 217] 170\n[139, 193, 209, 217, 170] 60\n[193, 209, 217, 170, 60] 288\n[209, 217, 170, 60, 288] 72\n[217, 170, 60, 288, 72] 217\n[170, 60, 288, 72, 217] 284\n[60, 288, 72, 217, 284] 286\n[288, 72, 217, 284, 286] 254\n[72, 217, 284, 286, 254] 14\n[217, 284, 286, 254, 14] 115\n[284, 286, 254, 14, 115] 163\n[286, 254, 14, 115, 163] 288\n[254, 14, 115, 163, 288] 201\n[14, 115, 163, 288, 201] 288\n[115, 163, 288, 201, 288] 287\n[163, 288, 201, 288, 287] 287\n[288, 201, 288, 287, 287] 287\n[201, 288, 287, 287, 287] 287\n[288, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[224, 243, 113, 154, 67] 276\n[243, 113, 154, 67, 276] 277\n[113, 154, 67, 276, 277] 269\n[154, 67, 276, 277, 269] 167\n[67, 276, 277, 269, 167] 112\n[276, 277, 269, 167, 112] 11\n[277, 269, 167, 112, 11] 150\n[269, 167, 112, 11, 150] 24\n[167, 112, 11, 150, 24] 174\n[112, 11, 150, 24, 174] 177\n[11, 150, 24, 174, 177] 207\n[150, 24, 174, 177, 207] 1\n[24, 174, 177, 207, 1] 75\n[174, 177, 207, 1, 75] 257\n[177, 207, 1, 75, 257] 88\n[207, 1, 75, 257, 88] 88\n[1, 75, 257, 88, 88] 61\n[75, 257, 88, 88, 61] 175\n[257, 88, 88, 61, 175] 288\n[88, 88, 61, 175, 288] 125\n[88, 61, 175, 288, 125] 89\n[61, 175, 288, 125, 89] 288\n[175, 288, 125, 89, 288] 78\n[288, 125, 89, 288, 78] 165\n[125, 89, 288, 78, 165] 251\n[89, 288, 78, 165, 251] 152\n[288, 78, 165, 251, 152] 214\n[78, 165, 251, 152, 214] 266\n[165, 251, 152, 214, 266] 72\n[251, 152, 214, 266, 72] 288\n[152, 214, 266, 72, 288] 5\n[214, 266, 72, 288, 5] 288\n[266, 72, 288, 5, 288] 84\n[72, 288, 5, 288, 84] 245\n[288, 5, 288, 84, 245] 28\n[5, 288, 84, 245, 28] 196\n[288, 84, 245, 28, 196] 177\n[84, 245, 28, 196, 177] 239\n[245, 28, 196, 177, 239] 202\n[28, 196, 177, 239, 202] 270\n[196, 177, 239, 202, 270] 252\n[177, 239, 202, 270, 252] 288\n[239, 202, 270, 252, 288] 288\n[202, 270, 252, 288, 288] 43\n[270, 252, 288, 288, 43] 163\n[252, 288, 288, 43, 163] 156\n[288, 288, 43, 163, 156] 116\n[288, 43, 163, 156, 116] 287\n[43, 163, 156, 116, 287] 287\n[163, 156, 116, 287, 287] 287\n[156, 116, 287, 287, 287] 287\n[116, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[224, 154, 67, 78, 194] 204\n[154, 67, 78, 194, 204] 16\n[67, 78, 194, 204, 16] 75\n[78, 194, 204, 16, 75] 277\n[194, 204, 16, 75, 277] 133\n[204, 16, 75, 277, 133] 113\n[16, 75, 277, 133, 113] 119\n[75, 277, 133, 113, 119] 75\n[277, 133, 113, 119, 75] 251\n[133, 113, 119, 75, 251] 167\n[113, 119, 75, 251, 167] 155\n[119, 75, 251, 167, 155] 177\n[75, 251, 167, 155, 177] 102\n[251, 167, 155, 177, 102] 37\n[167, 155, 177, 102, 37] 172\n[155, 177, 102, 37, 172] 129\n[177, 102, 37, 172, 129] 91\n[102, 37, 172, 129, 91] 267\n[37, 172, 129, 91, 267] 228\n[172, 129, 91, 267, 228] 121\n[129, 91, 267, 228, 121] 170\n[91, 267, 228, 121, 170] 118\n[267, 228, 121, 170, 118] 28\n[228, 121, 170, 118, 28] 216\n[121, 170, 118, 28, 216] 178\n[170, 118, 28, 216, 178] 234\n[118, 28, 216, 178, 234] 34\n[28, 216, 178, 234, 34] 11\n[216, 178, 234, 34, 11] 53\n[178, 234, 34, 11, 53] 53\n[234, 34, 11, 53, 53] 107\n[34, 11, 53, 53, 107] 288\n[11, 53, 53, 107, 288] 102\n[53, 53, 107, 288, 102] 183\n[53, 107, 288, 102, 183] 288\n[107, 288, 102, 183, 288] 288\n[288, 102, 183, 288, 288] 40\n[102, 183, 288, 288, 40] 40\n[183, 288, 288, 40, 40] 214\n[288, 288, 40, 40, 214] 152\n[288, 40, 40, 214, 152] 28\n[40, 40, 214, 152, 28] 165\n[40, 214, 152, 28, 165] 288\n[214, 152, 28, 165, 288] 255\n[152, 28, 165, 288, 255] 126\n[28, 165, 288, 255, 126] 183\n[165, 288, 255, 126, 183] 286\n[288, 255, 126, 183, 286] 149\n[255, 126, 183, 286, 149] 46\n[126, 183, 286, 149, 46] 4\n[183, 286, 149, 46, 4] 126\n[286, 149, 46, 4, 126] 172\n[149, 46, 4, 126, 172] 172\n[46, 4, 126, 172, 172] 268\n[4, 126, 172, 172, 268] 163\n[126, 172, 172, 268, 163] 154\n[172, 172, 268, 163, 154] 196\n[172, 268, 163, 154, 196] 150\n[268, 163, 154, 196, 150] 137\n[163, 154, 196, 150, 137] 288\n[154, 196, 150, 137, 288] 288\n[196, 150, 137, 288, 288] 271\n[150, 137, 288, 288, 271] 288\n[137, 288, 288, 271, 288] 27\n[288, 288, 271, 288, 27] 245\n[288, 271, 288, 27, 245] 90\n[271, 288, 27, 245, 90] 287\n[288, 27, 245, 90, 287] 287\n[27, 245, 90, 287, 287] 287\n[245, 90, 287, 287, 287] 287\n[67, 243, 194, 276, 16] 154\n[243, 194, 276, 16, 154] 113\n[194, 276, 16, 154, 113] 193\n[276, 16, 154, 113, 193] 121\n[16, 154, 113, 193, 121] 75\n[154, 113, 193, 121, 75] 75\n[113, 193, 121, 75, 75] 150\n[193, 121, 75, 75, 150] 23\n[121, 75, 75, 150, 23] 28\n[75, 75, 150, 23, 28] 135\n[75, 150, 23, 28, 135] 230\n[150, 23, 28, 135, 230] 277\n[23, 28, 135, 230, 277] 242\n[28, 135, 230, 277, 242] 256\n[135, 230, 277, 242, 256] 183\n[230, 277, 242, 256, 183] 183\n[277, 242, 256, 183, 183] 128\n[242, 256, 183, 183, 128] 224\n[256, 183, 183, 128, 224] 184\n[183, 183, 128, 224, 184] 102\n[183, 128, 224, 184, 102] 87\n[128, 224, 184, 102, 87] 151\n[224, 184, 102, 87, 151] 50\n[184, 102, 87, 151, 50] 175\n[102, 87, 151, 50, 175] 102\n[87, 151, 50, 175, 102] 242\n[151, 50, 175, 102, 242] 139\n[50, 175, 102, 242, 139] 16\n[175, 102, 242, 139, 16] 142\n[102, 242, 139, 16, 142] 145\n[242, 139, 16, 142, 145] 30\n[139, 16, 142, 145, 30] 186\n[16, 142, 145, 30, 186] 186\n[142, 145, 30, 186, 186] 182\n[145, 30, 186, 186, 182] 182\n[30, 186, 186, 182, 182] 29\n[186, 186, 182, 182, 29] 212\n[186, 182, 182, 29, 212] 212\n[182, 182, 29, 212, 212] 262\n[182, 29, 212, 212, 262] 13\n[29, 212, 212, 262, 13] 154\n[212, 212, 262, 13, 154] 98\n[212, 262, 13, 154, 98] 131\n[262, 13, 154, 98, 131] 288\n[13, 154, 98, 131, 288] 168\n[154, 98, 131, 288, 168] 4\n[98, 131, 288, 168, 4] 60\n[131, 288, 168, 4, 60] 59\n[288, 168, 4, 60, 59] 78\n[168, 4, 60, 59, 78] 48\n[4, 60, 59, 78, 48] 217\n[60, 59, 78, 48, 217] 268\n[59, 78, 48, 217, 268] 183\n[78, 48, 217, 268, 183] 249\n[48, 217, 268, 183, 249] 265\n[217, 268, 183, 249, 265] 266\n[268, 183, 249, 265, 266] 183\n[183, 249, 265, 266, 183] 24\n[249, 265, 266, 183, 24] 288\n[265, 266, 183, 24, 288] 35\n[266, 183, 24, 288, 35] 137\n[183, 24, 288, 35, 137] 167\n[24, 288, 35, 137, 167] 28\n[288, 35, 137, 167, 28] 152\n[35, 137, 167, 28, 152] 177\n[137, 167, 28, 152, 177] 124\n[167, 28, 152, 177, 124] 124\n[28, 152, 177, 124, 124] 287\n[152, 177, 124, 124, 287] 287\n[177, 124, 124, 287, 287] 287\n[167, 150, 113, 133, 224] 174\n[150, 113, 133, 224, 174] 177\n[113, 133, 224, 174, 177] 154\n[133, 224, 174, 177, 154] 277\n[224, 174, 177, 154, 277] 178\n[174, 177, 154, 277, 178] 267\n[177, 154, 277, 178, 267] 243\n[154, 277, 178, 267, 243] 127\n[277, 178, 267, 243, 127] 100\n[178, 267, 243, 127, 100] 4\n[267, 243, 127, 100, 4] 60\n[243, 127, 100, 4, 60] 216\n[127, 100, 4, 60, 216] 119\n[100, 4, 60, 216, 119] 125\n[4, 60, 216, 119, 125] 242\n[60, 216, 119, 125, 242] 59\n[216, 119, 125, 242, 59] 205\n[119, 125, 242, 59, 205] 198\n[125, 242, 59, 205, 198] 276\n[242, 59, 205, 198, 276] 143\n[59, 205, 198, 276, 143] 268\n[205, 198, 276, 143, 268] 16\n[198, 276, 143, 268, 16] 288\n[276, 143, 268, 16, 288] 288\n[143, 268, 16, 288, 288] 157\n[268, 16, 288, 288, 157] 268\n[16, 288, 288, 157, 268] 183\n[288, 288, 157, 268, 183] 238\n[288, 157, 268, 183, 238] 248\n[157, 268, 183, 238, 248] 11\n[268, 183, 238, 248, 11] 207\n[183, 238, 248, 11, 207] 152\n[238, 248, 11, 207, 152] 288\n[248, 11, 207, 152, 288] 40\n[11, 207, 152, 288, 40] 103\n[207, 152, 288, 40, 103] 196\n[152, 288, 40, 103, 196] 232\n[288, 40, 103, 196, 232] 199\n[40, 103, 196, 232, 199] 204\n[103, 196, 232, 199, 204] 147\n[196, 232, 199, 204, 147] 116\n[232, 199, 204, 147, 116] 31\n[199, 204, 147, 116, 31] 84\n[204, 147, 116, 31, 84] 97\n[147, 116, 31, 84, 97] 287\n[116, 31, 84, 97, 287] 287\n[31, 84, 97, 287, 287] 287\n[84, 97, 287, 287, 287] 287\n[97, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[167, 150, 113, 28, 224] 174\n[150, 113, 28, 224, 174] 177\n[113, 28, 224, 174, 177] 154\n[28, 224, 174, 177, 154] 277\n[224, 174, 177, 154, 277] 251\n[174, 177, 154, 277, 251] 288\n[177, 154, 277, 251, 288] 133\n[154, 277, 251, 288, 133] 11\n[277, 251, 288, 133, 11] 178\n[251, 288, 133, 11, 178] 187\n[288, 133, 11, 178, 187] 100\n[133, 11, 178, 187, 100] 53\n[11, 178, 187, 100, 53] 90\n[178, 187, 100, 53, 90] 183\n[187, 100, 53, 90, 183] 286\n[100, 53, 90, 183, 286] 105\n[53, 90, 183, 286, 105] 204\n[90, 183, 286, 105, 204] 149\n[183, 286, 105, 204, 149] 46\n[286, 105, 204, 149, 46] 257\n[105, 204, 149, 46, 257] 214\n[204, 149, 46, 257, 214] 69\n[149, 46, 257, 214, 69] 75\n[46, 257, 214, 69, 75] 111\n[257, 214, 69, 75, 111] 111\n[214, 69, 75, 111, 111] 66\n[69, 75, 111, 111, 66] 288\n[75, 111, 111, 66, 288] 288\n[111, 111, 66, 288, 288] 272\n[111, 66, 288, 288, 272] 4\n[66, 288, 288, 272, 4] 137\n[288, 288, 272, 4, 137] 16\n[288, 272, 4, 137, 16] 183\n[272, 4, 137, 16, 183] 15\n[4, 137, 16, 183, 15] 209\n[137, 16, 183, 15, 209] 59\n[16, 183, 15, 209, 59] 34\n[183, 15, 209, 59, 34] 268\n[15, 209, 59, 34, 268] 80\n[209, 59, 34, 268, 80] 241\n[59, 34, 268, 80, 241] 276\n[34, 268, 80, 241, 276] 183\n[268, 80, 241, 276, 183] 15\n[80, 241, 276, 183, 15] 15\n[241, 276, 183, 15, 15] 167\n[276, 183, 15, 15, 167] 249\n[183, 15, 15, 167, 249] 218\n[15, 15, 167, 249, 218] 226\n[15, 167, 249, 218, 226] 139\n[167, 249, 218, 226, 139] 288\n[249, 218, 226, 139, 288] 205\n[218, 226, 139, 288, 205] 78\n[226, 139, 288, 205, 78] 252\n[139, 288, 205, 78, 252] 43\n[288, 205, 78, 252, 43] 131\n[205, 78, 252, 43, 131] 114\n[78, 252, 43, 131, 114] 114\n[252, 43, 131, 114, 114] 288\n[43, 131, 114, 114, 288] 192\n[131, 114, 114, 288, 192] 192\n[114, 114, 288, 192, 192] 70\n[114, 288, 192, 192, 70] 15\n[288, 192, 192, 70, 15] 288\n[192, 192, 70, 15, 288] 287\n[192, 70, 15, 288, 287] 287\n[70, 15, 288, 287, 287] 287\n[15, 288, 287, 287, 287] 287\n[288, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[121, 251, 23, 28, 67] 142\n[251, 23, 28, 67, 142] 135\n[23, 28, 67, 142, 135] 133\n[28, 67, 142, 135, 133] 35\n[67, 142, 135, 133, 35] 46\n[142, 135, 133, 35, 46] 277\n[135, 133, 35, 46, 277] 214\n[133, 35, 46, 277, 214] 113\n[35, 46, 277, 214, 113] 60\n[46, 277, 214, 113, 60] 51\n[277, 214, 113, 60, 51] 154\n[214, 113, 60, 51, 154] 75\n[113, 60, 51, 154, 75] 276\n[60, 51, 154, 75, 276] 256\n[51, 154, 75, 276, 256] 15\n[154, 75, 276, 256, 15] 205\n[75, 276, 256, 15, 205] 167\n[276, 256, 15, 205, 167] 183\n[256, 15, 205, 167, 183] 183\n[15, 205, 167, 183, 183] 233\n[205, 167, 183, 183, 233] 288\n[167, 183, 183, 233, 288] 148\n[183, 183, 233, 288, 148] 75\n[183, 233, 288, 148, 75] 62\n[233, 288, 148, 75, 62] 288\n[288, 148, 75, 62, 288] 224\n[148, 75, 62, 288, 224] 19\n[75, 62, 288, 224, 19] 42\n[62, 288, 224, 19, 42] 52\n[288, 224, 19, 42, 52] 276\n[224, 19, 42, 52, 276] 73\n[19, 42, 52, 276, 73] 133\n[42, 52, 276, 73, 133] 106\n[52, 276, 73, 133, 106] 241\n[276, 73, 133, 106, 241] 102\n[73, 133, 106, 241, 102] 152\n[133, 106, 241, 102, 152] 288\n[106, 241, 102, 152, 288] 146\n[241, 102, 152, 288, 146] 288\n[102, 152, 288, 146, 288] 225\n[152, 288, 146, 288, 225] 10\n[288, 146, 288, 225, 10] 42\n[146, 288, 225, 10, 42] 19\n[288, 225, 10, 42, 19] 288\n[225, 10, 42, 19, 288] 230\n[10, 42, 19, 288, 230] 167\n[42, 19, 288, 230, 167] 233\n[19, 288, 230, 167, 233] 251\n[288, 230, 167, 233, 251] 67\n[230, 167, 233, 251, 67] 243\n[167, 233, 251, 67, 243] 271\n[233, 251, 67, 243, 271] 6\n[251, 67, 243, 271, 6] 150\n[67, 243, 271, 6, 150] 288\n[243, 271, 6, 150, 288] 288\n[271, 6, 150, 288, 288] 270\n[6, 150, 288, 288, 270] 287\n[150, 288, 288, 270, 287] 287\n[288, 288, 270, 287, 287] 287\n[288, 270, 287, 287, 287] 287\n[270, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[224, 243, 67, 230, 35] 242\n[243, 67, 230, 35, 242] 277\n[67, 230, 35, 242, 277] 286\n[230, 35, 242, 277, 286] 276\n[35, 242, 277, 286, 276] 154\n[242, 277, 286, 276, 154] 194\n[277, 286, 276, 154, 194] 141\n[286, 276, 154, 194, 141] 16\n[276, 154, 194, 141, 16] 33\n[154, 194, 141, 16, 33] 288\n[194, 141, 16, 33, 288] 162\n[141, 16, 33, 288, 162] 75\n[16, 33, 288, 162, 75] 211\n[33, 288, 162, 75, 211] 118\n[288, 162, 75, 211, 118] 75\n[162, 75, 211, 118, 75] 182\n[75, 211, 118, 75, 182] 209\n[211, 118, 75, 182, 209] 267\n[118, 75, 182, 209, 267] 288\n[75, 182, 209, 267, 288] 173\n[182, 209, 267, 288, 173] 288\n[209, 267, 288, 173, 288] 26\n[267, 288, 173, 288, 26] 26\n[288, 173, 288, 26, 26] 259\n[173, 288, 26, 26, 259] 150\n[288, 26, 26, 259, 150] 208\n[26, 26, 259, 150, 208] 13\n[26, 259, 150, 208, 13] 81\n[259, 150, 208, 13, 81] 53\n[150, 208, 13, 81, 53] 127\n[208, 13, 81, 53, 127] 33\n[13, 81, 53, 127, 33] 205\n[81, 53, 127, 33, 205] 81\n[53, 127, 33, 205, 81] 99\n[127, 33, 205, 81, 99] 276\n[33, 205, 81, 99, 276] 152\n[205, 81, 99, 276, 152] 269\n[81, 99, 276, 152, 269] 261\n[99, 276, 152, 269, 261] 16\n[276, 152, 269, 261, 16] 98\n[152, 269, 261, 16, 98] 235\n[269, 261, 16, 98, 235] 100\n[261, 16, 98, 235, 100] 172\n[16, 98, 235, 100, 172] 288\n[98, 235, 100, 172, 288] 251\n[235, 100, 172, 288, 251] 260\n[100, 172, 288, 251, 260] 7\n[172, 288, 251, 260, 7] 7\n[288, 251, 260, 7, 7] 196\n[251, 260, 7, 7, 196] 288\n[260, 7, 7, 196, 288] 278\n[7, 7, 196, 288, 278] 288\n[7, 196, 288, 278, 288] 260\n[196, 288, 278, 288, 260] 260\n[288, 278, 288, 260, 260] 97\n[278, 288, 260, 260, 97] 97\n[288, 260, 260, 97, 97] 193\n[260, 260, 97, 97, 193] 69\n[260, 97, 97, 193, 69] 232\n[97, 97, 193, 69, 232] 124\n[97, 193, 69, 232, 124] 162\n[193, 69, 232, 124, 162] 167\n[69, 232, 124, 162, 167] 78\n[232, 124, 162, 167, 78] 37\n[124, 162, 167, 78, 37] 288\n[162, 167, 78, 37, 288] 88\n[167, 78, 37, 288, 88] 82\n[78, 37, 288, 88, 82] 112\n[37, 288, 88, 82, 112] 206\n[288, 88, 82, 112, 206] 251\n[167, 251, 113, 28, 24] 154\n[251, 113, 28, 24, 154] 263\n[113, 28, 24, 154, 263] 142\n[28, 24, 154, 263, 142] 105\n[24, 154, 263, 142, 105] 133\n[154, 263, 142, 105, 133] 226\n[263, 142, 105, 133, 226] 214\n[142, 105, 133, 226, 214] 164\n[105, 133, 226, 214, 164] 75\n[133, 226, 214, 164, 75] 75\n[226, 214, 164, 75, 75] 212\n[214, 164, 75, 75, 212] 224\n[164, 75, 75, 212, 224] 155\n[75, 75, 212, 224, 155] 174\n[75, 212, 224, 155, 174] 124\n[212, 224, 155, 174, 124] 177\n[224, 155, 174, 124, 177] 177\n[155, 174, 124, 177, 177] 74\n[174, 124, 177, 177, 74] 150\n[124, 177, 177, 74, 150] 199\n[177, 177, 74, 150, 199] 100\n[177, 74, 150, 199, 100] 182\n[74, 150, 199, 100, 182] 77\n[150, 199, 100, 182, 77] 277\n[199, 100, 182, 77, 277] 24\n[100, 182, 77, 277, 24] 114\n[182, 77, 277, 24, 114] 239\n[77, 277, 24, 114, 239] 219\n[277, 24, 114, 239, 219] 7\n[24, 114, 239, 219, 7] 121\n[114, 239, 219, 7, 121] 51\n[239, 219, 7, 121, 51] 282\n[219, 7, 121, 51, 282] 276\n[7, 121, 51, 282, 276] 11\n[121, 51, 282, 276, 11] 157\n[51, 282, 276, 11, 157] 259\n[282, 276, 11, 157, 259] 218\n[276, 11, 157, 259, 218] 53\n[11, 157, 259, 218, 53] 107\n[157, 259, 218, 53, 107] 241\n[259, 218, 53, 107, 241] 172\n[218, 53, 107, 241, 172] 288\n[53, 107, 241, 172, 288] 66\n[107, 241, 172, 288, 66] 183\n[241, 172, 288, 66, 183] 7\n[172, 288, 66, 183, 7] 265\n[288, 66, 183, 7, 265] 229\n[66, 183, 7, 265, 229] 288\n[183, 7, 265, 229, 288] 287\n[7, 265, 229, 288, 287] 287\n[265, 229, 288, 287, 287] 287\n[229, 288, 287, 287, 287] 287\n[288, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[167, 52, 224, 276, 253] 256\n[52, 224, 276, 253, 256] 266\n[224, 276, 253, 256, 266] 28\n[276, 253, 256, 266, 28] 105\n[253, 256, 266, 28, 105] 154\n[256, 266, 28, 105, 154] 219\n[266, 28, 105, 154, 219] 180\n[28, 105, 154, 219, 180] 62\n[105, 154, 219, 180, 62] 212\n[154, 219, 180, 62, 212] 83\n[219, 180, 62, 212, 83] 243\n[180, 62, 212, 83, 243] 288\n[62, 212, 83, 243, 288] 162\n[212, 83, 243, 288, 162] 102\n[83, 243, 288, 162, 102] 211\n[243, 288, 162, 102, 211] 55\n[288, 162, 102, 211, 55] 215\n[162, 102, 211, 55, 215] 267\n[102, 211, 55, 215, 267] 178\n[211, 55, 215, 267, 178] 37\n[55, 215, 267, 178, 37] 102\n[215, 267, 178, 37, 102] 110\n[267, 178, 37, 102, 110] 46\n[178, 37, 102, 110, 46] 4\n[37, 102, 110, 46, 4] 81\n[102, 110, 46, 4, 81] 266\n[110, 46, 4, 81, 266] 266\n[46, 4, 81, 266, 266] 288\n[4, 81, 266, 266, 288] 34\n[81, 266, 266, 288, 34] 213\n[266, 266, 288, 34, 213] 73\n[266, 288, 34, 213, 73] 60\n[288, 34, 213, 73, 60] 78\n[34, 213, 73, 60, 78] 125\n[213, 73, 60, 78, 125] 224\n[73, 60, 78, 125, 224] 124\n[60, 78, 125, 224, 124] 168\n[78, 125, 224, 124, 168] 113\n[125, 224, 124, 168, 113] 251\n[224, 124, 168, 113, 251] 99\n[124, 168, 113, 251, 99] 155\n[168, 113, 251, 99, 155] 129\n[113, 251, 99, 155, 129] 131\n[251, 99, 155, 129, 131] 281\n[99, 155, 129, 131, 281] 108\n[155, 129, 131, 281, 108] 220\n[129, 131, 281, 108, 220] 287\n[131, 281, 108, 220, 287] 287\n[281, 108, 220, 287, 287] 287\n[108, 220, 287, 287, 287] 287\n[220, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[287, 287, 287, 287, 287] 287\n[167, 150, 113, 133, 105] 154\n[150, 113, 133, 105, 154] 51\n[113, 133, 105, 154, 51] 28\n[133, 105, 154, 51, 28] 224\n[105, 154, 51, 28, 224] 174\n[154, 51, 28, 224, 174] 174\n[51, 28, 224, 174, 174] 243\n[28, 224, 174, 174, 243] 277\n[224, 174, 174, 243, 277] 193\n[174, 174, 243, 277, 193] 75\n[174, 243, 277, 193, 75] 75\n[243, 277, 193, 75, 75] 266\n[277, 193, 75, 75, 266] 178\n[193, 75, 75, 266, 178] 35\n[75, 75, 266, 178, 35] 100\n[75, 266, 178, 35, 100] 251\n[266, 178, 35, 100, 251] 55\n[178, 35, 100, 251, 55] 55\n[35, 100, 251, 55, 55] 262\n[100, 251, 55, 55, 262] 118\n[251, 55, 55, 262, 118] 78\n[55, 55, 262, 118, 78] 22\n[55, 262, 118, 78, 22] 172\n[262, 118, 78, 22, 172] 234\n[118, 78, 22, 172, 234] 242\n[78, 22, 172, 234, 242] 146\n[22, 172, 234, 242, 146] 34\n[172, 234, 242, 146, 34] 175\n[234, 242, 146, 34, 175] 139\n[242, 146, 34, 175, 139] 288\n[146, 34, 175, 139, 288] 288\n[34, 175, 139, 288, 288] 59\n[175, 139, 288, 288, 59] 33\n[139, 288, 288, 59, 33] 217\n[288, 288, 59, 33, 217] 182\n[288, 59, 33, 217, 182] 182\n[59, 33, 217, 182, 182] 186\n[33, 217, 182, 182, 186] 186\n[217, 182, 182, 186, 186] 27\n[182, 182, 186, 186, 27] 267\n[182, 186, 186, 27, 267] 139\n[186, 186, 27, 267, 139] 219\n[186, 27, 267, 139, 219] 223\n[27, 267, 139, 219, 223] 29\n[267, 139, 219, 223, 29] 33\n[139, 219, 223, 29, 33] 26\n[219, 223, 29, 33, 26] 26\n[223, 29, 33, 26, 26] 241\n[29, 33, 26, 26, 241] 19\n[33, 26, 26, 241, 19] 242\n[26, 26, 241, 19, 242] 288\n[26, 241, 19, 242, 288] 60\n[241, 19, 242, 288, 60] 33\n[19, 242, 288, 60, 33] 11\n[242, 288, 60, 33, 11] 26\n[288, 60, 33, 11, 26] 193\n[60, 33, 11, 26, 193] 209\n[33, 11, 26, 193, 209] 118\n[11, 26, 193, 209, 118] 33\n[26, 193, 209, 118, 33] 205\n[193, 209, 118, 33, 205] 262\n[209, 118, 33, 205, 262] 139\n[118, 33, 205, 262, 139] 244\n[33, 205, 262, 139, 244] 9\n[205, 262, 139, 244, 9] 228\n[262, 139, 244, 9, 228] 26\n[139, 244, 9, 228, 26] 262\n[244, 9, 228, 26, 262] 212\n[9, 228, 26, 262, 212] 287\n[228, 26, 262, 212, 287] 287\n[167, 251, 113, 28, 24] 142\n[251, 113, 28, 24, 142] 105\n[113, 28, 24, 142, 105] 154\n[28, 24, 142, 105, 154] 263\n[24, 142, 105, 154, 263] 133\n[142, 105, 154, 263, 133] 226\n[105, 154, 263, 133, 226] 75\n[154, 263, 133, 226, 75] 176\n[263, 133, 226, 75, 176] 172\n[133, 226, 75, 176, 172] 37\n[226, 75, 176, 172, 37] 214\n[75, 176, 172, 37, 214] 75\n[176, 172, 37, 214, 75] 276\n[172, 37, 214, 75, 276] 9\n[37, 214, 75, 276, 9] 60\n[214, 75, 276, 9, 60] 40\n[75, 276, 9, 60, 40] 196\n[276, 9, 60, 40, 196] 253\n[9, 60, 40, 196, 253] 183\n[60, 40, 196, 253, 183] 26\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["In the following cells X and Y are saved in the DBFS"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dee34b92-9b28-4767-b8e7-7920c695b893"}}},{"cell_type":"code","source":["with open(\"/tmp/X_slide_windows_10-75_5steps_CompleteGames_freq_15K.txt\", \"w+\") as f:\n    for el in manipulated_x:\n      s=str(el[0])+\",\"+str(el[1])+\",\"+str(el[2])+\",\"+str(el[3])+\",\"+str(el[4])\n      f.write(\"{}\\n\".format(s))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"35bca8b1-2cba-438b-a33d-66bccfcd58d7"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.cp(\"file:/tmp/X_slide_windows_10-75_5steps_CompleteGames_freq_15K.txt\", \"dbfs:/tmp/X_slide_windows_10-75_5steps_CompleteGames_freq_15K.txt\")\ndisplay(dbutils.fs.ls(\"/tmp/X_slide_windows_10-75_5steps_CompleteGames_freq_15K.txt\"))\ndbutils.fs.cp(\"dbfs:/tmp/X_slide_windows_10-75_5steps_CompleteGames_freq_15K.txt\", \"/FileStore/X_slide_windows_10-75_5steps_CompleteGames_freq_15K.txt\")\n#community.cloud.databricks.com/files/X_slide_windows_10-75_5steps_CompleteGames_freq_15K.txt?o=5604700270830977"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2b441c09-900d-4640-8bdd-6fa303a8ad90"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["dbfs:/tmp/X_slide_windows_10-75_5steps_CompleteGames_freq_15K.txt","X_slide_windows_10-75_5steps_CompleteGames_freq_15K.txt",19666114,1654293421000]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"size","type":"\"long\"","metadata":"{}"},{"name":"modificationTime","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/tmp/X_slide_windows_10-75_5steps_CompleteGames_freq_15K.txt</td><td>X_slide_windows_10-75_5steps_CompleteGames_freq_15K.txt</td><td>19666114</td><td>1654293421000</td></tr></tbody></table></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[40]: True","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[40]: True"]}}],"execution_count":0},{"cell_type":"code","source":["with open(\"/tmp/Y_slide_windows_10-75_5steps_CompleteGames_freq_15K.txt\", \"w+\") as f:\n    for el in manipulated_y:\n      f.write(\"{}\\n\".format(str(el)))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"774c6093-1ab5-40c1-846b-76a5398580a2"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.cp(\"file:/tmp/Y_slide_windows_10-75_5steps_CompleteGames_freq_15K.txt\", \"dbfs:/tmp/Y_slide_windows_10-75_5steps_CompleteGames_freq_15K.txt\")\ndisplay(dbutils.fs.ls(\"/tmp/Y_slide_windows_10-75_5steps_CompleteGames_freq_15K.txt\"))\ndbutils.fs.cp(\"dbfs:/tmp/Y_slide_windows_10-75_5steps_CompleteGames_freq_15K.txt\", \"/FileStore/Y_slide_windows_10-75_5steps_CompleteGames_freq_15K.txt\")\n#community.cloud.databricks.com/files/Y_slide_windows_10-75_5steps_CompleteGames_freq_15K.txt?o=5604700270830977"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7f58444f-1e14-4554-b4f0-a2f8a428fc65"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["dbfs:/tmp/Y_slide_windows_10-75_5steps_CompleteGames_freq_15K.txt","Y_slide_windows_10-75_5steps_CompleteGames_freq_15K.txt",3943900,1654293490000]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"size","type":"\"long\"","metadata":"{}"},{"name":"modificationTime","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/tmp/Y_slide_windows_10-75_5steps_CompleteGames_freq_15K.txt</td><td>Y_slide_windows_10-75_5steps_CompleteGames_freq_15K.txt</td><td>3943900</td><td>1654293490000</td></tr></tbody></table></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[44]: True","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[44]: True"]}}],"execution_count":0},{"cell_type":"markdown","source":["Code to read X and Y saved in files"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1c7c54ef-102e-405b-b448-4f99f2c99a69"}}},{"cell_type":"code","source":["dbutils.fs.cp(\"dbfs:/tmp/X_slide_windows_10-75_5steps_CompleteGames_freq_15K.txt\", \"file:/tmp/X_slide_windows_10-75_5steps_CompleteGames_freq_15K.txt\")\nx=[]\nc=0\nfor line in open(\"/tmp/X_slide_windows_10-75_5steps_CompleteGames_freq_15K.txt\",\"r\"):\n    stripped_line = line.strip()\n    s=stripped_line.split(\",\")\n    s=[int(e) for e in s]\n    x.append(s)\n\nx = np.array(x, dtype=int)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1404f923-e32d-4ae3-8fba-9a5528118f97"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.cp(\"dbfs:/tmp/Y_slide_windows_10-75_5steps_CompleteGames_freq_15K.txt\", \"file:/tmp/Y_slide_windows_10-75_5steps_CompleteGames_freq_15K.txt\")\ny=[]\nfor line in open(\"/tmp/Y_slide_windows_10-75_5steps_CompleteGames_freq_15K.txt\",\"r\"):\n  stripped_line = line.strip()\n  y.append(int(stripped_line))\ny = np.array(y, dtype=int)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a3f68ce2-3ab5-40e6-a45f-4e0b741aa81b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["In the following cells, the train and test set are created. 20% of the matches are used for the test set."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f5cdee3b-d3df-40c3-8046-a74a971955be"}}},{"cell_type":"code","source":["print(\"TRAIN X\")\ntrain_freq_x= x[:int(len(x)*0.8)]\nprint(\"The number of elements in train_x is\",len(train_freq_x), \"(80% of the dataset)\")\n\nprint(\"TRAIN Y\")\ntrain_freq_y = y[:int(len(y)*0.8)]\nprint(\"The number of elements in train_y is\",len(train_freq_y), \"(80% of the dataset)\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"43f0fa93-b899-4129-960d-aeadf4a18830"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"TRAIN X\nThe number of elements in train_x are 840000 (80% of the dataset)\nTRAIN Y\nThe number of elements in train_y are 840000 (80% of the dataset)\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["TRAIN X\nThe number of elements in train_x are 840000 (80% of the dataset)\nTRAIN Y\nThe number of elements in train_y are 840000 (80% of the dataset)\n"]}}],"execution_count":0},{"cell_type":"code","source":["print(\"TEST X\")\ntest_freq_x = x[-int(len(x)*0.2):] \nprint(\"The number of elements in test_x is\",len(test_freq_x), \"(20% of the dataset)\")\n\nprint(\"TEST Y\")\ntest_freq_y= y[-int(len(y)*0.2):] \nprint(\"The number of elements in test_y is\",len(test_freq_y), \"(20% of the dataset)\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4a0cc39c-792c-4555-8f51-7388f9457ec2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"TEST X\nThe number of elements in test_x are 210000 (20% of the dataset)\nTEST Y\nThe number of elements in test_y are 210000 (20% of the dataset)\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["TEST X\nThe number of elements in test_x are 210000 (20% of the dataset)\nTEST Y\nThe number of elements in test_y are 210000 (20% of the dataset)\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["In the following cell, One Hot Encoding is applied on the labels of the train set."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1e1344a5-67e0-416d-b0e7-41add9198f91"}}},{"cell_type":"code","source":["train_freq_y = to_categorical(train_freq_y)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7a266894-c788-4f75-92c0-a073d5a4cf92"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["In the following cell we define values that will be used for embedding layer of the model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7b179761-5a84-437d-91c3-598927ff2ce6"}}},{"cell_type":"code","source":["seq_len=train_freq_x.shape[1]\nvocab_size=len(dict_freq_int)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"44c67425-7e22-4551-a327-86badc886632"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Defining a callback to prevent overfitting"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9a0d075b-78d1-4c3d-9a5d-490b8c7b3e3c"}}},{"cell_type":"code","source":["from tensorflow.keras.callbacks import EarlyStopping\ncallback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6b92bf57-f918-4340-ada3-e60078094158"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["In the next cell we have the model used for the train performed on kaggle. The Neural Network used for this model is the same of the first model in the first approach."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a714869b-1393-4b7f-a03f-09c025ad4312"}}},{"cell_type":"code","source":["model = Sequential()\nmodel.add(Embedding(vocab_size, n_steps, input_length=seq_len))\nmodel.add(LSTM(100, return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(100))\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dense(vocab_size, activation='softmax'))\nprint(model.summary())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c3a42ccc-ddef-405c-a122-519fc2d0d7fe"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Model: \"sequential_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n embedding_3 (Embedding)     (None, 5, 5)              1445      \n                                                                 \n lstm_6 (LSTM)               (None, 5, 100)            42400     \n                                                                 \n dropout_3 (Dropout)         (None, 5, 100)            0         \n                                                                 \n lstm_7 (LSTM)               (None, 100)               80400     \n                                                                 \n dense_6 (Dense)             (None, 100)               10100     \n                                                                 \n dense_7 (Dense)             (None, 289)               29189     \n                                                                 \n=================================================================\nTotal params: 163,534\nTrainable params: 163,534\nNon-trainable params: 0\n_________________________________________________________________\nNone\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Model: \"sequential_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n embedding_3 (Embedding)     (None, 5, 5)              1445      \n                                                                 \n lstm_6 (LSTM)               (None, 5, 100)            42400     \n                                                                 \n dropout_3 (Dropout)         (None, 5, 100)            0         \n                                                                 \n lstm_7 (LSTM)               (None, 100)               80400     \n                                                                 \n dense_6 (Dense)             (None, 100)               10100     \n                                                                 \n dense_7 (Dense)             (None, 289)               29189     \n                                                                 \n=================================================================\nTotal params: 163,534\nTrainable params: 163,534\nNon-trainable params: 0\n_________________________________________________________________\nNone\n"]}}],"execution_count":0},{"cell_type":"code","source":["# compile model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n# fit model\nhistory=model.fit(train_freq_x, train_freq_y, batch_size=64, epochs=50, callbacks=[callback])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9d09f0c1-819d-4c9a-8bc3-3e0051677144"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Epoch 1/50\n\r    1/13125 [..............................] - ETA: 21:59:46 - loss: 5.6662 - accuracy: 0.0000e+00\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    2/13125 [..............................] - ETA: 15:34 - loss: 5.6648 - accuracy: 0.1250       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    5/13125 [..............................] - ETA: 7:35 - loss: 5.6599 - accuracy: 0.2062 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    8/13125 [..............................] - ETA: 6:06 - loss: 5.6542 - accuracy: 0.2227\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   11/13125 [..............................] - ETA: 5:28 - loss: 5.6431 - accuracy: 0.2457\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   14/13125 [..............................] - ETA: 5:09 - loss: 5.6285 - accuracy: 0.2489\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   17/13125 [..............................] - ETA: 5:02 - loss: 5.6107 - accuracy: 0.2463\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   20/13125 [..............................] - ETA: 4:53 - loss: 5.5726 - accuracy: 0.2508\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   23/13125 [..............................] - ETA: 4:47 - loss: 5.5153 - accuracy: 0.2520\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   26/13125 [..............................] - ETA: 4:43 - loss: 5.4135 - accuracy: 0.2566\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   29/13125 [..............................] - ETA: 4:39 - loss: 5.3096 - accuracy: 0.2602\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   32/13125 [..............................] - ETA: 4:37 - loss: 5.2175 - accuracy: 0.2637\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   35/13125 [..............................] - ETA: 4:34 - loss: 5.1366 - accuracy: 0.2661\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   38/13125 [..............................] - ETA: 4:32 - loss: 5.0774 - accuracy: 0.2648\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   40/13125 [..............................] - ETA: 4:37 - loss: 5.0379 - accuracy: 0.2652\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   43/13125 [..............................] - ETA: 4:37 - loss: 4.9792 - accuracy: 0.2685\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   46/13125 [..............................] - ETA: 4:35 - loss: 4.9091 - accuracy: 0.2734\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   49/13125 [..............................] - ETA: 4:35 - loss: 4.8730 - accuracy: 0.2717\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   52/13125 [..............................] - ETA: 4:33 - loss: 4.8349 - accuracy: 0.2707","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Epoch 1/50\n\r    1/13125 [..............................] - ETA: 21:59:46 - loss: 5.6662 - accuracy: 0.0000e+00\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    2/13125 [..............................] - ETA: 15:34 - loss: 5.6648 - accuracy: 0.1250       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    5/13125 [..............................] - ETA: 7:35 - loss: 5.6599 - accuracy: 0.2062 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    8/13125 [..............................] - ETA: 6:06 - loss: 5.6542 - accuracy: 0.2227\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   11/13125 [..............................] - ETA: 5:28 - loss: 5.6431 - accuracy: 0.2457\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   14/13125 [..............................] - ETA: 5:09 - loss: 5.6285 - accuracy: 0.2489\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   17/13125 [..............................] - ETA: 5:02 - loss: 5.6107 - accuracy: 0.2463\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   20/13125 [..............................] - ETA: 4:53 - loss: 5.5726 - accuracy: 0.2508\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   23/13125 [..............................] - ETA: 4:47 - loss: 5.5153 - accuracy: 0.2520\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   26/13125 [..............................] - ETA: 4:43 - loss: 5.4135 - accuracy: 0.2566\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   29/13125 [..............................] - ETA: 4:39 - loss: 5.3096 - accuracy: 0.2602\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   32/13125 [..............................] - ETA: 4:37 - loss: 5.2175 - accuracy: 0.2637\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   35/13125 [..............................] - ETA: 4:34 - loss: 5.1366 - accuracy: 0.2661\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   38/13125 [..............................] - ETA: 4:32 - loss: 5.0774 - accuracy: 0.2648\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   40/13125 [..............................] - ETA: 4:37 - loss: 5.0379 - accuracy: 0.2652\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   43/13125 [..............................] - ETA: 4:37 - loss: 4.9792 - accuracy: 0.2685\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   46/13125 [..............................] - ETA: 4:35 - loss: 4.9091 - accuracy: 0.2734\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   49/13125 [..............................] - ETA: 4:35 - loss: 4.8730 - accuracy: 0.2717\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   52/13125 [..............................] - ETA: 4:33 - loss: 4.8349 - accuracy: 0.2707"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Cancelled","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["We upload the trained model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1b0c8497-1c05-4b8f-8172-88b7ad8b81de"}}},{"cell_type":"code","source":["%sh wget https://www.dropbox.com/s/uhoagxy944eppd6/Kaggle_model_completeGames_freq_15K.h5?dl=1 -O /tmp/Kaggle_Freq_completeGames.csv"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"830df2f9-dd84-4936-be1c-f49e8c5e2eb9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"--2022-06-04 07:58:10--  https://www.dropbox.com/s/uhoagxy944eppd6/Kaggle_model_completeGames_freq_15K.h5?dl=1\nResolving www.dropbox.com (www.dropbox.com)... 162.125.2.18, 2620:100:6016:18::a27d:112\nConnecting to www.dropbox.com (www.dropbox.com)|162.125.2.18|:443... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: /s/dl/uhoagxy944eppd6/Kaggle_model_completeGames_freq_15K.h5 [following]\n--2022-06-04 07:58:10--  https://www.dropbox.com/s/dl/uhoagxy944eppd6/Kaggle_model_completeGames_freq_15K.h5\nReusing existing connection to www.dropbox.com:443.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://uc8a9a0bc886ee5f6a69aaf272d7.dl.dropboxusercontent.com/cd/0/get/BmhDjyMBm7s8muwZYDz-U5SSNBPA2HpyB9z2EGjM_c1tNAtASKEKZ7nl5yg-mrKg8I6wpSjK9WZQ3Q6CcGhFsRFof02WbVPyzm6BRaz4DUXWT8ptOU8a9dwDy1PnmzWH-yGuJ2oFNV_tBkYD0iGOd6S0GbCIvh72-uP6IXhyt83utcjNZRR_dNN9wAIsCLGZNnM/file?dl=1# [following]\n--2022-06-04 07:58:11--  https://uc8a9a0bc886ee5f6a69aaf272d7.dl.dropboxusercontent.com/cd/0/get/BmhDjyMBm7s8muwZYDz-U5SSNBPA2HpyB9z2EGjM_c1tNAtASKEKZ7nl5yg-mrKg8I6wpSjK9WZQ3Q6CcGhFsRFof02WbVPyzm6BRaz4DUXWT8ptOU8a9dwDy1PnmzWH-yGuJ2oFNV_tBkYD0iGOd6S0GbCIvh72-uP6IXhyt83utcjNZRR_dNN9wAIsCLGZNnM/file?dl=1\nResolving uc8a9a0bc886ee5f6a69aaf272d7.dl.dropboxusercontent.com (uc8a9a0bc886ee5f6a69aaf272d7.dl.dropboxusercontent.com)... 162.125.8.15, 2620:100:6016:15::a27d:10f\nConnecting to uc8a9a0bc886ee5f6a69aaf272d7.dl.dropboxusercontent.com (uc8a9a0bc886ee5f6a69aaf272d7.dl.dropboxusercontent.com)|162.125.8.15|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2021428 (1.9M) [application/binary]\nSaving to: /tmp/Kaggle_Freq_completeGames.csv\n\n     0K .......... .......... .......... .......... ..........  2%  501K 4s\n    50K .......... .......... .......... .......... ..........  5% 1017K 3s\n   100K .......... .......... .......... .......... ..........  7% 1.20M 2s\n   150K .......... .......... .......... .......... .......... 10% 1.53M 2s\n   200K .......... .......... .......... .......... .......... 12% 2.97M 2s\n   250K .......... .......... .......... .......... .......... 15% 2.70M 1s\n   300K .......... .......... .......... .......... .......... 17% 3.09M 1s\n   350K .......... .......... .......... .......... .......... 20% 3.26M 1s\n   400K .......... .......... .......... .......... .......... 22% 5.12M 1s\n   450K .......... .......... .......... .......... .......... 25% 6.05M 1s\n   500K .......... .......... .......... .......... .......... 27% 6.19M 1s\n   550K .......... .......... .......... .......... .......... 30% 8.00M 1s\n   600K .......... .......... .......... .......... .......... 32% 7.31M 1s\n   650K .......... .......... .......... .......... .......... 35% 7.92M 1s\n   700K .......... .......... .......... .......... .......... 37% 10.3M 1s\n   750K .......... .......... .......... .......... .......... 40% 7.02M 1s\n   800K .......... .......... .......... .......... .......... 43% 3.55M 0s\n   850K .......... .......... .......... .......... .......... 45%  133M 0s\n   900K .......... .......... .......... .......... .......... 48%  138M 0s\n   950K .......... .......... .......... .......... .......... 50% 6.36M 0s\n  1000K .......... .......... .......... .......... .......... 53% 44.2M 0s\n  1050K .......... .......... .......... .......... .......... 55% 7.49M 0s\n  1100K .......... .......... .......... .......... .......... 58%  122M 0s\n  1150K .......... .......... .......... .......... .......... 60% 9.66M 0s\n  1200K .......... .......... .......... .......... .......... 63% 19.0M 0s\n  1250K .......... .......... .......... .......... .......... 65% 21.6M 0s\n  1300K .......... .......... .......... .......... .......... 68% 17.2M 0s\n  1350K .......... .......... .......... .......... .......... 70% 16.6M 0s\n  1400K .......... .......... .......... .......... .......... 73% 19.1M 0s\n  1450K .......... .......... .......... .......... .......... 75% 17.7M 0s\n  1500K .......... .......... .......... .......... .......... 78% 22.7M 0s\n  1550K .......... .......... .......... .......... .......... 81% 16.2M 0s\n  1600K .......... .......... .......... .......... .......... 83% 15.6M 0s\n  1650K .......... .......... .......... .......... .......... 86% 27.2M 0s\n  1700K .......... .......... .......... .......... .......... 88% 23.2M 0s\n  1750K .......... .......... .......... .......... .......... 91% 12.8M 0s\n  1800K .......... .......... .......... .......... .......... 93% 49.6M 0s\n  1850K .......... .......... .......... .......... .......... 96% 27.1M 0s\n  1900K .......... .......... .......... .......... .......... 98% 28.0M 0s\n  1950K .......... .......... ....                            100% 23.3M=0.4s\n\n2022-06-04 07:58:12 (4.64 MB/s) - /tmp/Kaggle_Freq_completeGames.csv saved [2021428/2021428]\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["--2022-06-04 07:58:10--  https://www.dropbox.com/s/uhoagxy944eppd6/Kaggle_model_completeGames_freq_15K.h5?dl=1\nResolving www.dropbox.com (www.dropbox.com)... 162.125.2.18, 2620:100:6016:18::a27d:112\nConnecting to www.dropbox.com (www.dropbox.com)|162.125.2.18|:443... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: /s/dl/uhoagxy944eppd6/Kaggle_model_completeGames_freq_15K.h5 [following]\n--2022-06-04 07:58:10--  https://www.dropbox.com/s/dl/uhoagxy944eppd6/Kaggle_model_completeGames_freq_15K.h5\nReusing existing connection to www.dropbox.com:443.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://uc8a9a0bc886ee5f6a69aaf272d7.dl.dropboxusercontent.com/cd/0/get/BmhDjyMBm7s8muwZYDz-U5SSNBPA2HpyB9z2EGjM_c1tNAtASKEKZ7nl5yg-mrKg8I6wpSjK9WZQ3Q6CcGhFsRFof02WbVPyzm6BRaz4DUXWT8ptOU8a9dwDy1PnmzWH-yGuJ2oFNV_tBkYD0iGOd6S0GbCIvh72-uP6IXhyt83utcjNZRR_dNN9wAIsCLGZNnM/file?dl=1# [following]\n--2022-06-04 07:58:11--  https://uc8a9a0bc886ee5f6a69aaf272d7.dl.dropboxusercontent.com/cd/0/get/BmhDjyMBm7s8muwZYDz-U5SSNBPA2HpyB9z2EGjM_c1tNAtASKEKZ7nl5yg-mrKg8I6wpSjK9WZQ3Q6CcGhFsRFof02WbVPyzm6BRaz4DUXWT8ptOU8a9dwDy1PnmzWH-yGuJ2oFNV_tBkYD0iGOd6S0GbCIvh72-uP6IXhyt83utcjNZRR_dNN9wAIsCLGZNnM/file?dl=1\nResolving uc8a9a0bc886ee5f6a69aaf272d7.dl.dropboxusercontent.com (uc8a9a0bc886ee5f6a69aaf272d7.dl.dropboxusercontent.com)... 162.125.8.15, 2620:100:6016:15::a27d:10f\nConnecting to uc8a9a0bc886ee5f6a69aaf272d7.dl.dropboxusercontent.com (uc8a9a0bc886ee5f6a69aaf272d7.dl.dropboxusercontent.com)|162.125.8.15|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2021428 (1.9M) [application/binary]\nSaving to: /tmp/Kaggle_Freq_completeGames.csv\n\n     0K .......... .......... .......... .......... ..........  2%  501K 4s\n    50K .......... .......... .......... .......... ..........  5% 1017K 3s\n   100K .......... .......... .......... .......... ..........  7% 1.20M 2s\n   150K .......... .......... .......... .......... .......... 10% 1.53M 2s\n   200K .......... .......... .......... .......... .......... 12% 2.97M 2s\n   250K .......... .......... .......... .......... .......... 15% 2.70M 1s\n   300K .......... .......... .......... .......... .......... 17% 3.09M 1s\n   350K .......... .......... .......... .......... .......... 20% 3.26M 1s\n   400K .......... .......... .......... .......... .......... 22% 5.12M 1s\n   450K .......... .......... .......... .......... .......... 25% 6.05M 1s\n   500K .......... .......... .......... .......... .......... 27% 6.19M 1s\n   550K .......... .......... .......... .......... .......... 30% 8.00M 1s\n   600K .......... .......... .......... .......... .......... 32% 7.31M 1s\n   650K .......... .......... .......... .......... .......... 35% 7.92M 1s\n   700K .......... .......... .......... .......... .......... 37% 10.3M 1s\n   750K .......... .......... .......... .......... .......... 40% 7.02M 1s\n   800K .......... .......... .......... .......... .......... 43% 3.55M 0s\n   850K .......... .......... .......... .......... .......... 45%  133M 0s\n   900K .......... .......... .......... .......... .......... 48%  138M 0s\n   950K .......... .......... .......... .......... .......... 50% 6.36M 0s\n  1000K .......... .......... .......... .......... .......... 53% 44.2M 0s\n  1050K .......... .......... .......... .......... .......... 55% 7.49M 0s\n  1100K .......... .......... .......... .......... .......... 58%  122M 0s\n  1150K .......... .......... .......... .......... .......... 60% 9.66M 0s\n  1200K .......... .......... .......... .......... .......... 63% 19.0M 0s\n  1250K .......... .......... .......... .......... .......... 65% 21.6M 0s\n  1300K .......... .......... .......... .......... .......... 68% 17.2M 0s\n  1350K .......... .......... .......... .......... .......... 70% 16.6M 0s\n  1400K .......... .......... .......... .......... .......... 73% 19.1M 0s\n  1450K .......... .......... .......... .......... .......... 75% 17.7M 0s\n  1500K .......... .......... .......... .......... .......... 78% 22.7M 0s\n  1550K .......... .......... .......... .......... .......... 81% 16.2M 0s\n  1600K .......... .......... .......... .......... .......... 83% 15.6M 0s\n  1650K .......... .......... .......... .......... .......... 86% 27.2M 0s\n  1700K .......... .......... .......... .......... .......... 88% 23.2M 0s\n  1750K .......... .......... .......... .......... .......... 91% 12.8M 0s\n  1800K .......... .......... .......... .......... .......... 93% 49.6M 0s\n  1850K .......... .......... .......... .......... .......... 96% 27.1M 0s\n  1900K .......... .......... .......... .......... .......... 98% 28.0M 0s\n  1950K .......... .......... ....                            100% 23.3M=0.4s\n\n2022-06-04 07:58:12 (4.64 MB/s) - /tmp/Kaggle_Freq_completeGames.csv saved [2021428/2021428]\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["model2_completeGames = keras.models.load_model(\"/tmp/Kaggle_Freq_completeGames.csv\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6c31a645-1d40-4da1-9bc0-2b72ea77d959"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["plt.plot(history.history['loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train'], loc='upper right')\nplt.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"08a57ab4-6279-45f4-ad8a-1b61a4504db3"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["<img src=\"https://www.dropbox.com/s/jjg0ktmyzq85kwe/Model8_loss.png?dl=1\">"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"31af1dd0-379a-4274-b4d1-207e9a28639d"}}},{"cell_type":"code","source":["plt.plot(history.history['accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train'], loc='upper left')\nplt.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cea51250-3413-4ded-9682-156e27e697a1"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["<img src=\"https://www.dropbox.com/s/igmu586vzfhmg04/Model8_accuracy.png?dl=1\">"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"934ddad7-4dbb-4a28-b568-6353169c0554"}}},{"cell_type":"markdown","source":["### Evaluation of Third approach models\nIn this section the evaluation of models trained with this approach is performed. For more information on the evaluation conducted the reader is invited to read the section \"Evaluation Backgorund\" in the Introduction."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fce43ac3-29c9-4eb7-9cb0-6f0c5b72b8ff"}}},{"cell_type":"code","source":["#How many times do the models predict correctly\nm1=0\nm2=0\n#Multiplication of probabilities for final calculation of perplexity\np_m1=1\np_m2=1\n#List for saving perplexities divided by match\np_model1=[]\np_model2=[]\n \ngam_count=0\nfor i in range(0,len(test_x[:35000])):\n  el = np.asarray(test_x[i])\n  el = np.reshape(el, (1, len(el), 1))\n  \n  el_freq = np.asarray(test_freq_x[i])\n  el_freq = np.reshape(el_freq, (1, len(el_freq), 1))\n  \n  #MODEL 1 \n  prediction = model1_completeGames.predict(el, verbose=0)\n  index = np.argmax(prediction)\n  p_m1=p_m1 * prediction[0][index]\n  \n  #MODEL 2\n  prediction2 = model2_completeGames.predict(el_freq, verbose=0)\n  index2 = np.argmax(prediction2)\n  p_m2=p_m2 * prediction2[0][index2]\n  \n  if gam_count==69:\n    p_model1.append(p_m1**(-(1/70)))\n    p_model2.append(p_m2**(-(1/70)))\n    p_m1=1\n    p_m2=1\n    gam_count=0\n\n  if index==test_y[i]:\n    m1+=1\n  if index2==test_freq_y[i]:\n    m2+=1\n  if i%100==0:\n    print(i)\n    \n  gam_count+=1\n  \nprint(\"-----------------------\")\nprint(\"model1 -> \", m1)\nprint(\"model2 -> \", m2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"80d6e4fd-4940-4b7d-82fb-28017bb3d03e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"0\n100\n200\n300\n400\n500\n600\n700\n800\n900\n1000\n1100\n1200\n1300\n1400\n1500\n1600\n1700\n1800\n1900\n2000\n2100\n2200\n2300\n2400\n2500\n2600\n2700\n2800\n2900\n3000\n3100\n3200\n3300\n3400\n3500\n3600\n3700\n3800\n3900\n4000\n4100\n4200\n4300\n4400\n4500\n4600\n4700\n4800\n4900\n5000\n5100\n5200\n5300\n5400\n5500\n5600\n5700\n5800\n5900\n6000\n6100\n6200\n6300\n6400\n6500\n6600\n6700\n6800\n6900\n7000\n7100\n7200\n7300\n7400\n7500\n7600\n7700\n7800\n7900\n8000\n8100\n8200\n8300\n8400\n8500\n8600\n8700\n8800\n8900\n9000\n9100\n9200\n9300\n9400\n9500\n9600\n9700\n9800\n9900\n10000\n10100\n10200\n10300\n10400\n10500\n10600\n10700\n10800\n10900\n11000\n11100\n11200\n11300\n11400\n11500\n11600\n11700\n11800\n11900\n12000\n12100\n12200\n12300\n12400\n12500\n12600\n12700\n12800\n12900\n13000\n13100\n13200\n13300\n13400\n13500\n13600\n13700\n13800\n13900\n14000\n14100\n14200\n14300\n14400\n14500\n14600\n14700\n14800\n14900\n15000\n15100\n15200\n15300\n15400\n15500\n15600\n15700\n15800\n15900\n16000\n16100\n16200\n16300\n16400\n16500\n16600\n16700\n16800\n16900\n17000\n17100\n17200\n17300\n17400\n17500\n17600\n17700\n17800\n17900\n18000\n18100\n18200\n18300\n18400\n18500\n18600\n18700\n18800\n18900\n19000\n19100\n19200\n19300\n19400\n19500\n19600\n19700\n19800\n19900\n20000\n20100\n20200\n20300\n20400\n20500\n20600\n20700\n20800\n20900\n21000\n21100\n21200\n21300\n21400\n21500\n21600\n21700\n21800\n21900\n22000\n22100\n22200\n22300\n22400\n22500\n22600\n22700\n22800\n22900\n23000\n23100\n23200\n23300\n23400\n23500\n23600\n23700\n23800\n23900\n24000\n24100\n24200\n24300\n24400\n24500\n24600\n24700\n24800\n24900\n25000\n25100\n25200\n25300\n25400\n25500\n25600\n25700\n25800\n25900\n26000\n26100\n26200\n26300\n26400\n26500\n26600\n26700\n26800\n26900\n27000\n27100\n27200\n27300\n27400\n27500\n27600\n27700\n27800\n27900\n28000\n28100\n28200\n28300\n28400\n28500\n28600\n28700\n28800\n28900\n29000\n29100\n29200\n29300\n29400\n29500\n29600\n29700\n29800\n29900\n30000\n30100\n30200\n30300\n30400\n30500\n30600\n30700\n30800\n30900\n31000\n31100\n31200\n31300\n31400\n31500\n31600\n31700\n31800\n31900\n32000\n32100\n32200\n32300\n32400\n32500\n32600\n32700\n32800\n32900\n33000\n33100\n33200\n33300\n33400\n33500\n33600\n33700\n33800\n33900\n34000\n34100\n34200\n34300\n34400\n34500\n34600\n34700\n34800\n34900\n-----------------------\nmodel1 ->  15520\nmodel2 ->  16434\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["0\n100\n200\n300\n400\n500\n600\n700\n800\n900\n1000\n1100\n1200\n1300\n1400\n1500\n1600\n1700\n1800\n1900\n2000\n2100\n2200\n2300\n2400\n2500\n2600\n2700\n2800\n2900\n3000\n3100\n3200\n3300\n3400\n3500\n3600\n3700\n3800\n3900\n4000\n4100\n4200\n4300\n4400\n4500\n4600\n4700\n4800\n4900\n5000\n5100\n5200\n5300\n5400\n5500\n5600\n5700\n5800\n5900\n6000\n6100\n6200\n6300\n6400\n6500\n6600\n6700\n6800\n6900\n7000\n7100\n7200\n7300\n7400\n7500\n7600\n7700\n7800\n7900\n8000\n8100\n8200\n8300\n8400\n8500\n8600\n8700\n8800\n8900\n9000\n9100\n9200\n9300\n9400\n9500\n9600\n9700\n9800\n9900\n10000\n10100\n10200\n10300\n10400\n10500\n10600\n10700\n10800\n10900\n11000\n11100\n11200\n11300\n11400\n11500\n11600\n11700\n11800\n11900\n12000\n12100\n12200\n12300\n12400\n12500\n12600\n12700\n12800\n12900\n13000\n13100\n13200\n13300\n13400\n13500\n13600\n13700\n13800\n13900\n14000\n14100\n14200\n14300\n14400\n14500\n14600\n14700\n14800\n14900\n15000\n15100\n15200\n15300\n15400\n15500\n15600\n15700\n15800\n15900\n16000\n16100\n16200\n16300\n16400\n16500\n16600\n16700\n16800\n16900\n17000\n17100\n17200\n17300\n17400\n17500\n17600\n17700\n17800\n17900\n18000\n18100\n18200\n18300\n18400\n18500\n18600\n18700\n18800\n18900\n19000\n19100\n19200\n19300\n19400\n19500\n19600\n19700\n19800\n19900\n20000\n20100\n20200\n20300\n20400\n20500\n20600\n20700\n20800\n20900\n21000\n21100\n21200\n21300\n21400\n21500\n21600\n21700\n21800\n21900\n22000\n22100\n22200\n22300\n22400\n22500\n22600\n22700\n22800\n22900\n23000\n23100\n23200\n23300\n23400\n23500\n23600\n23700\n23800\n23900\n24000\n24100\n24200\n24300\n24400\n24500\n24600\n24700\n24800\n24900\n25000\n25100\n25200\n25300\n25400\n25500\n25600\n25700\n25800\n25900\n26000\n26100\n26200\n26300\n26400\n26500\n26600\n26700\n26800\n26900\n27000\n27100\n27200\n27300\n27400\n27500\n27600\n27700\n27800\n27900\n28000\n28100\n28200\n28300\n28400\n28500\n28600\n28700\n28800\n28900\n29000\n29100\n29200\n29300\n29400\n29500\n29600\n29700\n29800\n29900\n30000\n30100\n30200\n30300\n30400\n30500\n30600\n30700\n30800\n30900\n31000\n31100\n31200\n31300\n31400\n31500\n31600\n31700\n31800\n31900\n32000\n32100\n32200\n32300\n32400\n32500\n32600\n32700\n32800\n32900\n33000\n33100\n33200\n33300\n33400\n33500\n33600\n33700\n33800\n33900\n34000\n34100\n34200\n34300\n34400\n34500\n34600\n34700\n34800\n34900\n-----------------------\nmodel1 ->  15520\nmodel2 ->  16434\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["Once we have calculated the model perplexity for each match we calculate an average of these perplexities to compare the models"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c34b8458-58f4-40c0-af6a-f8f32410bca6"}}},{"cell_type":"code","source":["s_m1=0\ns_m2=0\nfor i in range(0,len(p_model1)):\n  s_m1+=p_model1[i]\n  s_m2+=p_model2[i]\n  \navg_m1=s_m1/len(p_model1)\navg_m2=s_m2/len(p_model2)\n\nprint(\"The average perplexity of model 1 is\", avg_m1)\nprint(\"The average perplexity of model 2 is\", avg_m2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"58d6e304-ead7-4cbc-a0b8-bfd1c163b88f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"The average perplexity of model 1 is 6.041866274582815\nThe average perplexity of model 2 is 3.842215829198064\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["The average perplexity of model 1 is 6.041866274582815\nThe average perplexity of model 2 is 3.842215829198064\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Discussion of evaluation results\nThe models created were evaluated in the same way. In the extrinsic evaluation out of 35000 predictions. Model 1 correctly predicted 15520 times while model 2, which considers only the most frequent moves, correctly predicts 16434 times. The perplexity of these models follows the same trend. Model 2 also has lower perplexity than model 1. In fact, model 2 has a perplexity of 3.8 and model 2 of 6.04."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e1b37e44-23d2-42ab-80d9-951a7339515a"}}},{"cell_type":"markdown","source":["# Section 5: Application to use case\n<p>In order to show real use of these models, an application for playing chess was created.</p>\n<p>Since databricks does not support keyboard input, the application was developed on a kaggle notebook. It is possible to find the application <a href=\"https://www.kaggle.com/code/valeriogoretti98/bigdataapplication-chess\">here</a>.</p>\n<p>The cells that were created for application on kaggle are listed below for completeness of the project. However, the executable application is located on kaggle.</p>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f299b3f9-8218-42b6-99ab-ac2a78aa179a"}}},{"cell_type":"markdown","source":["Installation of required libraries"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7761c125-5b61-4fb3-a07a-97f9eb7c487d"}}},{"cell_type":"code","source":["%pip install chess\n%pip install gym-chess"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b5487bd9-1887-45ca-bd2b-1e04bda2628b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Import of installed libraries"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"99b3094c-ca20-4535-ba9f-855bf57cf586"}}},{"cell_type":"code","source":["import tensorflow as tf\nfrom tensorflow import keras\nimport json"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d1bc0227-ce3c-44ae-bf60-199ecf9e4554"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Loading models"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c9384047-3d11-4348-b1d8-79e6dab958fc"}}},{"cell_type":"code","source":["model_onlyPiece=keras.models.load_model(\"../input/chessmodelbigdata/model_kaggle_300kgames_50epoch.h5\")\nmodel_completeGames=keras.models.load_model(\"../input/chessmodelbigdata/Kaggle_model_completeGames_15K.h5\")\nmodel_onlyInitial=keras.models.load_model(\"../input/chessmodelbigdata/model_OnlyInitial_NoFreq_256Units.h5\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"71827898-dce8-46cb-99c8-b167a4f9ab82"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["Loading dictionaries"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3af34183-8f6e-4670-8c55-6ecacc0c90d4"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["with open('../input/chessmodelbigdata/dic_moves_onlyInitial_100kgames.txt') as json_file:\n    dic_OnlyInitial = json.load(json_file)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ac98a045-0127-415e-91f1-2a4d7e546582"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["with open('../input/chessmodelbigdata/dic_moves_completeGames_15k.txt') as json_file:\n    dic_completeGames = json.load(json_file)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"db971388-229e-466c-991c-c263ac5aa0ca"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["dic = {\"p\" : 0, \"n\" : 1, \"b\" : 2, \"r\" : 3, \"q\" : 4, \"k\" : 5, \"fill\":6}"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"08c17c91-6798-441f-890d-39359b64011e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Definition of a function that given a list of 5 elements and one element sets that element to the last position in the list and moves the others down one position. This function will serve us to track the moves made during the game so that the models can predict the next move."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4f58a540-6195-4858-ae22-bcb8eabd49cc"}}},{"cell_type":"code","source":["def shift(l,m):\n    for i in range(0,5):\n        if i!=4:\n            l[i]=l[i+1]\n        else:\n            l[i]=m\n    return l"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4d8d14a4-3a77-4b56-a358-2af20ad8bc56"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["In the following cell is declares the piece function that given a move returns the piece used. This function is used to create the sliding windows for the first approach model. Also defined is the decode_piece function that given an encoding returns the name of the piece."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2e686d40-56a6-440b-8613-d50f35b11f36"}}},{"cell_type":"code","source":["def piece(m):\n  if m.startswith('O-O'):\n    return \"k\"\n  if m[0].isupper():\n    if m[0].lower() in dic.keys():\n      return m[0].lower()\n  else:\n    return \"p\"\n\ndef decode_piece(n):\n    for k,v in dic.items():\n        if v==n:\n            return k"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"014d0b62-7c27-4ba2-9e9c-9393f1caebd5"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["In the following cell is declares the def move_onlyInitial function that given a move returns the appropriate parsed move for the second approach.  This function is used to create the sliding windows for the second approach model. Also defined is the decode_OnlyInitial function that given an encoding returns the name of the piece."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ced4930e-0a27-4978-9f81-27f724be506c"}}},{"cell_type":"code","source":["import re\n\ndef move_onlyInitial(m):\n  if m.startswith('O-O') or len(m)<=3:\n    if m[-1]==\"#\" or m[-1]==\"+\":\n      m=m[:-1]\n    return m\n  else:\n    z=\"\"\n    number = re.search(r\"\\d\", m)\n    z = m[0] + m[number.start()-1] + m[number.start()]\n    return z\n\ndef decode_OnlyInitial(n):\n    for k,v in dic_OnlyInitial.items():\n        if v==n:\n            return k"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"42f33fb5-770e-43fa-9404-e340471472d0"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["In the following cell is declares the def move_complete function that given a move returns the appropriate parsed move for the third approach.  This function is used to create the sliding windows for the third approach model. Also defined is the decode_completeGames function that given an encoding returns the name of the piece."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c0506d2e-a24c-40a7-80e2-a2c77bd19638"}}},{"cell_type":"code","source":["def move_complete(m):\n  if m.startswith('O-O'):\n    return m\n  if m[-1]==\"#\" or m[-1]==\"+\":\n      m=m[:-1]\n  cell=m[-2:]\n  m=m[:-2]\n  if len(m)==2 and m[1]=='x':\n    m=m[:1]\n  if len(m)==3 and m[2]=='x':\n    m=m[:2]\n  return m + cell\n\ndef decode_completeGames(n):\n    for k,v in dic_completeGames.items():\n        if v==n:\n            return k"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"378ac507-b3b5-434e-ab81-a75744f66517"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["In the following cell is the application that allows us to play and practice chess by following the moves recommended by the models"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"662607c4-09c9-4a52-852a-6ce98c6dc645"}}},{"cell_type":"code","source":["import gym\nimport gym_chess\nimport chess\nimport time\nimport numpy as np\n\nenv = gym.make('Chess-v0')\nboard = chess.Board()\nenv.reset()\n\nonly_piece=[0,0,0,0,0]\nonly_Initial=[0,0,0,0,0]\ncomplete_game=[0,0,0,0,0]\nprint(env.render(mode='unicode'))\nc=0\nwhile board.is_checkmate()==False or board.is_stalemate()==False:\n    if c>=5:\n        #ONLY PIECE\n        el = np.asarray(only_piece)\n        el = np.reshape(el, (1, len(el), 1))\n        prediction = model_onlyPiece.predict(el, verbose=0)\n        index = np.argmax(prediction)\n        print(\"Only Piece Advice -> \",decode_piece(index))\n        #ONLY INITIAL\n        el2 = np.asarray(only_Initial)\n        el2 = np.reshape(el2, (1, len(el2), 1))\n        prediction2 = model_onlyInitial.predict(el2, verbose=0)\n        index2 = np.argmax(prediction2)\n        print(\"Only Cell Advice -> \",decode_OnlyInitial(index2))\n        #COMPLETE GAMES\n        el3 = np.asarray(complete_game)\n        el3 = np.reshape(el3, (1, len(el3), 1))\n        prediction3 = model_completeGames.predict(el3, verbose=0)\n        index3 = np.argmax(prediction3)\n        print(\"Complete Moves Advice -> \",decode_completeGames(index3))\n    \n    if c%2==0:\n        print(\"\\n White player -> Insert your move:\")\n    else:\n        print(\"\\n Black player -> Insert your move:\")\n        \n    mo = input()\n    \n    only_piece=shift(only_piece,dic[piece(mo)])\n    only_Initial=shift(only_Initial,dic_OnlyInitial[move_onlyInitial(mo)])\n    complete_game=shift(complete_game,dic_completeGames[move_complete(mo)])\n    \n    \n    env.step(board.parse_san(mo))\n    board.push_san(mo)\n    \n    print(\"\\n\")\n    print(env.render(mode='unicode'))\n    c+=1"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7febf716-e612-429a-8a4a-ee345c3cf109"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"LSTMFinale","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3686562843671968}},"nbformat":4,"nbformat_minor":0}
